plot.bonf.T2()
pressure <- read.table('pressure.txt',
col.names = c('h.0', 'h.8', 'h.16', 'h.24'))
head(pressure)
dim(pressure)
# Test for multivariate normality (Henze-Zirkler test by default)
mvn(pressure)$multivariateNormality
# Plotting each observation as a line with 'h.0', 'h.8', 'h.16', 'h.24' on x-axis
matplot(t(pressure), type='l', lty = 1)
## Question (a) ##
n <- dim(pressure)[1]
q <- dim(pressure)[2]
M <- sapply(pressure, mean) # sample mean
M
S <- cov(pressure) # covariance matrix
S
# we build one of the possible contrast matrices to answer
# the question
C <- matrix(c(-1, 1, 0, 0,
-1, 0, 1, 0,
-1, 0, 0, 1), 3, 4, byrow=T)
C
# Test: H0: C %*% mu == c(0, 0, 0) vs H1: C %*% mu != c(0, 0, 0)
alpha   <- .05
delta.0 <- c(0, 0, 0)
Md <- C %*% M # Sample mean of the "contrasted" observations
Sd <- C %*% S %*% t(C) # Sample covariance of the contrasted observations
Sdinv <- solve(Sd)
# Hotelling T2 statistics
T2 <- n * t(Md - delta.0) %*% Sdinv %*% (Md - delta.0)
# (q-1)*(n-1)/(n-(q-1)) times the 1-alpha Fisher quantile with q-1 and n-q+1 df
cfr.fisher <- ((q - 1) * (n - 1) / (n - (q - 1))) * qf(1 - alpha, (q - 1), n - (q - 1))
T2 < cfr.fisher # Testing if we are in the rejection region
T2
cfr.fisher
# T2 is much higher than cfr.fisher => the p-value will be very small
P <- 1 - pf(T2 * (n - (q - 1)) / ((q - 1) * (n - 1)), (q - 1), n - (q - 1))
P
# Simultaneous T2 intervals in the direction of the contrasts h.* - h.0
IC.T2 <- cbind(Md - sqrt(cfr.fisher*diag(Sd)/n),
Md,
Md + sqrt(cfr.fisher*diag(Sd)/n))
IC.T2
# Bonferroni intervals
k <- q-1   # number of increments (i.e., dim(C)[1])
cfr.t <- qt(1-alpha/(2*k), n-1)
IC.BF <- cbind(Md - cfr.t*sqrt(diag(Sd)/n),
Md,
Md + cfr.t*sqrt(diag(Sd)/n))
IC.BF
plot.bonf.T2 <- function() {
matplot(t(matrix(1:3, 3, 3)), t(IC.BF), type='b', pch='', xlim=c(0,4), xlab='',
ylab='', main='Confidence intervals')
# Plotting the Bonferroni intervals
segments(matrix(1:3, 3, 1), IC.BF[,1], matrix(1:3, 3, 1), IC.BF[,3],
col='orange', lwd=2)
points(1:3, IC.BF[,2], col='orange', pch=16)
# Plotting delta.0 under H0 (delta.0 == c(0, 0, 0))
points(1:3+.05, delta.0, col='black', pch=16)
# Plotting the simultaneous T2
segments(matrix(1:3+.1,3,1),IC.T2[,1],matrix(1:3+.1,3,1),IC.T2[,3], col='blue', lwd=2)
points(1:3+.1,IC.T2[,2], col='blue', pch=16)
legend('topright', c('Bonf. IC', 'Sim-T2 IC'), col=c('orange', 'blue'), lty=1, lwd=2)
}
plot.bonf.T2()
### what happens if we change the constrast matrix?
Cbis <- matrix(c(-1, 1, 0, 0,
0, -1, 1, 0,
0, 0, -1, 1), 3, 4, byrow=T)
Cbis
Mdbis <- Cbis %*% M
Sdbis <- Cbis %*% S %*% t(Cbis)
Sdinvbis <- solve(Sdbis)
T2bis <- n * t(Mdbis) %*% Sdinvbis %*% Mdbis
T2bis < cfr.fisher
# compare the T2 test statistics associated with C and Cbis
T2bis
T2
# Bonferroni intervals for Cbis
IC.BFbis <- cbind(Mdbis - cfr.t*sqrt(diag(Sdbis)/n),
Mdbis,
Mdbis + cfr.t*sqrt(diag(Sdbis)/n))
# Sim. T2 intervals for Cbis
IC.T2bis <- cbind(Mdbis - sqrt(cfr.fisher*diag(Sdbis)/n),
Mdbis,
Mdbis + sqrt(cfr.fisher*diag(Sdbis)/n))
IC.BFbis
IC.BF
IC.T2bis
IC.T2
C <- matrix(c(-1, 1, 0, 0,
-1, 0, 1, 0,
-1, 0, 0, 1), 3, 4, byrow=T)
delta.0 <- c(-2, -2, 0) # testing, that differnece between first and sencond time we have difference of -2
C <- matrix(c(-1, 1, 0, 0,
0, -1, 1, 0,
0, 0, -1, 1), 3, 4, byrow=T)
delta.0 <- c(2.5, 0, 2) # test of difference between third and second times is 0
Md <- C %*% M # sample mean for the contrasted observations
Sd <- C %*% S %*% t(C) # sample covariance matrix
Sdinv <- solve(Sd)
# Hotelling T2 statistic
T2 <- n * t(Md - delta.0) %*% Sdinv %*% (Md - delta.0)
# (q-1)*(n-1)/(n-q+1) times the 1-alpha Fisher quantile with q-1 and n-q+1 df
cfr.fisher <- ((q - 1) * (n - 1) / (n - (q - 1))) * qf(1 - alpha, (q - 1), n - (q - 1))
T2 < cfr.fisher # Do we accept H0?
T2
cfr.fisher
# p-value
P <- 1 - pf(T2 * (n - (q - 1)) / ((q - 1) * (n - 1)), (q - 1), n - (q - 1))
P
# we build the data: two bivariate samples of resp. 3 and 4 obs
t1 <- matrix(c(3, 3, 1, 6, 2, 3), 2)
t1 <- data.frame(t(t1))
t2 <- matrix(c(2, 3, 5, 1, 3, 1, 2, 3), 2)
t2 <- data.frame(t(t2))
t1
t2
n1 <- dim(t1)[1] # n1 = 3
n2 <- dim(t2)[1] # n2 = 4
p  <- dim(t1)[2] # p = 2
t1.mean <- sapply(t1, mean)
t2.mean <- sapply(t2, mean)
t1.cov  <-  cov(t1)
t2.cov  <-  cov(t2)
Sp      <- ((n1 - 1) * t1.cov + (n2 - 1) * t2.cov) / (n1 + n2 - 2) # poll covariance matrix
# (thumb -rule)
# We compare the matrices -> here, using rule of thumb:
# we don't reject equality of covariance matrices if s1_ii and s2_ii differ from
# less than a factor ~4 (see J-W p.291) (factor ~4 <=> less than 0.25)
# короче если sigma11 <= 4sigma22 то все нор, ситаем дальше (без объяснения)
list(S1 = t1.cov, S2 = t2.cov, Spooled = Sp)
alpha   <- .01
delta.0 <- c(0, 0)
Spinv   <- solve(Sp)
T2 <- n1 * n2 / (n1 + n2) * (t1.mean - t2.mean - delta.0) %*% Spinv %*% (t1.mean - t2.mean - delta.0)
cfr.fisher <- (p * (n1 + n2 - 2) / (n1 + n2 - 1 - p)) * qf(1 - alpha, p, n1 + n2 - 1 - p)
T2 < cfr.fisher # TRUE: no statistical evidence to reject H0 at level 1%
P <- 1 - pf(T2 / (p * (n1 + n2 - 2) / (n1 + n2 - 1 - p)), p, n1 + n2 - 1 - p)
P
# Simultaneous T2 intervals
IC.T2.X1 <- c(
t1.mean[1] - t2.mean[1] - sqrt(cfr.fisher * Sp[1, 1] * (1 / n1 + 1 / n2)),
t1.mean[1] - t2.mean[1] + sqrt(cfr.fisher * Sp[1, 1] * (1 / n1 + 1 / n2))
)
IC.T2.X2 <- c(
t1.mean[2] - t2.mean[2] - sqrt(cfr.fisher * Sp[2, 2] * (1 / n1 + 1 / n2)),
t1.mean[2] - t2.mean[2] + sqrt(cfr.fisher * Sp[2, 2] * (1 / n1 + 1 / n2))
)
IC.T2 <- rbind(IC.T2.X1, IC.T2.X2)
dimnames(IC.T2)[[2]] <- c('inf', 'sup')
IC.T2
allergy <- read.table('hatingalmonds.txt')
head(allergy)
dim(allergy)
noallergy <- read.table('lovingalmonds.txt')
head(noallergy)
dim(noallergy)
n1 <- dim(allergy)[1]
n2 <- dim(noallergy)[1]
p <- dim(noallergy)[2]
x.mean1 <- sapply(allergy, mean)
x.mean2 <- sapply(noallergy, mean)
p.hat <- (x.mean1 * n1 + x.mean2 * n2) / (n1 + n2) #
x.var <- (p.hat * (1 - p.hat)) # poold variance
z.i <- (x.mean1 - x.mean2) / sqrt(x.var * (1 / n1 + 1 / n2))
p.i <- ifelse(z.i < 0, 2 * pnorm(z.i), 2 * (1 - pnorm(z.i)))
which(p.i<.01) # but we also could make type-1 error, and wrong reject,
# Bonferroni (control the family-wise error rate)
k <- 520
which(p.i*k<.01)
# or
p.Bf <- p.adjust(p.i, method='bonferroni')
which(p.Bf<.01)
# Benjamini-Hockberg (control the false discovery rate) (better)
p.BH <- p.adjust(p.i, method='BH')
which(p.BH<.01)
par(mfrow=c(1,3))
plot(p.i, main='Univariate')
abline(h=.01, lwd=2, col='red')
plot(p.Bf, main='Corrected - Bonferroni')
abline(h=.01, lwd=2, col='red')
plot(p.BH, main='Corrected - BH')
abline(h=.01, lwd=2, col='red')
# Additional exercise on mean differences between independent Gaussian:
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# a) Perform a statistical test of level 99\% to verify if the mean evaluations in the two
# Additional exercise on mean differences between independent Gaussian:
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# a) Perform a statistical test of level 99\% to verify if the mean evaluations in the two
#    cities differ. State and verify the model assumptions.
# The Galician Food Association has launched an award for the Best Pulpo a la Gallega (meaning
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# a) Perform a statistical test of level 99\% to verify if the mean evaluations in the two
#    cities differ. State and verify the model assumptions.
# b) Interpret the results of the test at point (a) through two Bonferroni intervals of
# Galician-style octopus). As part of the challenge, two tasters are sent to evaluate the
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# a) Perform a statistical test of level 99\% to verify if the mean evaluations in the two
#    cities differ. State and verify the model assumptions.
# b) Interpret the results of the test at point (a) through two Bonferroni intervals of
#    global level 99% for appropriate differences in the mean. Comment the result.
# 30 finalist octopus dishes in A Coruna and the 30 finalist octopus dishes in Pontevedra.
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# a) Perform a statistical test of level 99\% to verify if the mean evaluations in the two
#    cities differ. State and verify the model assumptions.
# b) Interpret the results of the test at point (a) through two Bonferroni intervals of
#    global level 99% for appropriate differences in the mean. Comment the result.
# c) Is there statistical evidence to state that, at level 99%, the average evaluations of
# Files "acoruna.txt" and "pontevedra.txt" collect the evaluations on each of the finalist
# dishes given by the tasters in A Coruna and Pontevedra, respectively. Assume the evaluations
# on different dishes to be independent, and the evaluations of the two tasters on the same
# octopus dish to come from a bivariate Gaussian distribution.
# a) Perform a statistical test of level 99\% to verify if the mean evaluations in the two
#    cities differ. State and verify the model assumptions.
# b) Interpret the results of the test at point (a) through two Bonferroni intervals of
#    global level 99% for appropriate differences in the mean. Comment the result.
# c) Is there statistical evidence to state that, at level 99%, the average evaluations of
#    A Coruna's octopus dishes are in mean higher than those of Pontevedra's octopus dishes?
effluent <- read.table('effluent.txt', header = T)
effluent
pairs(effluent, pch=19, main='Dataset effluent')
# we compute the sample of differences
D <- data.frame(
DBOD = effluent$BOD_Lab1 - effluent$BOD_Lab2,
DSS  = effluent$SS_Lab1 - effluent$SS_Lab2
)
D
# Scatter plot of the dataset of differences with the four quadrants
plot(D, asp=1, pch=19, main='Dataset of Differences')
abline(h=0, v=0, col='grey35')
points(0,0, pch=19, col='grey35')
# Test the Gaussian assumption (on D!)
result <- mvn(data = D)
result$multivariateNormality
# The p-value isn't very high (but I don't reject for levels 5%, 1%).
plot(D)
n <- dim(D)[1]
p <- dim(D)[2]
D.mean   <- sapply(D, mean)
D.cov    <- cov(D)
D.invcov <- solve(D.cov)
alpha   <- .05
delta.0 <- c(0, 0)
D.T2 <- n * (D.mean - delta.0) %*% D.invcov %*% (D.mean - delta.0)
D.T2
cfr.fisher <- ((n - 1) * p / (n - p)) * qf(1 - alpha, p, n - p)
cfr.fisher
D.T2 < cfr.fisher # FALSE: we reject H0 at level 5%
# we compute the p-value
# вероятность, что наше значение лежит правее квантиля 1-aplha
# same thing if we use lower.tail = FALSE
# pf(D.T2 * (n - p) / (p * (n - 1)), p, n - p, lower.tail = FALSE)
P <- 1 - pf(D.T2 * (n - p) / (p * (n - 1)), p, n - p)
P
# Ellipsoidal confidence region with confidence level 95%
plot(D, asp=1, pch=1, main='Dataset of the Differences', ylim=c(-15,60))
ellipse(center=D.mean, shape=D.cov/n, radius=sqrt(cfr.fisher), lwd=2)
# Adding delta.0 and the quadrants
points(delta.0[1], delta.0[2], pch=16, col='grey35', cex=1.5)
abline(h=delta.0[1], v=delta.0[2], col='grey35')
# Ellipsoidal confidence region with confidence level 99%
ellipse(
center = D.mean,
shape = D.cov / n,
radius = sqrt((n - 1) * p / (n - p) * qf(1 - 0.01, p, n - p)),
lty = 2,
col = 'grey',
lwd = 2
)
# What if we set the radius as the quantile of order 1-pval?
ellipse(
center = D.mean,
shape = D.cov / n,
radius = sqrt((n - 1) * p / (n - p) * qf(1 - as.numeric(P), p, n - p)),
lty = 1,
col = 'dark grey',
lwd = 2
)
# Simultaneous T2 intervals in the direction of DBOD and DSS
IC.T2.DBOD <-
c(D.mean[1] - sqrt(cfr.fisher * D.cov[1, 1] / n),
D.mean[1],
D.mean[1] + sqrt(cfr.fisher * D.cov[1, 1] / n))
IC.T2.DSS  <-
c(D.mean[2] - sqrt(cfr.fisher * D.cov[2, 2] / n),
D.mean[2],
D.mean[2] + sqrt(cfr.fisher * D.cov[2, 2] / n))
T2 <- rbind(IC.T2.DBOD, IC.T2.DSS)
dimnames(T2)[[2]] <- c('inf', 'center', 'sup')
T2
# Plot of the simultaneous T2 intervals
plot.simT2 <- function() {
plot(D, asp=1, pch=1, main='Dataset of the Differences',ylim=c(-15,60))
ellipse(center=D.mean, shape=D.cov/n, radius=sqrt(cfr.fisher), lwd=2, col='grey')
abline(v = T2[1,1], col='red', lwd=1, lty=2)
abline(v = T2[1,3], col='red', lwd=1, lty=2)
abline(h = T2[2,1], col='red', lwd=1, lty=2)
abline(h = T2[2,3], col='red', lwd=1, lty=2)
points(delta.0[1], delta.0[2], pch=16, col='grey35', cex=1.5)
abline(h=delta.0[1], v=delta.0[2], col='grey35')
segments(IC.T2.DBOD[1], 0, IC.T2.DBOD[3], 0, lty=1, lwd=2, col='red')
segments(0, IC.T2.DSS[1], 0, IC.T2.DSS[3], lty=1,lwd=2, col='red')
}
plot.simT2()
# => From the theory
#    - the maximum is realized (Hotelling T2-statistics)
D.T2
#    - the distribution of the maximum is known
#    - the direction along which the maximum is realized is known
worst <- D.invcov %*% (D.mean - delta.0)
worst <- worst / sqrt(sum(worst ^ 2)) # Normalization
worst
# Confidence interval along the worst direction:
IC.worst  <- c(D.mean %*% worst - sqrt(cfr.fisher*(t(worst) %*% D.cov %*% worst) / n),
D.mean %*% worst,
D.mean %*% worst + sqrt(cfr.fisher*(t(worst) %*% D.cov %*% worst) / n))
IC.worst
delta.0 %*% worst
(IC.worst[1] < delta.0 %*% worst) & (delta.0 %*% worst < IC.worst[3])
# Extremes of IC.worst in the coordinate system (x,y):
x.min <- IC.worst[1] * worst # (x,y) coords of the lower bound of the interval
x.max <- IC.worst[3] * worst # (x,y) coords of the upper bound of the interval
m1.ort <- - worst[1] / worst[2] # Slope of the line orthogonal to worst
q.min.ort <- x.min[2] - m1.ort * x.min[1] # Intercept of line of slope m1.ort
# passing by x.min
q.max.ort <- x.max[2] - m1.ort * x.max[1] # Intercept of line of slope m1.ort
# passing by x.max
abline(q.min.ort, m1.ort, col='forestgreen', lty=2,lwd=1)
abline(q.max.ort, m1.ort, col='forestgreen', lty=2,lwd=1)
m1 = worst[2] / worst[1] # worst direction
abline(0, m1, col='grey35') # Intercept 0 because the line has to pass
# by delta.0 which is (0, 0)
segments(x.min[1], x.min[2], x.max[1], x.max[2], lty=1, lwd=2, col='forestgreen')
# If we are not convinced yet, let's look at all the directions:
# we compute confidence intervals for a'x where a varies in all the
# directions between 0 and pi, with step pi/180. For each direction
# we compute the T2 statistic (univariate)
D <- as.matrix(D)
theta   <- seq(0, pi - pi/180, by = pi/180)
T2.d     <- NULL
Centerf  <- NULL
Maxf     <- NULL
Minf     <- NULL
for(i in 1:length(theta))
{
a   <- c(cos(theta[i]), sin(theta[i]))
t2  <- ( mean(D %*% a) - (delta.0 %*% a) )^2 / ( var(D %*% a) / n )
T2.d  <- c(T2.d, t2)
centerf  <- D.mean %*% a
maxf     <- D.mean %*% a + sqrt( t(a) %*% D.cov%*% a / n) * sqrt(cfr.fisher)
minf     <- D.mean %*% a - sqrt( t(a) %*% D.cov%*% a / n) * sqrt(cfr.fisher)
Centerf  <- c(Centerf, centerf)
Maxf     <- c(Maxf, maxf)
Minf     <- c(Minf, minf)
}
plot.simT2.ci <- function() {
par(mfrow=c(1,3))
# Scatter plot of DSS and DBOD
plot(D, asp=1, pch=1, main='Dataset of the Differences', ylim=c(-15,60))
# Adding the quadrants
abline(h=delta.0[1], v=delta.0[2], col='red', lty=3)
# Adding the 95% confidence region for the true mean
ellipse(center=D.mean, shape=D.cov/n, radius=sqrt(cfr.fisher), lwd=2, col='grey')
# Adding the simultaneous confidence intervals in the direction of DSS and DBOD
segments(IC.T2.DBOD[1],0,IC.T2.DBOD[3],0,lty=1,lwd=2,col='red')
segments(0,IC.T2.DSS[1],0,IC.T2.DSS[3],lty=1,lwd=2,col='red')
# Adding the simultaneous confidence interval in the "worst" direction
x.min <- IC.worst[1] * worst
x.max <- IC.worst[3] * worst
segments(x.min[1], x.min[2], x.max[1], x.max[2], lty=1, lwd=2, col='forestgreen')
abline(0, m1, col='forestgreen', lty=3)
points(delta.0[1], delta.0[2], pch=16, col='black')
# Second plot with the confidence intervals for each direction tested
plot(theta, Centerf, main = 'Simultaneous T2 confidence intervals', ylim = c(-30,35), col = 'grey25', type='l',ylab='IC')
for(i in 1:length(theta))
lines(c(theta[i], theta[i]), c(Minf[i], Maxf[i]), col = 'grey75')
abline(h=0, col='black')
lines(theta, Minf, col = 'red', lty = 2)
lines(theta, Maxf, col = 'red', lty = 2)
# Highlighting the directions of DSS and DBOD (theta = 0 and theta = pi/2)
lines(c(theta[1], theta[1]), c(Minf[1], Maxf[1]), col = 'red', lwd=2)
lines(c(theta[91], theta[91]), c(Minf[91], Maxf[91]), col = 'red', lwd=2)
# Highlighting the "worst" direction for the rejection of H0
lines(c(theta[which.max(T2.d)], theta[which.max(T2.d)]),
c(Minf[which.max(T2.d)], Maxf[which.max(T2.d)]), col = 'forestgreen', lwd=2)
# Third plot with the value of T2 statistic
plot(theta, T2.d, main = 'T2 statistics', ylim = c(0,15), col = 'blue', type='l')
abline(v=c(0,pi/2), col = 'red', lty = 3)
abline(v=theta[which.max(T2.d)], col = 'forestgreen', lty = 3)
# Drawing the threshold given by the Fisher quantile at level .95
abline(h=cfr.fisher, col = 'grey', lty = 1, lwd=2)
}
plot.simT2.ci()
allergy <- read.table('hatingalmonds.txt')
head(allergy)
dim(allergy)
noallergy <- read.table('lovingalmonds.txt')
head(noallergy)
dim(noallergy)
n1 <- dim(allergy)[1]
n2 <- dim(noallergy)[1]
p <- dim(noallergy)[2]
x.mean1 <- sapply(allergy, mean)
x.mean2 <- sapply(noallergy, mean)
p.hat <- (x.mean1 * n1 + x.mean2 * n2) / (n1 + n2) #
x.var <- (p.hat * (1 - p.hat)) # poold variance
z.i <- (x.mean1 - x.mean2) / sqrt(x.var * (1 / n1 + 1 / n2))
p.i <- ifelse(z.i < 0, 2 * pnorm(z.i), 2 * (1 - pnorm(z.i)))
which(p.i<.01) # but we also could make type-1 error, and wrong reject,
# Bonferroni (control the family-wise error rate)
k <- 520
which(p.i*k<.01)
# or
p.Bf <- p.adjust(p.i, method='bonferroni')
which(p.Bf<.01)
# Benjamini-Hockberg (control the false discovery rate) (better)
p.BH <- p.adjust(p.i, method='BH')
which(p.BH<.01)
par(mfrow=c(1,3))
plot(p.i, main='Univariate')
abline(h=.01, lwd=2, col='red')
plot(p.Bf, main='Corrected - Bonferroni')
abline(h=.01, lwd=2, col='red')
plot(p.BH, main='Corrected - BH')
abline(h=.01, lwd=2, col='red')
x.mean1 <- sapply(allergy, mean)
x.mean2 <- sapply(noallergy, mean)
p.hat <- (x.mean1 * n1 + x.mean2 * n2) / (n1 + n2) #
x.var <- (p.hat * (1 - p.hat)) # pooled variance
p.hat
x.var
p.hat
(1 - p.hat)
p.hat
allergy
allergy <- read.table('hatingalmonds.txt')
head(allergy)
dim(allergy)
noallergy <- read.table('lovingalmonds.txt')
head(noallergy)
dim(noallergy)
n1 <- dim(allergy)[1]
n2 <- dim(noallergy)[1]
p <- dim(noallergy)[2]
x.mean1 <- sapply(allergy, mean)
x.mean2 <- sapply(noallergy, mean)
p.hat <- (x.mean1 * n1 + x.mean2 * n2) / (n1 + n2) #
x.var <- (p.hat * (1 - p.hat)) # pooled variance
z.i <- (x.mean1 - x.mean2) / sqrt(x.var * (1 / n1 + 1 / n2))
p.i <- ifelse(z.i < 0, 2 * pnorm(z.i), 2 * (1 - pnorm(z.i)))
which(p.i<.01) # but we also could make type-1 error, and wrong reject,
