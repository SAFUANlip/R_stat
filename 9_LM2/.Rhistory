c(beta0=as.numeric(beta0),beta1=as.numeric(beta1),beta2=as.numeric(beta2))
fm$coefficients
fmp.pc$coefficients
fm.pc$coefficients
fm$coefficients
fm.pc
plot(sp1.pc,distance, xlab='PC1', ylab='Stopping distance', las=1, xlim=c(-250,361), ylim=c(-5,130))
x <- seq(-250,361,by=1)
b <- coef(fm.pc)
lines(x, b[1]+b[2]*x)
plot(speed1,distance, ylab='Stopping distance', las=1, ylim=c(-5,130))
x <- seq(0,25,by=1)
lines(x, beta0 + beta1*x + beta2*x^2)
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(fm.pc)
shapiro.test(residuals(fm.pc))
fm.2 <- lm(distance ~ speed2)
summary(fm.2)
# Note: when using as regressors polynomials of the same variable (i.e., in
# the framework of polynomial regression), some authors recommend instead to
# keep all the orders lower than the maximum that one wants to keep (e.g., if
# keeping the maximum order 3, they recommend keeping also all the terms
# of order 2 and 1, for a "hierarchy" principle)
par(mfrow=c(1,2))
plot(speed2, distance, xlab='speed2', ylab='Stopping distance', las=1, xlim=c(15,626), ylim=c(-5,130))
x <- seq(0,650,by=1)
b <- coef(fm.2)
lines(x, b[1]+b[2]*x)
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-5,130))
x <- seq(0,30,by=0.1)
b <- coef(fm.2)
lines(x, b[1]+b[2]*x^2)
dev.off()
### Ridge regression ---------------------------------------------------------------------------
help(lm.ridge)
### Ridge regression ---------------------------------------------------------------------------
help(lm.ridge)
# Fix lambda
lambda <- .5
fit.ridge <- lm.ridge(distance ~ speed1 + speed2, lambda = lambda)
coef.ridge <- coef(fit.ridge)
yhat.lm <- cbind(rep(1,n), speed1, speed2)%*%coef(fm)  # LM fitted values
yhat.r <- cbind(rep(1,n), speed1, speed2)%*%coef.ridge # ridge fitted values
par(mfrow=c(1,1))
plot(speed1, yhat.lm, type='l', lty=4, lwd=2, ylab='Distance',xlab='Speed')
points(speed1, distance, pch=1, cex=.8)
matlines(speed1, yhat.r, type='l', lty=1,col=grey.colors(length(lambda)), lwd=2)
legend("topleft",c("lm","ridge"),lty=c(4,1),col=c("black",grey.colors(length(lambda))),lwd=2)
# Repeat for a grid of lambda's
lambda.c <- seq(0,10,0.01)
fit.ridge <- lm.ridge(distance ~ speed1 + speed2, lambda = lambda.c)
par(mfrow=c(1,3))
plot(lambda.c,coef(fit.ridge)[,1], type='l', xlab=expression(lambda),
ylab=expression(beta[0]))
abline(h=coef(fm)[1], lty=2)
plot(lambda.c,coef(fit.ridge)[,2], type='l', xlab=expression(lambda),
ylab=expression(beta[1]))
abline(h=coef(fm)[2], lty=2)
plot(lambda.c,coef(fit.ridge)[,3], type='l', xlab=expression(lambda),
ylab=expression(beta[2]))
abline(h=coef(fm)[3], lty=2)
yhat.lm <- cbind(rep(1,n), speed1, speed2)%*%coef(fm)
par(mfrow=c(1,1))
plot(speed1, yhat.lm, type='l', lty=1, lwd=2, ylab='Distance',
xlab='Speed')
points(speed1, distance, pch=1, cex=.8)
yhat.r <- NULL
for(i in 1:length(lambda.c))
yhat.r=cbind(yhat.r, cbind(rep(1,n), speed1, speed2)%*%coef(fit.ridge)[i,])
matlines(speed1, yhat.r, type='l', lty=1,
col=grey.colors(length(lambda.c)))
lines(speed1, yhat.lm, type='l', lty=4, lwd=2, ylab='Distance',
xlab='Speed')
# Choice of the optimal lambda, e.g., via cross-validation
# GCV -> Generalized Cross Validation
select(fit.ridge)
fit.ridge
# Choice of the optimal lambda, e.g., via cross-validation
# GCV -> Generalized Cross Validation # approximation of leave one out cross validation
select(fit.ridge)
# or
lambda.opt <- lambda.c[which.min(fit.ridge$GCV)]
lambda.opt
par(mfrow=c(1,3))
plot(lambda.c,coef(fit.ridge)[,1], type='l', xlab=expression(lambda),
ylab=expression(beta[0]))
abline(h=coef(fm)[1], lty=1, col='grey')
abline(v=lambda.opt, col=2, lty=2)
plot(lambda.c,coef(fit.ridge)[,2], type='l', xlab=expression(lambda),
ylab=expression(beta[1]))
abline(h=coef(fm)[2], lty=1, col='grey')
abline(v=lambda.opt, col=2, lty=2)
plot(lambda.c,coef(fit.ridge)[,3], type='l', xlab=expression(lambda),
ylab=expression(beta[2]))
abline(h=coef(fm)[3], lty=1, col='grey')
abline(v=lambda.opt, col=2, lty=2)
par(mfrow=c(1,1))
plot(speed1, distance, pch=1, cex=.8, ylab='Distance',
xlab='Speed')
matlines(speed1, yhat.r, type='l', lty=1,
col=grey.colors(length(lambda.c)))
lines(speed1, yhat.lm, type='l', lty=4, lwd=2, ylab='Distance',
xlab='Speed')
lines(speed1, yhat.r[,which.min(fit.ridge$GCV)], type='l', lty=1, lwd=2,
col=2, ylab='Distance', xlab='Speed')
legend("topleft", c('LM', 'Ridge opt.' ), lty=c(4,1), col=c(1,2), lwd=2)
coef.ridge <- coef(fit.ridge)[which.min(fit.ridge$GCV),]
coef.ridge
### Lasso regression ---------------------------------------------------------------------------
help(glmnet)
# Build the matrix of predictors
x <- model.matrix(distance~speed1+speed2)[,-1]
# Build the vector of response
y <- distance
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) # default: alpha=1 -> lasso
# [note: if alpha=0 -> ridge regression]
par(mfrow=c(1,1))
plot(fit.lasso, xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
lambda.grid
# Let's set lambda via cross validation
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid) # default: 10-fold CV
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
optlam.lasso <- cv.lasso$lambda.1se
optlam.lasso
plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)
abline(v=log(optlam.lasso), lty=1)
## Example 2: Dataset concrete --------------------------------------------------------------
data <- read.table('concrete.txt', header=T)
head(data)
dim(data)
names(data)
pairs(data)
attach(data)
y <- Hardness_concrete
par(mfrow=c(2,2))
plot(Alluminium, y, main='Hardness vs Alluminium', lwd=2,
xlab='Alluminium', ylab='Hardness concrete')
plot(Silicate, y, main='Hardness vs Silicate', lwd=2,
xlab='Silicate', ylab='Hardness concrete')
plot(Alluminium_ferrite, y, main='Hardness vs Alluminium ferrite', lwd=2,
xlab='Alluminium ferrite', ylab='Hardness concrete')
plot(Silicate_bicalcium, y, main='Hardness vs Silicate bicalcium', lwd=2,
xlab='Silicate bicalcium', ylab='Hardness concrete')
# Multiple linear regression
result <- lm(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)
summary(result)
vif(result)
# PCA regression
result.pc <- princomp(cbind(Alluminium,Silicate,Alluminium_ferrite,Silicate_bicalcium), scores=TRUE)
summary(result.pc)
result.pc$load
par(mfrow=c(2,2))
plot(Alluminium, y, main='Hardness vs Alluminium', lwd=2,
xlab='Alluminium', ylab='Hardness concrete')
plot(Silicate, y, main='Hardness vs Silicate', lwd=2,
xlab='Silicate', ylab='Hardness concrete')
plot(Alluminium_ferrite, y, main='Hardness vs Alluminium ferrite', lwd=2,
xlab='Alluminium ferrite', ylab='Hardness concrete')
plot(Silicate_bicalcium, y, main='Hardness vs Silicate bicalcium', lwd=2,
xlab='Silicate bicalcium', ylab='Hardness concrete')
# Multiple linear regression
result <- lm(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)
summary(result)
vif(result)
# PCA regression
result.pc <- princomp(cbind(Alluminium,Silicate,Alluminium_ferrite,Silicate_bicalcium), scores=TRUE)
summary(result.pc)
result.pc$load
# Explained variance
layout(matrix(c(2,3,1,3),2,byrow=T))
barplot(result.pc$sdev^2, las=2, main='Principal Components', ylab='Variances')
barplot(c(sd(Alluminium),sd(Silicate),sd(Alluminium_ferrite),sd(Silicate_bicalcium))^2, las=2, main='Original variables', ylab='Variances')
plot(cumsum(result.pc$sdev^2)/sum(result.pc$sde^2), type='b', axes=F, xlab='number of components', ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.9, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:4,labels=1:4,las=2)
# Loadings
par(mfrow = c(4,1))
for(i in 1:4)barplot(result.pc$load[,i], ylim = c(-1, 1))
par(mfrow=c(1,1))
# Dimensionality reduction: select first two PCs:
pc1 <- result.pc$scores[,1]
pc2 <- result.pc$scores[,2]
# Now we estimate the model using the first two PCs as regressors.
# Model: y = b0 + b1*PC1+ b2*PC2 + eps, eps~N(0,sigma^2)
fm.pc <- lm(y ~ pc1 + pc2)
summary(fm.pc)
# y= b0 + b1*PC1 + b2*PC2 + eps =
#  = b0 + b1*(e11*(Alluminium-m1)+e21*(Silicate-m2)+e31*(Alluminium_ferrite-m3)+e41*(Silicate_bicalcium-m4)) +
#       + b2*(e12*(Alluminium-m1)+e22*(Silicate-m2)+e32*(Alluminium_ferrite-m3)+e42*(Silicate_bicalcium-m4)) + eps =
#  = b0 - b1*e11*m1 - b2*e12*m1 - b1*e21*m2 - b2*e22*m2 +
#       - b1*e31*m3 - b2*e32*m3 - b1*e41*m4 - b2*e42*m4 +
#       + (b1*e11+b2*e12)*Alluminium + (b1*e21+b2*e22)*Silicate +
#       + (b1*e31+b2*e32)*Alluminium_ferrite + (b1*e41+b2*e42)*Silicate_bicalcium + eps
# where e.ij are the loadings, i=1,2,3,4, j=1,2.
# => We can compute the coefficients of the model which used the original
#    regressors
m1 <- mean(Alluminium)
m2 <- mean(Silicate)
m3 <- mean(Alluminium_ferrite)
m4 <- mean(Silicate_bicalcium)
beta0 <- coefficients(fm.pc)[1] -
coefficients(fm.pc)[2]*result.pc$load[1,1]*m1 -
coefficients(fm.pc)[3]*result.pc$load[1,2]*m1 -
coefficients(fm.pc)[2]*result.pc$load[2,1]*m2 -
coefficients(fm.pc)[3]*result.pc$load[2,2]*m2 -
coefficients(fm.pc)[2]*result.pc$load[3,1]*m3 -
coefficients(fm.pc)[3]*result.pc$load[3,2]*m3 -
coefficients(fm.pc)[2]*result.pc$load[4,1]*m4 -
coefficients(fm.pc)[3]*result.pc$load[4,2]*m4
beta1 <- coefficients(fm.pc)[2]*result.pc$load[1,1] +
coefficients(fm.pc)[3]*result.pc$load[1,2]
beta2 <- coefficients(fm.pc)[2]*result.pc$load[2,1] +
coefficients(fm.pc)[3]*result.pc$load[2,2]
beta3 <- coefficients(fm.pc)[2]*result.pc$load[3,1] +
coefficients(fm.pc)[3]*result.pc$load[3,2]
beta4 <- coefficients(fm.pc)[2]*result.pc$load[4,1] +
coefficients(fm.pc)[3]*result.pc$load[4,2]
c(beta0=as.numeric(beta0),beta1=as.numeric(beta1),beta2=as.numeric(beta2),beta3=as.numeric(beta3),beta4=as.numeric(beta4))
result$coefficients
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(fm.pc)
par(mfrow=c(1,1))
shapiro.test(residuals(fm.pc))
# Ridge and Lasso regression with glmnet
x <- model.matrix(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)[,-1] # matrix of predictors
y <- y # vector of response
lambda.grid <- 10^seq(5,-3,length=50)
# Ridge regression
fit.ridge <- glmnet(x,y, lambda = lambda.grid, alpha=0) # alpha=0 -> ridge
plot(fit.ridge,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
shapiro.test(residuals(fm.pc))
# Ridge and Lasso regression with glmnet
x <- model.matrix(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)[,-1] # matrix of predictors
y <- y # vector of response
lambda.grid <- 10^seq(5,-3,length=50)
# Ridge regression
fit.ridge <- glmnet(x,y, lambda = lambda.grid, alpha=0) # alpha=0 -> ridge
plot(fit.ridge,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
norm_l2 <- NULL
for(i in 1:50)
norm_l2 <- c(norm_l2,sqrt(sum((fit.ridge$beta[,i])^2)))
plot(log(lambda.grid),norm_l2)
# Let's set lambda via CV
set.seed(1)
cv.ridge <- cv.glmnet(x,y,alpha=0,nfolds=3,lambda=lambda.grid)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
optlam.ridge <- cv.ridge$lambda.1se
optlam.ridge
plot(cv.ridge)
abline(v=log(bestlam.ridge), lty=1)
abline(v=log(optlam.ridge), lty=1)
# Get the coefficients for the optimal lambda
coef.ridge <- predict(fit.ridge, s=bestlam.ridge, type = 'coefficients')[1:5,]
coef.ridge
# Ridge regression
fit.ridge <- glmnet(x,y, lambda = lambda.grid, alpha=0) # alpha=0 -> ridge
plot(fit.ridge,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
norm_l2 <- NULL
for(i in 1:50)
norm_l2 <- c(norm_l2,sqrt(sum((fit.ridge$beta[,i])^2)))
plot(log(lambda.grid),norm_l2)
# Let's set lambda via CV
set.seed(1)
cv.ridge <- cv.glmnet(x,y,alpha=0,nfolds=3,lambda=lambda.grid)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
optlam.ridge <- cv.ridge$lambda.1se
optlam.ridge
plot(cv.ridge)
abline(v=log(bestlam.ridge), lty=1)
abline(v=log(optlam.ridge), lty=1)
# Get the coefficients for the optimal lambda
coef.ridge <- predict(fit.ridge, s=bestlam.ridge, type = 'coefficients')[1:5,]
coef.ridge
plot(fit.ridge,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
abline(v=log(bestlam.ridge))
abline(v=log(optlam.ridge))
# Lasso regression
fit.lasso <- glmnet(x,y, lambda = lambda.grid, alpha=1) # alpha=1 -> lasso
plot(fit.lasso,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
norm_l1 <- NULL
for(i in 1:50)
norm_l1 <- c(norm_l1,sum(abs(fit.ridge$beta[,i])))
plot(log(lambda.grid),norm_l1)
# Let's set lambda via CV
set.seed(1)
cv.lasso <- cv.glmnet(x,y,alpha=1,nfolds=3,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)
# Get the coefficients for the optimal lambda
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')[1:5,]
coef.lasso
plot(fit.lasso,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
abline(v=log(bestlam.lasso))
# Compare coefficients estimates for LS, Ridge and Lasso
plot(rep(0, dim(x)[2]), coef(lm(y~x))[-1], col=rainbow(dim(x)[2]), pch=20, xlim=c(-1,3), ylim=c(-1,2), xlab='', ylab=expression(beta),
axes=F)
points(rep(1, dim(x)[2]), coef.ridge[-1], col=rainbow(dim(x)[2]), pch=20)
points(rep(2, dim(x)[2]), coef.lasso[-1], col=rainbow(dim(x)[2]), pch=20)
abline(h=0, col='grey41', lty=1)
box()
axis(2)
axis(1, at=c(0,1,2), labels = c('LS', 'Ridge', 'Lasso'))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), pch=20, cex=1)
# l2 norm
sqrt(sum((coef(lm(y~x))[-1])^2)) # LS
sqrt(sum((coef.ridge[-1])^2))    # ridge
# l1 norm
sum(abs(coef(lm(y~x))[-1])) # LS
sum(abs(coef.lasso[-1]))    # lasso
# Variable selection
result <- lm(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)
summary(result)
result1 <- lm(y ~ Alluminium + Alluminium_ferrite + Silicate_bicalcium)
summary(result1)
result2 <- lm(y ~ Alluminium + Silicate_bicalcium)
summary(result2)
# alternatively
linearHypothesis(result, rbind(c(0,0,1,0,0),c(0,0,0,1,0)),c(0,0))
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(result2)
# alternatively
linearHypothesis(result, rbind(c(0,0,1,0,0),c(0,0,0,1,0)),c(0,0))
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(result2)
shapiro.test(residuals(result2))
library(ISLR)
# Hitters dataset
help(Hitters)
names(Hitters)
dim(Hitters)
# remove NA's
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
#  Subset Selection Methods
library(leaps)
help(regsubsets)
# Best Subset Selection: Exhaustive Search
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
# Best Subset Selection: Exhaustive Search
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
regfit.full <- regsubsets(Salary~., data=Hitters, nvmax=19)
summary(regfit.full)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$which
reg.summary$rsq   # r-squared
reg.summary$adjr2 # adjusted r-squared
reg.summary$rss   # residual sum of squares
par(mfrow=c(1,3))
plot(reg.summary$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
# extract coefficient estimates associated with the models
which.max(reg.summary$adjr2)
coef(regfit.full,11)
coef(regfit.full,6)
# Forward and Backward Stepwise Selection
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.fwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.fwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
par(mfrow=c(1,3))
plot(summary(regfit.bwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.bwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.bwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
coef(regfit.full,7) # Exhaustive search
coef(regfit.fwd,7)  # Forward Stepwise Selection
coef(regfit.bwd,7)  # Backward Stepwise Selection
# Forward and Backward Stepwise Selection
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
# Forward and Backward Stepwise Selection
# only add variables
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.fwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.fwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
folds
table(folds)
# function that performs the prediction for regsubsets
predict.regsubsets <- function(object, newdata, id) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k) {
best.fit <- regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19) {
pred <- predict.regsubsets(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i] <- mean( (Hitters$Salary[folds==j]-pred)^2 )
}
}
cv.errors
root.mean.cv.errors <- sqrt(apply(cv.errors,2,mean)) # average over the columns
root.mean.cv.errors
plot(root.mean.cv.errors,type='b')
which.min(root.mean.cv.errors)
points(which.min(root.mean.cv.errors),root.mean.cv.errors[which.min(root.mean.cv.errors)], col='red',pch=19)
# estimation on the full dataset
reg.best <- regsubsets(Salary~., data=Hitters, nvmax=19)
coef(reg.best,10)
# k-fold cross validation after model selection (WRONG WAY!)
best.fit <- regsubsets(Salary~., data=Hitters, nvmax=19)
summary(best.fit)
cv.errors_wrong <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k)
for(i in 1:19) {
covariate <- which(summary(best.fit)$which[i,-c(1,19)])
mod <- lm(Salary~., data=Hitters[folds!=j, c(covariate,19)])
pred <- predict(mod,Hitters)[folds==j]
cv.errors_wrong[j,i] <- mean( (Hitters$Salary[folds==j]-pred)^2)
}
cv.errors_wrong
root.mean.cv.errors_wrong <- sqrt(apply(cv.errors_wrong,2,mean)) # average over the columns
root.mean.cv.errors_wrong
cv.errors_wrong
root.mean.cv.errors_wrong <- sqrt(apply(cv.errors_wrong,2,mean)) # average over the columns
root.mean.cv.errors_wrong
plot(root.mean.cv.errors_wrong,type='b')
which.min(root.mean.cv.errors_wrong)
points(which.min(root.mean.cv.errors_wrong),root.mean.cv.errors_wrong[which.min(root.mean.cv.errors_wrong)], col='red',pch=19)
points(root.mean.cv.errors,type='b',col='blue')
# Ridge and Lasso regression with glmnet
x <- model.matrix(Salary~.,Hitters)[,-1] # predictor matrix
y <- Hitters$Salary # response
grid <- 10^seq(10,-2,length=100) # grid of lambda
ridge.mod <- glmnet(x,y,alpha=0,lambda=grid)
plot(ridge.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=0,nfold=3,lambda=grid)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
log(bestlam.ridge)
abline(v=log(bestlam.ridge))
plot(ridge.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.ridge))
lasso.mod <- glmnet(x,y,alpha=1,lambda=grid)
plot(lasso.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=1,nfold=3,lambda=grid)
plot(cv.out)
plot(lasso.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=1,nfold=3,lambda=grid)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
log(bestlam.lasso)
abline(v=log(bestlam.lasso))
plot(lasso.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.lasso))
coef.ridge <- predict(ridge.mod, s=bestlam.ridge, type = 'coefficients')[1:20,]
coef.lasso <- predict(lasso.mod, s=bestlam.lasso, type = 'coefficients')[1:20,]
coef.ridge
coef.lasso
plot(rep(0, dim(x)[2]), coef(lm(y~x))[-1], col=rainbow(dim(x)[2]), pch=20, xlim=c(-1,3), ylim=c(-1,2), xlab='', ylab=expression(beta),
axes=F)
points(rep(1, dim(x)[2]), coef.ridge[-1], col=rainbow(dim(x)[2]), pch=20)
points(rep(2, dim(x)[2]), coef.lasso[-1], col=rainbow(dim(x)[2]), pch=20)
abline(h=0, col='grey41', lty=1)
box()
axis(2)
axis(1, at=c(0,1,2), labels = c('LS', 'Ridge', 'Lasso'))
