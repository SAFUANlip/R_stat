# Hitters dataset
help(Hitters)
names(Hitters)
dim(Hitters)
# remove NA's
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
#  Subset Selection Methods
library(leaps)
help(regsubsets)
# Best Subset Selection: Exhaustive Search
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
# Best Subset Selection: Exhaustive Search
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
regfit.full <- regsubsets(Salary~., data=Hitters, nvmax=19)
summary(regfit.full)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$which
reg.summary$rsq   # r-squared
reg.summary$adjr2 # adjusted r-squared
reg.summary$rss   # residual sum of squares
par(mfrow=c(1,3))
plot(reg.summary$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
# extract coefficient estimates associated with the models
which.max(reg.summary$adjr2)
coef(regfit.full,11)
coef(regfit.full,6)
# Forward and Backward Stepwise Selection
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.fwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.fwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
par(mfrow=c(1,3))
plot(summary(regfit.bwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.bwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.bwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
coef(regfit.full,7) # Exhaustive search
coef(regfit.fwd,7)  # Forward Stepwise Selection
coef(regfit.bwd,7)  # Backward Stepwise Selection
# Forward and Backward Stepwise Selection
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
# Forward and Backward Stepwise Selection
# only add variables
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.fwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.fwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
folds
table(folds)
# function that performs the prediction for regsubsets
predict.regsubsets <- function(object, newdata, id) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k) {
best.fit <- regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19) {
pred <- predict.regsubsets(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i] <- mean( (Hitters$Salary[folds==j]-pred)^2 )
}
}
cv.errors
root.mean.cv.errors <- sqrt(apply(cv.errors,2,mean)) # average over the columns
root.mean.cv.errors
plot(root.mean.cv.errors,type='b')
which.min(root.mean.cv.errors)
points(which.min(root.mean.cv.errors),root.mean.cv.errors[which.min(root.mean.cv.errors)], col='red',pch=19)
# estimation on the full dataset
reg.best <- regsubsets(Salary~., data=Hitters, nvmax=19)
coef(reg.best,10)
# k-fold cross validation after model selection (WRONG WAY!)
best.fit <- regsubsets(Salary~., data=Hitters, nvmax=19)
summary(best.fit)
cv.errors_wrong <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k)
for(i in 1:19) {
covariate <- which(summary(best.fit)$which[i,-c(1,19)])
mod <- lm(Salary~., data=Hitters[folds!=j, c(covariate,19)])
pred <- predict(mod,Hitters)[folds==j]
cv.errors_wrong[j,i] <- mean( (Hitters$Salary[folds==j]-pred)^2)
}
cv.errors_wrong
root.mean.cv.errors_wrong <- sqrt(apply(cv.errors_wrong,2,mean)) # average over the columns
root.mean.cv.errors_wrong
cv.errors_wrong
root.mean.cv.errors_wrong <- sqrt(apply(cv.errors_wrong,2,mean)) # average over the columns
root.mean.cv.errors_wrong
plot(root.mean.cv.errors_wrong,type='b')
which.min(root.mean.cv.errors_wrong)
points(which.min(root.mean.cv.errors_wrong),root.mean.cv.errors_wrong[which.min(root.mean.cv.errors_wrong)], col='red',pch=19)
points(root.mean.cv.errors,type='b',col='blue')
# Ridge and Lasso regression with glmnet
x <- model.matrix(Salary~.,Hitters)[,-1] # predictor matrix
y <- Hitters$Salary # response
grid <- 10^seq(10,-2,length=100) # grid of lambda
ridge.mod <- glmnet(x,y,alpha=0,lambda=grid)
plot(ridge.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=0,nfold=3,lambda=grid)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
log(bestlam.ridge)
abline(v=log(bestlam.ridge))
plot(ridge.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.ridge))
lasso.mod <- glmnet(x,y,alpha=1,lambda=grid)
plot(lasso.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=1,nfold=3,lambda=grid)
plot(cv.out)
plot(lasso.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=1,nfold=3,lambda=grid)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
log(bestlam.lasso)
abline(v=log(bestlam.lasso))
plot(lasso.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.lasso))
coef.ridge <- predict(ridge.mod, s=bestlam.ridge, type = 'coefficients')[1:20,]
coef.lasso <- predict(lasso.mod, s=bestlam.lasso, type = 'coefficients')[1:20,]
coef.ridge
coef.lasso
plot(rep(0, dim(x)[2]), coef(lm(y~x))[-1], col=rainbow(dim(x)[2]), pch=20, xlim=c(-1,3), ylim=c(-1,2), xlab='', ylab=expression(beta),
axes=F)
points(rep(1, dim(x)[2]), coef.ridge[-1], col=rainbow(dim(x)[2]), pch=20)
points(rep(2, dim(x)[2]), coef.lasso[-1], col=rainbow(dim(x)[2]), pch=20)
abline(h=0, col='grey41', lty=1)
box()
axis(2)
axis(1, at=c(0,1,2), labels = c('LS', 'Ridge', 'Lasso'))
### Lasso regression ---------------------------------------------------------------------------
help(glmnet)
# Build the matrix of predictors
x <- model.matrix(distance~speed1+speed2)[,-1]
# Build the vector of response
y <- distance
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) # default: alpha=1 -> lasso
### Lasso regression ---------------------------------------------------------------------------
help(glmnet)
library(glmnet) # to use LASSO
### Lasso regression ---------------------------------------------------------------------------
help(glmnet)
# Build the matrix of predictors
x <- model.matrix(distance~speed1+speed2)[,-1]
# Build the vector of response
y <- distance
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) # default: alpha=1 -> lasso
# [note: if alpha=0 -> ridge regression] # aplha = 1 -> lasso, 0 < alpha < 1 -> elastic
par(mfrow=c(1,1))
plot(fit.lasso, xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
# Let's set lambda via cross validation
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid) # default: 10-fold CV
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
optlam.lasso <- cv.lasso$lambda.1se
optlam.lasso
plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)
abline(v=log(optlam.lasso), lty=1) # optimal lambda, penalysing model, but still having good MSE
# Get the coefficients for the optimal lambda
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')[1:3,]
coef.lasso
coef.lasso <- predict(fit.lasso, s=optlam.lasso, type = 'coefficients')[1:3,]
coef.lasso
### Lasso regression ---------------------------------------------------------------------------
help(glmnet)
# Build the matrix of predictors
x <- model.matrix(distance~speed1+speed2)[,-1]
# Build the vector of response
y <- distance
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) # default: alpha=1 -> lasso
# [note: if alpha=0 -> ridge regression] # aplha = 1 -> lasso, 0 < alpha < 1 -> elastic
par(mfrow=c(1,1))
plot(fit.lasso, xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
# Let's set lambda via cross validation
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid) # default: 10-fold CV
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
optlam.lasso <- cv.lasso$lambda.1se
optlam.lasso
plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)
abline(v=log(optlam.lasso), lty=1) # optimal lambda, penalysing model, but still having good MSE
# Get the coefficients for the optimal lambda
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')[1:3,]
coef.lasso
coef.lasso <- predict(fit.lasso, s=optlam.lasso, type = 'coefficients')[1:3,]
coef.lasso
## Example 2: Dataset concrete --------------------------------------------------------------
data <- read.table('concrete.txt', header=T)
head(data)
dim(data)
names(data)
pairs(data)
attach(data)
y <- Hardness_concrete
par(mfrow=c(2,2))
plot(Alluminium, y, main='Hardness vs Alluminium', lwd=2,
xlab='Alluminium', ylab='Hardness concrete')
plot(Silicate, y, main='Hardness vs Silicate', lwd=2,
xlab='Silicate', ylab='Hardness concrete')
plot(Alluminium_ferrite, y, main='Hardness vs Alluminium ferrite', lwd=2,
xlab='Alluminium ferrite', ylab='Hardness concrete')
plot(Silicate_bicalcium, y, main='Hardness vs Silicate bicalcium', lwd=2,
xlab='Silicate bicalcium', ylab='Hardness concrete')
# Multiple linear regression
result <- lm(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)
summary(result)
vif(result)
# PCA regression
result.pc <- princomp(cbind(Alluminium,Silicate,Alluminium_ferrite,Silicate_bicalcium), scores=TRUE)
summary(result.pc)
result.pc$load
# Explained variance
layout(matrix(c(2,3,1,3),2,byrow=T))
barplot(result.pc$sdev^2, las=2, main='Principal Components', ylab='Variances')
barplot(c(sd(Alluminium),sd(Silicate),sd(Alluminium_ferrite),sd(Silicate_bicalcium))^2, las=2, main='Original variables', ylab='Variances')
plot(cumsum(result.pc$sdev^2)/sum(result.pc$sde^2), type='b', axes=F, xlab='number of components', ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.9, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:4,labels=1:4,las=2)
# Loadings
par(mfrow = c(4,1))
for(i in 1:4)barplot(result.pc$load[,i], ylim = c(-1, 1))
par(mfrow=c(1,1))
# Dimensionality reduction: select first two PCs:
pc1 <- result.pc$scores[,1]
pc2 <- result.pc$scores[,2]
# Now we estimate the model using the first two PCs as regressors.
# Model: y = b0 + b1*PC1+ b2*PC2 + eps, eps~N(0,sigma^2)
fm.pc <- lm(y ~ pc1 + pc2)
summary(fm.pc)
# y= b0 + b1*PC1 + b2*PC2 + eps =
#  = b0 + b1*(e11*(Alluminium-m1)+e21*(Silicate-m2)+e31*(Alluminium_ferrite-m3)+e41*(Silicate_bicalcium-m4)) +
#       + b2*(e12*(Alluminium-m1)+e22*(Silicate-m2)+e32*(Alluminium_ferrite-m3)+e42*(Silicate_bicalcium-m4)) + eps =
#  = b0 - b1*e11*m1 - b2*e12*m1 - b1*e21*m2 - b2*e22*m2 +
#       - b1*e31*m3 - b2*e32*m3 - b1*e41*m4 - b2*e42*m4 +
#       + (b1*e11+b2*e12)*Alluminium + (b1*e21+b2*e22)*Silicate +
#       + (b1*e31+b2*e32)*Alluminium_ferrite + (b1*e41+b2*e42)*Silicate_bicalcium + eps
# where e.ij are the loadings, i=1,2,3,4, j=1,2.
# => We can compute the coefficients of the model which used the original
#    regressors
m1 <- mean(Alluminium)
m2 <- mean(Silicate)
m3 <- mean(Alluminium_ferrite)
m4 <- mean(Silicate_bicalcium)
beta0 <- coefficients(fm.pc)[1] -
coefficients(fm.pc)[2]*result.pc$load[1,1]*m1 -
coefficients(fm.pc)[3]*result.pc$load[1,2]*m1 -
coefficients(fm.pc)[2]*result.pc$load[2,1]*m2 -
coefficients(fm.pc)[3]*result.pc$load[2,2]*m2 -
coefficients(fm.pc)[2]*result.pc$load[3,1]*m3 -
coefficients(fm.pc)[3]*result.pc$load[3,2]*m3 -
coefficients(fm.pc)[2]*result.pc$load[4,1]*m4 -
coefficients(fm.pc)[3]*result.pc$load[4,2]*m4
beta1 <- coefficients(fm.pc)[2]*result.pc$load[1,1] +
coefficients(fm.pc)[3]*result.pc$load[1,2]
beta2 <- coefficients(fm.pc)[2]*result.pc$load[2,1] +
coefficients(fm.pc)[3]*result.pc$load[2,2]
beta3 <- coefficients(fm.pc)[2]*result.pc$load[3,1] +
coefficients(fm.pc)[3]*result.pc$load[3,2]
beta4 <- coefficients(fm.pc)[2]*result.pc$load[4,1] +
coefficients(fm.pc)[3]*result.pc$load[4,2]
c(beta0=as.numeric(beta0),beta1=as.numeric(beta1),beta2=as.numeric(beta2),beta3=as.numeric(beta3),beta4=as.numeric(beta4))
result$coefficients
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(fm.pc)
par(mfrow=c(1,1))
shapiro.test(residuals(fm.pc))
# Ridge and Lasso regression with glmnet
x <- model.matrix(y ~ Alluminium + Silicate + Alluminium_ferrite + Silicate_bicalcium)[,-1] # matrix of predictors
y <- y # vector of response
lambda.grid <- 10^seq(5,-3,length=50)
# Ridge regression
fit.ridge <- glmnet(x,y, lambda = lambda.grid, alpha=0) # alpha=0 -> ridge
plot(fit.ridge,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
norm_l2 <- NULL
for(i in 1:50)
norm_l2 <- c(norm_l2,sqrt(sum((fit.ridge$beta[,i])^2)))
plot(log(lambda.grid),norm_l2)
# Let's set lambda via CV
set.seed(1)
cv.ridge <- cv.glmnet(x,y,alpha=0,nfolds=3,lambda=lambda.grid)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
optlam.ridge <- cv.ridge$lambda.1se
optlam.ridge
plot(cv.ridge)
abline(v=log(bestlam.ridge), lty=1)
abline(v=log(optlam.ridge), lty=1)
# Get the coefficients for the optimal lambda
coef.ridge <- predict(fit.ridge, s=bestlam.ridge, type = 'coefficients')[1:5,]
coef.ridge
plot(fit.ridge,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
abline(v=log(bestlam.ridge))
abline(v=log(optlam.ridge))
# Lasso regression
fit.lasso <- glmnet(x,y, lambda = lambda.grid, alpha=1) # alpha=1 -> lasso
plot(fit.lasso,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
plot(fit.lasso, xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
### Lasso regression ---------------------------------------------------------------------------
help(glmnet)
# Build the matrix of predictors
x <- model.matrix(distance~speed1+speed2)[,-1]
# Build the vector of response
y <- distance
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) # default: alpha=1 -> lasso
# [note: if alpha=0 -> ridge regression] # aplha = 1 -> lasso, 0 < alpha < 1 -> elastic
par(mfrow=c(1,1))
plot(fit.lasso, xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
fit.lasso
library(ISLR)
# Hitters dataset
help(Hitters)
names(Hitters)
dim(Hitters)
# remove NA's
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
#  Subset Selection Methods
# in froward - we start with one variable, and add more and more, until some moment
library(leaps)
help(regsubsets)
# Best Subset Selection: Exhaustive Search
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
regfit.full <- regsubsets(Salary~., data=Hitters, nvmax=19)
summary(regfit.full)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$which
reg.summary$rsq   # r-squared
reg.summary$adjr2 # adjusted r-squared
reg.summary$rss   # residual sum of squares
par(mfrow=c(1,3))
plot(reg.summary$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
# extract coefficient estimates associated with the models
which.max(reg.summary$adjr2)
coef(regfit.full,11)
coef(regfit.full,6)
# Forward and Backward Stepwise Selection
# only add variables
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.fwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.fwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
# in bwd we start from all variabels, and then remove them, chosing that one, having smallest redusing effect on target
# FORCED IN, FORCED OUT - we can specify, which variabels, have to be or should't be in model
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
par(mfrow=c(1,3))
plot(summary(regfit.bwd)$rsq,xlab="Number of Variables",ylab="R-squared",type="b")
plot(summary(regfit.bwd)$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="b")
plot(summary(regfit.bwd)$rss,xlab="Number of Variables",ylab="RSS",type="b")
par(mfrow=c(1,1))
coef(regfit.full,7) # Exhaustive search
coef(regfit.fwd,7)  # Forward Stepwise Selection
coef(regfit.bwd,7)  # Backward Stepwise Selection
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
folds
table(folds)
# function that performs the prediction for reg subsets
predict.regsubsets <- function(object, newdata, id) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k) {
best.fit <- regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19) {
pred <- predict.regsubsets(best.fit, Hitters[folds==j,], id=i)
cv.errors[j,i] <- mean( (Hitters$Salary[folds==j]-pred)^2 )
}
}
cv.errors
root.mean.cv.errors <- sqrt(apply(cv.errors,2,mean)) # average over the columns
root.mean.cv.errors
plot(root.mean.cv.errors,type='b')
which.min(root.mean.cv.errors)
points(which.min(root.mean.cv.errors),root.mean.cv.errors[which.min(root.mean.cv.errors)], col='red',pch=19)
# estimation on the full dataset
reg.best <- regsubsets(Salary~., data=Hitters, nvmax=19)
coef(reg.best,10)
# k-fold cross validation after model selection (WRONG WAY!) (adding test set in model selection)
best.fit <- regsubsets(Salary~., data=Hitters, nvmax=19)
summary(best.fit)
cv.errors_wrong <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k)
for(i in 1:19) {
covariate <- which(summary(best.fit)$which[i,-c(1,19)])
mod <- lm(Salary~., data=Hitters[folds!=j, c(covariate,19)])
pred <- predict(mod,Hitters)[folds==j]
cv.errors_wrong[j,i] <- mean( (Hitters$Salary[folds==j]-pred)^2)
}
cv.errors_wrong
root.mean.cv.errors_wrong <- sqrt(apply(cv.errors_wrong,2,mean)) # average over the columns
root.mean.cv.errors_wrong
plot(root.mean.cv.errors_wrong,type='b')
which.min(root.mean.cv.errors_wrong)
points(which.min(root.mean.cv.errors_wrong),root.mean.cv.errors_wrong[which.min(root.mean.cv.errors_wrong)], col='red',pch=19)
points(root.mean.cv.errors,type='b',col='blue')
# Ridge and Lasso regression with glmnet
x <- model.matrix(Salary~.,Hitters)[,-1] # predictor matrix
y <- Hitters$Salary # response
grid <- 10^seq(10,-2,length=100) # grid of lambda
ridge.mod <- glmnet(x,y,alpha=0,lambda=grid)
plot(ridge.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=0,nfold=3,lambda=grid)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
log(bestlam.ridge)
abline(v=log(bestlam.ridge))
plot(ridge.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.ridge))
lasso.mod <- glmnet(x,y,alpha=1,lambda=grid)
plot(lasso.mod,xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(123)
cv.out <- cv.glmnet(x,y,alpha=1,nfold=3,lambda=grid)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
log(bestlam.lasso)
abline(v=log(bestlam.lasso))
plot(lasso.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.lasso))
library(MASS)
library(car)
library(rgl)
library(glmnet) # to use LASSO
options(rgl.printRglwidget = TRUE)
## Example 1: Dataset cars --------------------------------------------------------------------
# Dataset cars: distance taken to stop [ft] as a function of velocity [mph]
# for some cars in the 1920s
cars
plot(cars, xlab='Speed', ylab='Stopping distance', las=1)
n          <- dim(cars)[[1]]
distance   <- cars$dist
speed1     <- cars$speed
speed2     <- cars$speed^2
fm <- lm(distance ~ speed1 + speed2)
summary(fm) # separatly variables useless, but together they are important, because F-statistic <0.05
# Variance inflation factor
help(vif)
