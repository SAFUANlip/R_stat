PVRE # it is high!
# Visualization of confidence intervals
#--------------------------------------#
intervals(fm16.1)
# fixed effects
library(sjPlot)
plot_model(fm16.1)
# random effects
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
re = ranef(fm16.1)
dat = data.frame(x= row.names(re),y=re[,attr(re,'effectName')])
var_b
var_eps
get_variance_residual(fm16.1)
get_variance_random(fm16.1)
PVRE <- var_b/(var_b+var_eps) # 80.60829/(80.60829+74.43401)
PVRE # it is high!
# Visualization of confidence intervals
#--------------------------------------#
intervals(fm16.1)
# fixed effects
library(sjPlot)
plot_model(fm16.1)
# random effects
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
re = ranef(fm16.1)
dat = data.frame(x= row.names(re),y=re[,attr(re,'effectName')])
# The dotplot shows the point and interval estimates for the random effects
# ordered
dotplot(reorder(x,y)~y,data=dat)
# not ordered
plot(ranef(fm16.1))
#_____________________________________________________________________________
# nicer visualization of the random effects with lmer() function
# formulation with lmer() for including a random intercept at subject level
fm16.1mer <- lmer(visual ~ visual0 + time * treat.f + (1|subject), data = armd)
# we can highligh which are significantly different from the mean (0)
# visualization of the random intercepts with their intervals
dotplot(ranef(fm16.1mer))
#install.packages('TMB', type = 'source')
plot_model(fm16.1mer, type='re') #--> positive (blu) and negative (red) effect
#Test for (variance of) random intercept
#--------------------------------------#
# H0 is "variance of a random effect is 0
#       in an LMM with a known correlation structure of the tested random effect and
#       independent and identically distributed random errors"
library(RLRsim)
exactRLRT(fm16.1)        # M16.1 (alternative model)
# Diagnostic plots
#----------------#
# 1) Assessing Assumption on the within-group errors
plot(fm16.1)  # Pearson and raw residuals are the same now
qqnorm(resid(fm16.1)) # normality of the residuals
qqline(resid(fm16.1), col='red', lwd=2)
# 2) Assessing Assumption on the Random Effects
qqnorm(fm16.1, ~ranef(.), main='Normal Q-Q Plot - Random Effects on Intercept')
fm16.2A <- lme(lm2.form, random = ~1 + time | subject, data = armd) # time - is slope
summary(fm16.2A)
intervals(fm16.2A)
# Var-Cov matrix of random-effects and errors
print(vc <- VarCorr(fm16.2A), comp = c("Variance", "Std.Dev."))
sigma <- summary(fm16.2A)$sigma
# Let's compute the conditional and marginal var-cov matrix of Y
#---------------------------------------------------------------#
# the conditional variance-covariance matrix of Y (diagonal matrix)
getVarCov(fm16.2A,
type = "conditional",       # sigma^2 * R_i
individual = "2")
# we extract sigma^2 * R_i for patients i=2,3,4
sR = getVarCov(fm16.2A, type = "conditional", individual = 2:4)
# and we plot them
plot(as.matrix(bdiag(sR$`2`, sR$`3`, sR$`4`)),
main = 'Conditional estimated Var-Cov matrix of Y given b')
# the marginal estimated variance-covariance matrix of Y (block-diagonal matrix)
(sVi = getVarCov(fm16.2A,
type = "marginal",      # sigma^2 * V_i
individual = "2"))
(cov2cor(sVi[[1]]))                     # Corr(sigma^2 * V_i)
# we extract sigma^2 * V_i for patients i=2,3,4
sV <- getVarCov(fm16.2A, type = "marginal", individual = 2:4)
# and we plot them
plot(as.matrix(bdiag(sV$`2`, sV$`3`, sV$`4`)),
main = 'Marginal estimated Var-Cov matrix of Y')
sR
sVi
VarCorr(fm16.2A) #[1,1]
var_eps <- as.numeric(vc[3,1])
var_eps
var_b <- as.numeric(vc[1,1]) + as.numeric(vc[2,1])*mean(armd$time^2) +
+ 2*as.numeric(vc[2,3])*as.numeric(vc[1,2])*as.numeric(vc[2,2])*mean(armd$time)
var_b
var_eps
get_variance_residual(fm16.2A)
get_variance_random(fm16.2A)
var_b
get_variance_residual(fm16.2A)
get_variance_random(fm16.2A)
# Сначала извлечь матрицу D
D <- matrix(as.numeric(vc[1:2, 1:2]), nrow = 2)  # если 2 случайных эффекта
# Среднее значение t
t_bar <- mean(armd$time)
# Вектор Z: [1, t]
Z <- matrix(c(1, t_bar), ncol = 1)
# Посчитать var_b
var_b <- t(Z) %*% D %*% Z
var_b
PVRE <- var_b/(var_b+var_eps)
PVRE # very high!
# Visualization
#-------------#
# visualization of the random intercepts & slopes with their 95% confidence intervals
# Random effects: b_0i, b_1i for i=1,...,234
plot(ranef(fm16.2A))
# as before, we can fit better plots through lmer()
fm16.2Amer <- lmer(visual ~ visual0 + time * treat.f + (1+time|subject), data = armd,
control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
dotplot(ranef(fm16.2Amer))
# for a better visualization
plot_model(fm16.2Amer, type='re') #--> positive (blu) and negative (red) effect
# Comparing models
#-----------------#
anova(fm16.1, fm16.2A)
# Diagnostic plots
#-----------------#
# 1) Assessing Assumption on the within-group errors
plot(fm16.2A)
# we still assume, that our residuals and Intercept & Slope have normal distribution
qqnorm(resid(fm16.2A))
qqline(resid(fm16.2A), col='red', lwd=2) #
# 2) Assessing Assumption on the Random Effects
qqnorm(fm16.2A, ~ranef(.), main='Normal Q-Q Plot - Random Effects on Intercept & Slope')
intervals(fm16.2A, which = "var-cov")
fm16.2B <- lme(lm2.form, random = list(subject = pdDiag(~1+time)), data = armd)
intervals(fm16.2B)                       # 95% CI for betas, sigma
summary(fm16.2B)
# Var-Cov matrix of random-effects and residuals
print(vc <- VarCorr(fm16.2B), comp = c("Variance", "Std.Dev."))
sigma <- summary(fm16.2B)$sigma
sigma
# Let's compute the conditional and marginal var-cov matrix of Y
#---------------------------------------------------------------#
# the conditional variance-covariance matrix of Y (diagonal matrix)
getVarCov(fm16.2B,
type = "conditional",       # sigma^2 * R_i
individual = "2")
# we extract sigma^2 * R_i for patients i=2,3,4
sR = getVarCov(fm16.2B, type = "conditional", individual = 2:4)
# and we plot them
plot(as.matrix(bdiag(sR$`2`, sR$`3`, sR$`4`)),
main = 'Conditional estimated Var-Cov matrix of Y given b')
# the marginal variance-covariance matrix of Y (block-diagonal matrix)
(sVi = getVarCov(fm16.2B,
type = "marginal",      # sigma^2 * V_i
individual = "2"))
(cov2cor(sVi[[1]]))                     # Corr(sigma^2 * V_i)
# we extract sigma^2 * V_i for patients i=2,3,4
sV <- getVarCov(fm16.2B, type = "marginal", individual = 2:4)
# and we plot it
plot(as.matrix(bdiag(sV$`2`, sV$`3`, sV$`4`)),
main = 'Marginal estimated Var-Cov matrix of Y')
#-----#
# In this case the variance of random effects represents the mean random
# effect variance of the model and is given by
# var_b = Var(b0,b1) = sigma2_b0 + 0 + sigma2_b1*mean(z^2)
# See equation (10) in Johnson (2014), Methods in Ecology and Evolution, 5(9), 944-946.
vc
var_eps <- as.numeric(vc[3,1])
var_eps
var_b <- as.numeric(vc[1,1]) + mean(armd$time^2)*as.numeric(vc[2,1]) # 54.07117 + 0.07935904*mean(armd$time^2)
var_b
get_variance_random(fm16.2B)
get_variance_residual(fm16.2B)
# Let's compute the conditional and marginal var-cov matrix of Y
#---------------------------------------------------------------#
# the conditional variance-covariance matrix of Y (diagonal matrix)
getVarCov(fm16.2B,
type = "conditional",       # sigma^2 * R_i
individual = "2")
# we extract sigma^2 * R_i for patients i=2,3,4
sR = getVarCov(fm16.2B, type = "conditional", individual = 2:4)
# and we plot them
plot(as.matrix(bdiag(sR$`2`, sR$`3`, sR$`4`)),
main = 'Conditional estimated Var-Cov matrix of Y given b')
# the marginal variance-covariance matrix of Y (block-diagonal matrix)
(sVi = getVarCov(fm16.2B,
type = "marginal",      # sigma^2 * V_i
individual = "2"))
(cov2cor(sVi[[1]]))                     # Corr(sigma^2 * V_i)
# we extract sigma^2 * V_i for patients i=2,3,4
sV <- getVarCov(fm16.2B, type = "marginal", individual = 2:4)
# and we plot it
plot(as.matrix(bdiag(sV$`2`, sV$`3`, sV$`4`)),
main = 'Marginal estimated Var-Cov matrix of Y')
#-----#
# In this case the variance of random effects represents the mean random
# effect variance of the model and is given by
# var_b = Var(b0,b1) = sigma2_b0 + 0 + sigma2_b1*mean(z^2)
# See equation (10) in Johnson (2014), Methods in Ecology and Evolution, 5(9), 944-946.
vc
var_eps <- as.numeric(vc[3,1])
var_eps
var_b <- as.numeric(vc[1,1]) + mean(armd$time^2)*as.numeric(vc[2,1]) # 54.07117 + 0.07935904*mean(armd$time^2)
var_b
get_variance_random(fm16.2B)
vc
rm(list=ls())
graphics.off()
# Topics: ####
#   - Linear Mixed Models with Random Intercept
#   - Linear Mixed Models with Random Intercept + Random Slope
#   - Inference
#   - Assessing Assumptions
#   - Prediction
library(ggplot2)
library(insight)
library(lattice)
library(lme4)
rm(list=ls())
graphics.off()
# Dataset school - Student achievement
# We have data from 1000 pupils who attend 50 different primary schools.
# Students are tested in mathematics with a standardized test across schools.
# The response variable is the achievement test score (numeric).
# We have two explanatory variables at the student level:
#   - pupil gender (1 = male, 0 = female)
#   - a scale centered in 0 for pupil socioeconomic status, pupil escs.
# Moreover, we know the anonymous school identification number.
school= read.table('school.txt', header=T)
school$gender= as.factor(school$gender)
school$school_id= as.factor(school$school_id)
head(school)
str(school)
# We look at achievement scores for students.
# The source of dependency is due to students attending the same primary school.
# For our mixed model we'll look at the effects for gender and socioeconomic status (escs)
# on scholastic achievement,
# taking into account the source of dependency given to the hierarchical structure.
summary(school)
sd(school$achiev)
sqrt(var(school$achiev))
# Achievement variability in primary schools
x11()
rm(list=ls())
graphics.off()
library(ggplot2)
library(insight)
library(lattice)
library(lme4)
rm(list=ls())
graphics.off()
school= read.table('school.txt', header=T)
school$gender= as.factor(school$gender)
school$school_id= as.factor(school$school_id)
head(school)
str(school)
summary(school)
sd(school$achiev)
sqrt(var(school$achiev))
ggplot(data=school, aes(x=as.factor(school_id), y=achiev, fill=as.factor(school_id))) +
geom_boxplot() +
labs(x='Primary School', y='Achievement') +
ggtitle('Boxplot of achievements among primary schools') +
theme_minimal() +
theme(axis.text=element_text(size=rel(1.15)),axis.title=element_text(size=rel(1.5)),
plot.title = element_text(face="bold", size=rel(1.75)), legend.text = element_text(size=rel(1.15)),
legend.position = 'none')
lm1 = lm(achiev ~ gender + escs, data = school)
summary(lm1)
plot(school$escs,school$achiev, col='blue')
abline(9.91880,1.86976, col='green', lw=4)          # females
abline(9.91880 -0.68298,1.86976, col='orange', lw=4)  # males
plot(lm1$residuals)
boxplot(lm1$residuals ~ school$school_id, col='orange', xlab='School ID', ylab='Residuals')
lmm1_lme <- lme(achiev ~ 1 + gender + escs, random = ~1|school_id, data = school)
summary(lmm1_lme)
lmm1 = lmer(achiev ~ gender + escs + (1|school_id),
data = school)
summary(lmm1)
# Fixed Effects and 95% CIs
#-------------------------------
confint(lmm1, oldNames=TRUE)
intervals(lmm1_lme)
fixef(lmm1)
print(vc <- VarCorr(lmm1), comp = c("Variance", "Std.Dev."))
help(get_variance)
get_variance(lmm1_lme) # also work with lme models
sigma2_eps <- as.numeric(get_variance_residual(lmm1))
sigma2_eps
sigma2_b <- as.numeric(get_variance_random(lmm1))
sigma2_b
# Another way to interpret the variance output is to note percentage of the student variance out
# of the total, i.e. the Percentage of Variance explained by the Random Effect (PVRE).
# This is also called the intraclass correlation (ICC), because it is also an estimate of the within
# cluster correlation.
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
PVRE
# Random effects: b_0i
#----------------------------
ranef(lmm1)
x11()
dotplot(ranef(lmm1))
# Random intercepts and fixed slopes: (beta_0+b_0i, beta_1, beta_2)
coef(lmm1)
head(coef(lmm1)$school_id)
# Let's plot all the regression lines
## FEMALES
x11()
par(mfrow=c(1,2))
plot(school$escs[school$gender==0], school$achiev[school$gender==0],col='blue',
xlab='escs', ylab='achievement',ylim=c(-5,30),main='Data and regression lines for females')
abline(10.02507,1.96618, col='red', lw=6)
for(i in 1:50){
abline(coef(lmm1)$school_id[i,1], coef(lmm1)$school_id[i,3])
}
## MALES
plot(school$escs[school$gender==1], school$achiev[school$gender==1],col='blue',
xlab='escs', ylab='achievement',ylim=c(-5,30),main='Data and regression lines for males')
abline(10.02507-0.91180,1.96618, col='red', lw=6)
for(i in 1:50){
abline(coef(lmm1)$school_id[i,1] + coef(lmm1)$school_id[i,2], coef(lmm1)$school_id[i,3])
}
# Diagnostic plots
#------------------
# 1) Assessing Assumption on the within-group errors
x11()
plot(lmm1)
qqnorm(resid(lmm1))
qqline(resid(lmm1), col='red', lwd=2)
# 2) Assessing Assumption on the Random Effects
x11()
qqnorm(unlist(ranef(lmm1)$school_id), main='Normal Q-Q Plot - Random Effects for Primary School')
qqline(unlist(ranef(lmm1)$school_id), col='red', lwd=2)
# Prediction from regression model
predict_lm <- predict(lm1)
head(predict_lm) # see our prediction
# Prediction from mixed model:
# 1) Without random effects ->  re.form=NA
predict_no_re <- predict(lmm1, re.form=NA)
head(predict_no_re) # same predictions
# 2) With random effects
predict_re <- predict(lmm1)
head(predict_re)
# Prediction from mixed model on a test observation from a subject present in the training set:
test.data = data.frame(school_id= '50', gender = as.factor(1), escs = 3)
# 1) Without random effects ->  re.form=NA
predict_no_re <- predict(lmm1, newdata = test.data, re.form=NA)
predict_no_re # (9.94471 - 0.74963*1 + 1.86878*test.data$escs )
# 2) With random effects
predict_re <- predict(lmm1, newdata = test.data)
predict_re # (9.94471 - 0.74963*1 + 1.86878*test.data$escs +  0.8234367)
summary(lmm1)
summary(lmm1)$Random
lmm1
summary(lmm1)
# where 0.8234367 comes from the random intercept vector and corresponds to the school_id 50
re = ranef(lmm1)[[1]]
re[row.names(re)==test.data$school_id,]
ranef(lmm1)[[1]]
# NEW SCHOOL (NEW GROUP)
## --> remember to allow new levels in the RE if any!!!
your_new_data = data.frame(school_id= '51', gender = factor(1), escs = 3)
# 1) Without random effects ->  re.form=NA
predict(lmm1, newdata = your_new_data, re.form=NA)
# 2) With random effects
predict(lmm1, newdata = your_new_data)
# it does not recognize the subject --> allow.new.levels = T
predict(lmm1, newdata = your_new_data, allow.new.levels = TRUE) # assuming of random intercept = 0
new_student1 = data.frame(gender=as.factor(1), escs=0.7, school_id=32) # observed school
new_student1 = data.frame(gender=as.factor(1), escs=0.7, school_id=32) # observed school
new_student2 = data.frame(gender=as.factor(1), escs=0.7, school_id=11) # observed school
new_student3= data.frame(gender=as.factor(1), escs=0.7, school_id=53) # new school
predict(lmm1, new_student1, re.form=NA)
predict(lmm1, new_student1)
predict(lmm1, new_student2, re.form=NA)
predict(lmm1, new_student2)
predict(lmm1, new_student3, re.form=NA)
predict(lmm1, new_student3, allow.new.levels = T)
summary(lmm1)
graphics.off()
lmm2_lme <- lme(achieve ~ 1 + escs, random=~1+escs|school_id, data=school)
lmm2_lme <- lme(achiev ~ 1 + escs, random=~1+escs|school_id, data=school)
summary(lmm2_lme)
lmm2 = lmer(achiev ~ gender + escs + (1 + escs|school_id),
data = school)
summary(lmm2)
summary(lmm2_lme)
summary(lmm2_lme)
summary(lmm2)
lmm2_lme <- lme(achiev ~ 1 + escs + gender, random=~1+escs|school_id, data=school)
summary(lmm2_lme)
summary(lmm2)
confint(lmm2, oldNames=TRUE)
print(vc <- VarCorr(lmm2), comp = c("Variance", "Std.Dev."))
sigma2_eps <- as.numeric(get_variance_residual(lmm2))
sigma2_eps
sigma2_b <- as.numeric(get_variance_random(lmm2))  ## it automatically computes Var(b0,b1)
# 4.3228 + 2*0.164*2.0791*1.6451* mean(school$escs, na.rm=T) + 2.7063*mean(school$escs^2, na.rm=T)
sigma2_b
# 4.3228 + 2 * 0.164 * 2.0791 * 1.6451 * mean(school$escs, na.rm=T) + 2.7063*mean(school$escs^2, na.rm=T)
sigma2_b
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
PVRE
# Fixed effects: (beta_0, beta_1, beta_2)
fixef(lmm2)
# Random effects: (b_0i, b_1i) for i=1,...,200
ranef(lmm2)
head(ranef(lmm2)$school_id)
dotplot(ranef(lmm2))
# Random intercepts and slopes: (beta_0+b_0i, beta_1, beta_2+b_2i)
coef(lmm2)
head(coef(lmm2)$school)
# Let's plot all the regression lines
## FEMALES
x11()
par(mfrow=c(1,2))
plot(school$escs[school$gender==0], school$achiev[school$gender==0],col='blue',
xlab='escs', ylab='achievement',ylim=c(-5,30),main='Data and regression lines for females')
abline(10.0546535,1.6790886, col='red', lw=6)
for(i in 1:50){
abline(coef(lmm2)$school_id[i,1], coef(lmm2)$school_id[i,3])
}
## MALES
plot(school$escs[school$gender==1], school$achiev[school$gender==1],col='blue',
xlab='escs', ylab='achievement',ylim=c(-5,30),main='Data and regression lines for males')
abline(10.02507-0.91180,1.96618, col='red', lw=6)
for(i in 1:50){
abline(coef(lmm2)$school_id[i,1] + coef(lmm2)$school_id[i,2], coef(lmm2)$school_id[i,3])
}
plot(lmm2)
x11()
qqnorm(resid(lmm2))
qqline(resid(lmm2), col='red', lwd=2)
# 2) Assessing Assumption on the Random Effects
x11()
par(mfrow=c(1,2))
qqnorm(unlist(ranef(lmm2)$school_id[1]), main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(lmm2)$school_id[1]), col='red', lwd=2)
qqnorm(unlist(ranef(lmm2)$school_id[2]), main='Normal Q-Q Plot - Random Effects on escs')
qqline(unlist(ranef(lmm2)$school_id[2]), col='red', lwd=2)
x11()
plot(unlist(ranef(lmm2)$school_id[2]),unlist(ranef(lmm2)$school_id[1]),
ylab=expression(paste('Intercept  ', b['0i'])),
xlab=expression(paste('escs  ', b['1i'])), col='dodgerblue2',
main='Scatterplot of estimated random effects')
abline(v=0,h=0)
# Comparing models
#------------------
# The anova function, when given two or more arguments representing fitted models,
# produces likelihood ratio tests comparing the models.
anova(lmm1, lmm2)
anova(lmm1_lme, lmm2_lme)
library(nlmeU)
library(corrplot)
library(nlme)
library(lattice)
library(plot.matrix)
library(lme4)
library(insight)
rm(list=ls())
graphics.off()
data(armd, package = "nlmeU") # Age-Related Macular Degeneration
rlims <- c(-4.5, 4.5)
xlims <- c(0, 90)
xlimc <- c("4", "12", "24", "52wks")
# We now treat time as a numeric variable (as assessed in the last lab)
lm2.form <- formula(visual ~ visual0 + time + treat.f + treat.f:time)
fm16.1 <- lme(lm2.form, random = ~1|subject, data = armd)
summary(fm16.1)
# sigma*sqrt(d11) = 8.98 (standard deviation of the random intercepts)
# sigma = 8.63           (residual standard deviation)
VarCorr(fm16.1)
(8.98/sigma)^2
(8.98/8.63)^2
# print out the estimated fixed-effects table
printCoefmat(summary(fm16.1)$tTable, has.Pvalue = TRUE, P.values = TRUE)
# to get the confidence intervals of all the estimated parameters
intervals(fm16.1)
# Number of parameters ---------------------------------------------------------
# how many parameters estimated - don't forget about sigma and d11
# Num of fixed params
n_fixed <- length(fixef(fm16.1))
# Num of variance params
# Каждая уникальная случайная группа и ее ковариационная матрица
re_structures <- VarCorr(fm16.1)
re_structures
n_random <- attr(re_structures, "sc")  # residual std dev
n_random
n_variance_params <- nrow(re_structures) - 1  # минус строка residual
# Всегда 1 параметр для остаточной дисперсии (sigma)
n_sigma <- 1 # (we work with homoscedastic, so there i only one sigma)
# Общее число параметров
n_total <- n_fixed + n_variance_params + n_sigma
cat("Total number of estimated parameters:", n_total, "\n")
# Var-Cov matrix of fixed-effects
vcovb <- vcov(fm16.1)
vcovb
# and Correlation of fixed effects
corb <- cov2cor(vcovb)
nms <- abbreviate(names(fixef(fm16.1)), 5)
rownames(corb) <- nms
corb
# Var-Cov matrix of random-effects and residuals (what we observe in the summary)
print(vc <- VarCorr(fm16.1), comp = c("Variance", "Std.Dev."))
VarCorr(fm16.1)
var_eps = as.numeric(vc[2,1])
var_eps
sd_eps <- summary(fm16.1)$sigma
sd_eps
var_b = as.numeric(vc[1,1])
var_b
