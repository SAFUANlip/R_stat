metalic.cov
# Directions and axis length ---------------------------------------------------
n_metalic <- nrow(metalic_data)
p <- 2
alpha <- 0.05
cfr.fisher <- ((n_metalic - 1) * p / (n_metalic - p)) * qf(1 - alpha, p, n_metalic - p)
cfr.fisher
eig <- eigen(metalic.cov / n_metalic)
directions <- eig$vectors      # направления (ось эллипса)
lengths <- sqrt(eig$values * cfr.fisher)  # длины полуосей
# Результаты:
directions  # Each column — direction of axis
lengths     # длины полуосей (вдоль направлений)
# Ellipsoidal confidence region with confidence level 95%
plot(metalic_data, asp=1, pch=1, main='Metallic data')
# plottin ellipse, don't forget to divide cov/n
ellipse(center=metalic.m, shape=metalic.cov/n_metalic, radius=sqrt(cfr.fisher), lwd=2)
arrows(metalic.m[1], metalic.m[2],
metalic.m[1] + lengths[1]*directions[1,1],
metalic.m[2] + lengths[1]*directions[2,1],
col="red", lwd=2)
arrows(metalic.m[1], metalic.m[2],
metalic.m[1] + lengths[2]*directions[1,2],
metalic.m[2] + lengths[2]*directions[2,2],
col="darkgreen", lwd=2)
abline(a = metalic.m[2] - eigen(metalic.cov)$vectors[2, 1] / eigen(metalic.cov)$vectors[1, 1] * metalic.m[1],
b = eigen(metalic.cov)$vectors[2, 1] / eigen(metalic.cov)$vectors[1, 1],
lty = 2, col = 'dark red', lwd = 2)
abline(a = metalic.m[2] - eigen(metalic.cov)$vectors[2, 2] / eigen(metalic.cov)$vectors[1, 2] * metalic.m[1],
b = eigen(metalic.cov)$vectors[2, 2] / eigen(metalic.cov)$vectors[1, 2],
lty = 2, col = 'red', lwd = 2)
storage <- read.table("2024_01_17/StorageCentres.txt", h=TRUE)
storage <- read.table("2024_01_17/StorageCentres.txt", h=TRUE)
lm_model <- lm(costs ~ 1 + as.factor(time) + costs0 + growth:as.factor(time) + rad_less_15_city + size,
data=storage)
storage <- read.table("2024_01_17/StorageCentres.txt", h=TRUE)
lm_model <- lm(costs ~ 1 + as.factor(time) + costs0 + growth:as.factor(time) + rad_less_15_city + size,
data=storage)
summary(lm_model)
AIC(lm_model) # 1259.965
lm_model <- lm(costs ~ 1 + as.factor(time) + costs0 + growth*as.factor(time) + rad_less_15_city + size,
data=storage)
summary(lm_model)
lm_model <- lm(costs ~ 1 + as.factor(time) + costs0 + growth:as.factor(time) + rad_less_15_city + size,
data=storage)
summary(lm_model)
par(mfrow=c(2,2)); plot(lm_model)
# plot to confirm that residuals are normal
qqnorm(lm_model$residuals)
qqline(resid(lm_model), col='red', lwd=2)
plot(lm_model, which = 1) # plot to confirm that residuals are normal
plot(lm_model, which = 2) # plot showing homoscedasticity,
shapiro.test(lm_model$residuals) # So we reject H0 of residuals normality
boxplot(storage)
storage_data <- storage[, -6]
boxplot(storage_data)
boxplot(storage[, 6]) # looking at box-plot, the are many outliers os costs - in target variabel
plot(lm_model$residuals)
plot(lm_model$residuals)
# Variance function:
#   Structure: Power of variance covariate
# Formula: ~time
# Parameter estimates:
#   power
# 0.8899608 - delta
AIC(gls_model1)
gls_model1 <- gls(
costs ~ 1 + as.factor(time) + costs0 + growth:as.factor(time) + rad_less_15_city + size,
weights = varPower(form = ~time),
data=storage
)
summary(gls_model1)
# Variance function:
#   Structure: Power of variance covariate
# Formula: ~time
# Parameter estimates:
#   power
# 0.8899608 - delta
AIC(gls_model1)
par(mfrow=c(2,2)); plot(lm_model)
plot(lm_model, which = 1) # plot to confirm that residuals are normal
plot(lm_model, which = 2) # plot showing homoscedasticity,
plot(lm_model$residuals)
shapiro.test(lm_model$residuals) # So we reject H0 of residuals normality
boxplot(storage)
# plot to confirm that residuals are normal
qqnorm(lm_model$residuals)
qqline(resid(lm_model), col='red', lwd=2)
plot(lm_model, which = 1) # plot to confirm that residuals are normal
plot(lm_model, which = 2) # plot showing homoscedasticity,
plot(lm_model$residuals)
View(storage)
crop_yields <- read.table("2025_01_17/crop_yield.txt", h=TRUE)
lm0 <- lm(formula = yield ~ -1 + as.factor(irrigation) + temperature + rainfall + sunny + soil_quality + fertilizer,
data=crop_yields)
summary(lm0)
par(mfrow=c(2,2)); plot(lm0)
# plot to confirm that residuals are normal
qqnorm(lm0$residuals)
qqline(resid(lm0), col='red', lwd=2)
plot(lm0, which = 1) # plot to confirm that residuals are normal
plot(lm0, which = 2) # plot showing homoscedasticity
# to check this assumtpion we can run shapiro test (to check normality),
# and look on graphics of model
shapiro.test(lm0$residuals) # Can not reject H0 of nresiduals normality
plot(lm0$residuals)
View(crop_yields)
plot(lm0$residuals)
beverages <- read.table("2024_01_17/beverages.txt", h=TRUE)
coordinates(beverages) <- c('x', 'y')
v <- variogram(sales ~ -1 + as.factor(holiday) + temp, beverages)
plot(v)
# params: psill = NA, model, range = NA, nugget
v.fit <- fit.variogram(v, vgm(0.45, "Sph", 2000, 0.15), fit.method = 7) # 7 - by default (Weighted least squares)
v.fit # here we see psill and range (estimated)
#    model     psill    range
# 1   Nug  0.2087394    0.000
# 2   Sph 0.2388645 2141.746
# psill = 0.2087394 + 0.2388645 = 0.4476039
# range = 2141.746
plot(v, v.fit)
# b) Estimate the parameters b0,0, b0,1 and b1 using the generalized least squares method.
g.tr <- gstat(formula = sales ~  -1 + as.factor(holiday) + temp, data = beverages, model = v.fit)
g.tr
s0.new=data.frame(x=0.0, y=0.0, holiday=FALSE, temp = 0, central = TRUE) # UTM coordinates
coordinates(s0.new)=c('x','y')
predict(g.tr, s0.new, BLUE = TRUE) # 1.149987 = betta_0_0
s1.new=data.frame(x=0.0, y=0.0, holiday=TRUE, temp = 0, central = TRUE) # UTM coordinates
coordinates(s1.new)=c('x','y')
predict(g.tr, s1.new, BLUE = TRUE) # 1.579688 = betta_0_1
s2.new=data.frame(x=0.0, y=0.0, holiday=TRUE, temp = 1, central = TRUE) # UTM coordinates
coordinates(s2.new)=c('x','y')
predict(g.tr, s2.new, BLUE = TRUE) # 1.599747 = betta_0_1 + betta_1
s3.new=data.frame(x=0.0, y=0.0, holiday=FALSE, temp = 1, central = TRUE) # UTM coordinates
coordinates(s3.new)=c('x','y')
predict(g.tr, s3.new, BLUE = TRUE) # 1.170046 = betta_0_0 + betta_1
# 31 day in July, 8 holidays, 23 working days
s4.new=data.frame(x=0.0, y=0.0, holiday=FALSE, temp = 30, central = TRUE) # UTM coordinates
coordinates(s4.new)=c('x','y')
predict(g.tr, s4.new, BLUE = TRUE) # 1.751743 => 1.751743 * 23 = 40.29009
s5.new=data.frame(x=0.0, y=0.0, holiday=TRUE, temp = 30, central = TRUE) # UTM coordinates
coordinates(s5.new)=c('x','y')
predict(g.tr, s5.new, BLUE = TRUE) # 2.181444 => 2.181444 * 8 = 17.45155
summary(lm(sales ~ -1 + as.factor(holiday):as.factor(central) + temp:as.factor(central), beverages))
summary(lm(sales ~ -1 + as.factor(holiday):as.factor(central) + temp:as.factor(central), beverages))
v2 <- variogram(sales ~ -1 + as.factor(holiday):as.factor(central) + temp:as.factor(central), beverages)
plot(v2)
# params: psill = NA, model, range = NA, nugget
v2.fit <- fit.variogram(v2, vgm(0.25, "Sph", 2000), fit.method = 7) # 7 - by default (Weighted least squares)
v2.fit # here we see psill and range (estimated)
#    model     psill    range
# 1   Sph 0.26918 1815.396
# range = 1815.396
plot(v2, v2.fit)
v2 <- variogram(sales ~  as.factor(holiday):as.factor(central) + temp:as.factor(central), beverages)
plot(v2)
# params: psill = NA, model, range = NA, nugget
v2.fit <- fit.variogram(v2, vgm(0.25, "Sph", 2000), fit.method = 7) # 7 - by default (Weighted least squares)
v2.fit # here we see psill and range (estimated)
#    model     psill    range
# 1   Sph 0.26918 1815.396
# range = 1815.396
plot(v2, v2.fit)
summary(lm(sales ~ -1 + as.factor(holiday):as.factor(central) + temp:as.factor(central), beverages))
v2 <- variogram(sales ~ -1 + as.factor(holiday):as.factor(central) + temp:as.factor(central), beverages)
plot(v2)
# params: psill = NA, model, range = NA, nugget
v2.fit <- fit.variogram(v2, vgm(0.25, "Sph", 2000), fit.method = 7) # 7 - by default (Weighted least squares)
v2.fit # here we see psill and range (estimated)
#    model     psill    range
# 1   Sph 0.26918 1815.396
# range = 1815.396
plot(v2, v2.fit)
plot(lm_model$residuals)
storage <- read.table("2024_01_17/StorageCentres.txt", h=TRUE)
lm_model <- lm(costs ~ 1 + as.factor(time) + costs0 + growth:as.factor(time) + rad_less_15_city + size,
data=storage)
summary(lm_model)
AIC(lm_model) # 1259.965
par(mfrow=c(2,2)); plot(lm_model)
# plot to confirm that residuals are normal
qqnorm(lm_model$residuals)
qqline(resid(lm_model), col='red', lwd=2)
plot(lm_model, which = 1) # plot to confirm that residuals are normal
plot(lm_model, which = 2) # plot showing homoscedasticity,
plot(lm_model$residuals)
View(storage)
boxplot(lm_model$residuals ~ storage$time, col=colori,
xlab='Time.f', ylab='Residuals')  ## -> the variance of th observations increases in time
colors <- as.factor(storage$time)
colors <- as.factor(storage$time)
boxplot(lm_model$residuals ~ storage$time, col=colors,
xlab='Time.f', ylab='Residuals')  ## -> the variance of th observations increases in time
#colors <- as.factor(storage$time)
colors = rainbow(4)
boxplot(lm_model$residuals ~ storage$time, col=colors,
xlab='Time.f', ylab='Residuals')  ## -> the variance of th observations increases in time
library(nlmeU) ## --> for the dataset
library(nlme)
library(insight)
satisfaction <- read.table("2023_09_04/satisfaction.txt", h=TRUE)
View(satisfaction)
lm_model <- lm(score ~ 1 + purch amount  + memb duration + age,
lm_model <- lm(score ~ 1 + purch_mount  + memb_duration + age,
data=satisfaction)
lm_model <- lm(score ~ 1 + purch_amount  + memb_duration + age,
data=satisfaction)
summary(lm_model)
plot(lm_model)
par(mfrow=c(2,2))
plot(lm_model)
shapiro.test(lm_model$residuals)
plot(lm_model$residuals)
confint(lm_model)
confint(lm_model, parm = ("memb_duration", "age"))
confint(lm_model, parm = c("memb_duration", "age"))
confint(lm_model, parm = c("memb_duration", "age"), level=1-0.05/2)
# first remove memb_duration as it has higer p-value
lm_model2 <- lm(score ~ 1 + purch_amount + age,
data=satisfaction)
summary(lm_model2)
# now remove age
lm_model3 <- lm(score ~ 1 + purch_amount,
data=satisfaction)
summary(lm_model3)
lme_model <- lme(score ~ 1 + purch_amount,
random =  ~1|as.factor(store)
data=satisfaction)
lme_model <- lme(score ~ 1 + purch_amount,
random =  ~1|as.factor(store),
data=satisfaction)
summary(lme_model)
var_b <- get_variance_random(lme_model)
var_eps <- get_variance_residual(lme_model)
var_b
var_eps
PVRE <- var_b/(var_b+var_eps) # we can take them from summary as ^2 of StdDev
PVRE
PVRE
PVRE
var_b <- get_variance_random(lme_model)
var_eps <- get_variance_residual(lme_model)
var_b
var_eps
PVRE <- var_b/(var_b+var_eps) # we can take them from summary as ^2 of StdDev
PVRE
ranef(lme_model)
plot(ranef(lme_model))
max(ranef(lme_model))
confint(lm_model, parm = c("memb_duration", "age"), level=1-0.05/2)
var_b
var_eps #
wheelworks <- read.table("2023_09_04/wheelworks.txt", h=TRUE)
cyclecraft <- read.table("2023_09_04/cyclecraft.txt", h=TRUE)
View(cyclecraft)
View(wheelworks)
boxplot(wheelworks)
boxplot(wheelworks[,2:3])
boxplot(cyclecraft[,2:3])
data_difference <- data.frame(
delta = wheelworks - cyclecraft,
)
data_difference <- data.frame(
delta = wheelworks[,2:3] - cyclecraft[,2:3],
)
wheelworks[,2:3] - cyclecraft[,2:3]
data_difference <- data.frame(
delta = wheelworks[,2:3] - cyclecraft[,2:3]
)
boxplot(data_difference)
plot(data_difference)
mvn(data_difference) # differences
mu0      <- c(0, 0) # H0, that difference is zero
x.mean   <- colMeans(data_difference)
x.cov    <- cov(data_difference)
View(data_difference)
n <- 20 # each group has 10 observations
p <- 2 # check two means on zero
x.T2 <- n * (x.mean-mu0) %*% x.invcov %*% (x.mean-mu0)
mu0      <- c(0, 0) # H0, that difference is zero
x.mean   <- colMeans(data_difference)
x.cov    <- cov(data_difference)
x.invcov <- solve(x.cov)
n <- 20 # each group has 10 observations
p <- 2 # check two means on zero
x.T2 <- n * (x.mean-mu0) %*% x.invcov %*% (x.mean-mu0)
Pb <- 1-pf(x.T2*(n-p)/(p*(n-1)), p, n-p)
Pb # 0.006252078 < 0.05, so we can reject, that mean difference = 0
# b) Which are the assumptions of the previous test? Are they met?
# Delta features have to be MVN
mvn(data_difference)
# mean under H0 (blue)
points(mu0[1], mu0[2], col='blue', pch=16)
# sample mean (black)
points(x.mean[1], x.mean[2], col='black', pch=16)
# we represent the confidence region of level 95%: where does mu0 fall?
alpha <- .05
cfr.fisher <- (p*(n-1)/(n-p))*qf(1-alpha,p,n-p)
ellipse(center=x.mean, shape=x.cov/n, radius=sqrt(cfr.fisher), lwd=2)
# mean under H0 (blue)
points(mu0[1], mu0[2], col='blue', pch=16)
# sample mean (black)
points(x.mean[1], x.mean[2], col='black', pch=16)
# we represent the confidence region of level 95%: where does mu0 fall?
alpha <- .05
cfr.fisher <- (p*(n-1)/(n-p))*qf(1-alpha,p,n-p)
ellipse(center=x.mean, shape=x.cov/n, radius=sqrt(cfr.fisher), lwd=2)
# mean under H0 (blue)
plot(data_difference)
# mean under H0 (blue)
plot(data_difference)
points(mu0[1], mu0[2], col='blue', pch=16)
# sample mean (black)
points(x.mean[1], x.mean[2], col='black', pch=16)
# we represent the confidence region of level 95%: where does mu0 fall?
alpha <- .05
cfr.fisher <- (p*(n-1)/(n-p))*qf(1-alpha,p,n-p)
ellipse(center=x.mean, shape=x.cov/n, radius=sqrt(cfr.fisher), lwd=2)
k <- 4 # if we also wanted to plot for variances => k = 4
alpha <- 0.05
ICmean <- cbind(inf=x.mean - sqrt(diag(x.cov)/n) * qt(1 - alpha/(2*k), n-1),
center= x.mean,
sup= x.mean + sqrt(diag(x.cov)/n) * qt(1 - alpha/(2*k), n-1))
ICvar <- cbind(inf=diag(x.cov)*(n-1) / qchisq(1 - alpha/(2*k), n-1),
center=diag(x.cov),
sup=diag(x.cov)*(n-1) / qchisq(alpha/(2*k), n-1))
ICmean
ICvar
library(MASS)
library(car)
library(rgl)
library(glmnet)
izoard <- read.table("2023_09_04/izoard.txt", h=TRUE)
View(izoard)
plot(izoard$distance, izoard$altitude, xlab="ascent distance",ylab="altitude")
plot(izoard$distance, izoard$altitude, xlab="ascent distance",ylab="altitude")
nrow(izoard) # 126 points
nrow(izoard) # 126 points
nrow(izoard) # 101 points
# nbasis = norder + number_of_knots
# number_of_knots = 101 - 2 (knot divide space)
# => 99 knots, norder = 99 + 4 = 103
nbasis <- 103
basis <- create.bspline.basis(rangeval=c(0,20), nbasis=nbasis, norder=m)
library(fda)
nrow(izoard) # 101 points
# nbasis = norder + number_of_knots
# number_of_knots = 101 - 2 (knot divide space)
# => 99 knots, norder = 99 + 4 = 103
nbasis <- 103
basis <- create.bspline.basis(rangeval=c(0,20), nbasis=nbasis, norder=m)
m <- 4 # "using a basis of CUBIC B-splines", then we have to use norder = 4
nrow(izoard) # 101 points
# nbasis = norder + number_of_knots
# number_of_knots = 101 - 2 (knot divide space)
# => 99 knots, norder = 99 + 4 = 103
nbasis <- 103
basis <- create.bspline.basis(rangeval=c(0,20), nbasis=nbasis, norder=m)
basis$nbasis
abscissa <- izoard$distance # seq(0,25,0.2)
# but, we can use "breaks" to automaticall indicate number of knots (nbasis)
breaks <- abscissa
basis_breaks <- create.bspline.basis(rangeval=c(0,25), breaks=breaks, norder=m)
basis_breaks$nbasis
functionalPar <- fdPar(fdobj=basis, lambda=0.1) # Lfdobj=m-2 penalise second derivative
Xobs0 <- izoard$altitude
Xss <- smooth.basis(abscissa, Xobs0, functionalPar)
Xss0 <- eval.fd(abscissa, Xss$fd, Lfd=0) # zero derivative (fit to data)
Xss1 <- eval.fd(abscissa, Xss$fd, Lfd=1) # first derivative (fit to first derivative)
Xss2 <- eval.fd(abscissa, Xss$fd, Lfd=2)
gcv <- Xss$gcv  #  the value of the GCV statistic (Generalisd cross validation)
gcv # 1490.361
plot(abscissa,Xobs0, type = "l")
plot(basis)
df <- Xss$df   #  number of parameters (so dimension of space)
df  # 14.31518
cat("The fitted curve lives in a functional space of approximately", round(df,3), " dimensions.")
par(mfrow=c(1,2))
plot(abscissa, Xss0, type = "l", lwd = 2, col="red", ylab="fitted curve (RED)")
points(abscissa, Xobs0)
plot(abscissa, Xss1, type = "l", lwd = 2, col="blue", ylab="first derivative")
# generalized cross-validation
lambda <- 10^seq(-1,3, by = 0.5)
gcv <- numeric(length(lambda)) # GCV - general cross validation
for (i in 1:length(lambda)){
functionalPar <- fdPar(fdobj=basis, Lfdobj=m-2, lambda=lambda[i])
gcv[i] <- smooth.basis(abscissa, Xobs0, functionalPar)$gcv
}
par(mfrow=c(1,1))
plot(log10(lambda),gcv)
lambda[which.min(gcv)] # 3.162278
abline(v = log10(lambda[which.min(gcv)]), col = 2)
opt_lambda <- lambda[which.min(gcv)]
cat("Optimal lambda:", opt_lambda, "\nGCV error:", min(gcv), "\n")
# Refit
functionalPar_refit <- fdPar(fdobj=basis, Lfdobj=m-2, lambda=3.162278)
Xss_refit <- smooth.basis(abscissa, Xobs0, functionalPar_refit)
Xss0_refit <- eval.fd(abscissa, Xss_refit$fd, Lfd=0) # zero derivative (fit to data)
Xss1_refit <- eval.fd(abscissa, Xss_refit$fd, Lfd=1) # first derivative (fit to first derivative)
Xss2_refit <- eval.fd(abscissa, Xss_refit$fd, Lfd=2)
par(mfrow=c(1,2))
plot(abscissa, Xss0_refit, type = "l", lwd = 2, col="red", ylab="fitted curve (RED)")
points(abscissa, Xobs0)
plot(abscissa, Xss1_refit, type = "l", lwd = 2, col="blue", ylab="first derivative")
# d) Calculate the slope at the steepest point of the ascent.
max_slope <- max(Xss1_refit)
max_index <- which.max(Xss1_refit)
max_distance <- abscissa[max_index]
max_distance # 11.2
help(chickwts)
head(chickwts)
dim(chickwts)
summary(chickwts)
attach(chickwts)
chickwts
bartlett.test(weight, feed)
fit <- aov(weight ~ feed)
summary(fit)
weight
# Number of groups = 5 + 1 = 6
unique(feed)
# Number of groups = 5 + 1 = 6
length(unique(feed))
# Number of observations: n - g = 65 => n = 65 + 6 = 71
nrow(feed)
# Number of observations: n - g = 65 => n = 65 + 6 = 71
length(feed)
231129/5
SStreat <- summary(fit)$SStreat
# Value of F-statistic:
# F0 =  (1/(g-1) * SStreat) / (1/(n-g) * SSres) ~ F(g-1, n-g)
F0 <- (1/(6-1) * SStreat) / (1/(71-6) * SSres)
SStreat <- 231129
SSres   <- 195556
# Value of F-statistic:
# F0 =  (1/(g-1) * SStreat) / (1/(n-g) * SSres) ~ F(g-1, n-g)
F0 <- (1/(6-1) * SStreat) / (1/(71-6) * SSres)
F0
qf(1-alpha, 6-1, 71-6)
## p-value ---------------------------------------------------------------------
# We reject H0 at level alpha if F0 > F_1-alpha_(g-1, n-g) (quantile level of 1-alpha)
alpha <- 0.05
qf(1-alpha, 6-1, 71-6)
p_value <- 1 - pf(F0, 6-1, 71-6)
p_value
attach(iris)
species.name <- factor(Species, labels=c('setosa','versicolor','virginica'))
iris4        <- iris[,1:4] # variables
iris4
species.name
fit <- manova(as.matrix(iris4) ~ species.name) # "Pillai", "Wilks", "Hotelling-Lawley", "Roy" -
# - it's or matricies with differen coefficients [1, -1], [1, 0, -1]
summary.manova(fit)
# Via ANOVA: for each of the p=4 variables we perform an ANOVA test
#            to verify if the membership to a group has influence
#            on the mean of the variable (we explore separately the
#            4 axes directions in R^4)
summary.aov(fit)
# - it's or matricies with differen coefficients [1, -1], [1, 0, -1]
summary.manova(fit)
# - it's or matricies with differen coefficients [1, -1], [1, 0, -1]
summary.manova(fit)
# Access E and H matrices
manova_model <- manova(as.matrix(iris[,1:4]) ~ iris$Species)
H <- crossprod(fitted(manova_model))   # H = hypothesis SSCP
E <- crossprod(residuals(manova_model)) # E = error SSCP
# Wilks' Lambda:
wilks_lambda <- det(E) / det(E + H)
wilks_lambda
# - it's or matricies with differen coefficients [1, -1], [1, 0, -1]
summary.manova(fit)
# Degrees of freedom:
g <- length(unique(iris$Species))  # 3
p <- ncol(iris[,1:4])              # 4
n <- nrow(iris)                    # 150
v <- n - g                         # residual degrees of freedom
g
fit <- manova(as.matrix(iris[,1:4]) ~ iris$Species)
summary_fit <- summary(fit, test = "Wilks")
summary_fit
wilks_lambda
library(MASS)
library(car)
library(rgl)
library(glmnet)
students <- read.table("2023_11_07/students.txt", h=TRUE)
lm_model <- lm(
watchtv ~ -1 + as.factor(gender) + age + height + distance + siblings + computertime + exercisehours + musiccds + playgames,
data=students)
summary(lm_model)
# Build the matrix of predictors
x <- model.matrix(watchtv ~ -1 + as.factor(gender) + age + height + distance + siblings + computertime + exercisehours + musiccds + playgames, data=students)#[,-1]
# Build the vector of response
y <- students$watchtv
fit.lasso <- glmnet(x, y, lambda = 0.3)
plot(fit.lasso, xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)
coef(fit.lasso) < 0.0001
grid <- seq(0.01, 10,length=1000)
lasso.mod <- glmnet(x,y,alpha=1,lambda=grid)
plot(lasso.mod, xvar='lambda',label=TRUE)
# choosing the parameter lambda
set.seed(20231108)
cv.out <- cv.glmnet(x,y,alpha=1,nfold=3,lambda=grid)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
abline(v=(bestlam.lasso))
plot(lasso.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam.lasso))
coef(cv.out, s = "lambda.min")
min(cv.out$cvm)
