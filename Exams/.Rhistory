lm_model <- lm(energy ~ -1 + as.factor(diet) + sugar, data = sugar_energy)
summary(lm_model)
lm_model$coefficients[2] # betta_0_2 = 1.071348
lm_model$coefficients[201] # betta_1 = 0.5116741
lme_model <- lme(energy ~ 1 + sugar, random = ~1|as.factor(diet), data = sugar_energy)
summary(lme_model)
sigma2 <- get_variance_random(lme_model)
sqrt(sigma2) # sigma_b = 0.9410168
VarCorr(lme_model)
lme_model22 <- lme(energy ~ 1 + sugar, random = ~1+sugar|as.factor(diet), data = sugar_energy)
summary(lme_model22)
n_obs <- nobs(lme_model22)          # 2101
n_fixef <- length(fixef(lme_model22))  # 2
df_resid <- n_obs - n_fixef         # приблизительная оценка
df_resid
summary(lme_model22)$tTable
random_effects <- ranef(lme_model22)
head(random_effects)
lme_model2 <- lme(energy ~ 1 + sugar:as.factor(diet), random = ~1|as.factor(diet), data = sugar_energy)
summary(lme_model2)
# d) Comment on whether M1 or M2 better explains the data, supporting your answer with an appropriate test.
anova(lme_model, lme_model2, lme_model22) # 22 - model with random intercept and slope has only 6 params
# d) Comment on whether M1 or M2 better explains the data, supporting your answer with an appropriate test.
anova(lme_model, lme_model22) # 22 - model with random intercept and slope has only 6 params
pc.athlete_stats$scale
pc.athlete_stats.sd$scale
princomp(athlete_stats, center=TRUE, scores=T)
princomp(athlete_stats, center=TRUE, scores=T)
princomp(athlete_stats, scores=T)
summary(pc.athlete_stats)
pc.athlete_stats$loadings
head(pc.athlete_stats$scores)
help(princomp)
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
barplot(pc.athlete_stats$sdev^2, las = 2, main = 'Principal Components', ylim = c(0, 1), ylab = 'Variances')
abline(h = 1, col = 'blue')
barplot(sapply(athlete_stats, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 1),
ylab = 'Variances')
plot(cumsum(pc.athlete_stats$sdev^2) / sum(pc.athlete_stats$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of Components', ylab = 'Contribution to the Total Variance', ylim = c(0, 1))
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(athlete_stats), labels = 1:ncol(athlete_stats), las = 2)
# Loadings
n_lod_show <- 3
athlete_stats <- read.table("2025_01_17/athlete_stats.txt", h=TRUE)
n <- dim(athlete_stats)[1]
p <- dim(athlete_stats)[2]
# Exploration
# I would not standartise variabels, due to each feature have almost same sample variance
boxplot(athlete_stats, col = 'gold', main = 'Original Variables')
pc.athlete_stats <- princomp(athlete_stats, scores=T)
pc.athlete_stats
summary(pc.athlete_stats)
pc.athlete_stats$loadings
head(pc.athlete_stats$scores)
help(princomp)
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
barplot(pc.athlete_stats$sdev^2, las = 2, main = 'Principal Components', ylim = c(0, 1), ylab = 'Variances')
abline(h = 1, col = 'blue')
barplot(sapply(athlete_stats, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 1),
ylab = 'Variances')
plot(cumsum(pc.athlete_stats$sdev^2) / sum(pc.athlete_stats$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of Components', ylab = 'Contribution to the Total Variance', ylim = c(0, 1))
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(athlete_stats), labels = 1:ncol(athlete_stats), las = 2)
# Loadings
n_lod_show <- 3
load.athlete_stats <- pc.athlete_stats$loadings
par(mar = c(1, n_lod_show, 0, 2), mfrow = c(n_lod_show, 1))
for (i in 1:n_lod_show) {
barplot(load.athlete_stats[, i], ylim = c(-1, 1))
}
# c) Report the biplot of the data along the second and the third principal components. How would you qualify the
# athlete labeled as 78 (row index) based on the biplot?
x_coord <- pc.athlete_stats$scores[78, 2]
y_coord <- pc.athlete_stats$scores[78, 3]
biplot(pc.athlete_stats, choices = c(2, 3)) # how to change Principal components, where we plot our data (PC2 and PC3)
points(x_coord, y_coord, col = "red", pch = 19, cex = 1.5)
text(x_coord, y_coord, labels = "78", pos = 4, col = "red")
new_obs <- c(1.85, 1.74, 1.92, 1.89, 1.78, 1.81, 1.69, 1.76, 1.84)
new_obs_df <- as.data.frame(t(new_obs))
colnames(new_obs_df) <- colnames(athlete_stats)
pc.athlete_stats$loadings[,]
pc.athlete_stats$center
# what happen if don't centralise input
t(as.matrix(new_obs)) %*% as.matrix(pc.athlete_stats$loadings)
colMeans(athlete_stats)
pc.athlete_stats$center
pc.athlete_stats$scale
diag(var(athlete_stats))
# If centralise
t(as.matrix(new_obs - colMeans(athlete_stats))) %*% as.matrix(pc.athlete_stats$loadings)
predict(pc.athlete_stats, new_obs_df) # model knowing, that data has not zero mean, centralise input by automatically
pc.athlete_stats$center
pc.athlete_stats$scale
t(as.matrix(new_obs - colMeans(athlete_stats))) %*% as.matrix(pc.athlete_stats$loadings)
predict(pc.athlete_stats.sd, (new_obs_df)/(apply(athlete_stats, 2, sd)))
/(apply(athlete_stats, 2, sd))
(apply(athlete_stats, 2, sd))
attr(athlete_stats.sd, "scaled:scale")
t(as.matrix(new_obs - colMeans(athlete_stats))/(apply(athlete_stats, 2, sd))) %*% as.matrix(pc.athlete_stats$loadings)
predict(pc.athlete_stats.sd, (new_obs_df)/(apply(athlete_stats, 2, sd)))
pc.athlete_stats.sd$center
predict(pc.athlete_stats.sd, (new_obs_df- colMeans(athlete_stats))/(apply(athlete_stats, 2, sd)))
t(as.matrix(new_obs - colMeans(athlete_stats))/(apply(athlete_stats, 2, sd))) %*% as.matrix(pc.athlete_stats$loadings)
std <- (apply(athlete_stats, 2, sd))
std <- attr(athlete_stats.sd, "scaled:scale")
std <- (apply(athlete_stats, 2, sd))
std <- apply(athlete_stats, 2, sd)
means <- colMeans(athlete_stats)
t(as.matrix(new_obs - means)/std) %*% as.matrix(pc.athlete_stats.sd$loadings)
predict(pc.athlete_stats.sd, (new_obs_df - colMeans(athlete_stats))/(apply(athlete_stats, 2, sd)))
predict(pc.athlete_stats.sd, (new_obs_df)/(apply(athlete_stats, 2, sd)))
predict(pc.athlete_stats.sd, (new_obs_df - means)/std)
t(as.matrix(new_obs - means)/std) %*% as.matrix(pc.athlete_stats.sd$loadings)
predict(pc.athlete_stats.sd, (new_obs_df - means)/std)
# About prediction -------------------------------------------------------------
t(as.matrix(new_obs - colMeans(athlete_stats))) %*% as.matrix(pc.athlete_stats$loadings)
predict(pc.athlete_stats, new_obs_df)
plants_mood <- read.table("2025_06_12/plants_mood.txt", h=TRUE)
summary(lm(mood ~ zone_id, data = plants_mood))
lme_model <- lme(mood ~ plants + light + social, random =
~1|as.factor(zone_id), data = plants_mood)
summary(lme_model)
summary(lme_model2)
lme_model2 <- lme(mood ~ plants + light + social, random =
~1|as.factor(zone_id),
weights = varPower(form = ~social) ,data = plants_mood)
summary(lme_model2)
# c) Should M2 be preferred over M1? Support your answer with a test.
anova(lme_model, lme_model2) # so model with heteroscedastic residuals better
# d) Estimate (using M2) the mood of a person having 5 plants,
# 12 hours of natural light exposure and 5 hours of in-person interaction per day.
test_data = data.frame(zone_id= '21', plants=5, light = 12, social=5)
predict(lme_model2, test_data, level = FALSE)
var_b2 <- get_variance_random(lme_model2)
var_eps2 <- get_variance_residual(lme_model2)
sqrt(var_eps2)
music <- read.table("./20250206/music.txt", h=TRUE)
classical   <- factor(music$classical) # Treat.1
upbeat   <- factor(music$upbeat) # Treat.2
cls_up <- factor(paste(classical, upbeat, sep=''))
cls_up
music_data <- music[, 1:2]
music <- read.table("2025_02_06/music.txt", h=TRUE)
classical   <- factor(music$classical) # Treat.1
upbeat   <- factor(music$upbeat) # Treat.2
cls_up <- factor(paste(classical, upbeat, sep=''))
cls_up
music_data <- music[, 1:2]
fit <- manova(as.matrix(music_data) ~ classical + upbeat + classical:upbeat)
summary.manova(fit)
# b) Identify and check the assumptions of the model introduced in (a).
# we need check MVN and homogenity
Ps <- c(mvn(music_data[cls_up==levels(cls_up)[1],],)$multivariateNormality$`p value`,
mvn(music_data[cls_up==levels(cls_up)[2],],)$multivariateNormality$`p value`,
mvn(music_data[cls_up==levels(cls_up)[3],],)$multivariateNormality$`p value`,
mvn(music_data[cls_up==levels(cls_up)[4],],)$multivariateNormality$`p value`)
Ps # all values > 0.05 (So we can not reject H0 of normlaity assumptions)
S1 <-  cov(music_data[cls_up==levels(cls_up)[1],])
S2 <-  cov(music_data[cls_up==levels(cls_up)[2],])
S3 <-  cov(music_data[cls_up==levels(cls_up)[3],])
S4 <-  cov(music_data[cls_up==levels(cls_up)[4],])
summary(boxM(music_data, cls_up)) # also same covariance structure
fit2 <- manova(as.matrix(music_data) ~ classical + upbeat)
summary.manova(fit2)
sum(cls_up == "FALSETRUE")
sum(cls_up == "FALSEFALSE")
athlete_stats <- read.table("2025_01_17/athlete_stats.txt", h=TRUE)
n <- dim(athlete_stats)[1]
p <- dim(athlete_stats)[2]
# Exploration
# I would not standartise variabels, due to each feature have almost same sample variance
boxplot(athlete_stats, col = 'gold', main = 'Original Variables')
# what happen if don't centralise input
t(as.matrix(new_obs - colMeans(athlete_stats))) %*% as.matrix(pc.athlete_stats$loadings)
predict(pc.athlete_stats, new_obs_df)
# to check this assumtpion we can run shapiro test (to check normality),
# and look on graphics of model
shapiro.test(lm0$residuals) # Can not reject H0 of nresiduals normality
plot(lm0$residuals)
crop_yields <- read.table("2025_01_17/crop_yield.txt", h=TRUE)
lm0 <- lm(formula = yield ~ -1 + as.factor(irrigation) + temperature + rainfall + sunny + soil_quality + fertilizer,
data=crop_yields)
summary(lm0)
par(mfrow=c(2,2)); plot(lm0)
# plot to confirm that residuals are normal
qqnorm(lm0$residuals)
qqline(resid(lm0), col='red', lwd=2)
plot(lm0, which = 1) # plot to confirm that residuals are normal
plot(lm0, which = 2) # plot showing homoscedasticity
# to check this assumtpion we can run shapiro test (to check normality),
# and look on graphics of model
shapiro.test(lm0$residuals) # Can not reject H0 of nresiduals normality
plot(lm0$residuals)
plot(lm0$residuals)
confint(lm0, level=0.99)
confint(lm0, level=0.99)
confint(lm0, "temperature", level=0.99)
co2 <- read.table("2024_09_06/co2.txt", h=TRUE)
#
lme1 <- lme(co2 ~ 1 + purchases + heating + flights, random = ~1|as.factor(IDCity), data=co2)
summary(lme1)
# pvre:
var_b <- get_variance_random(lme1)
var_eps <- get_variance_residual(lme1)
var_b # 39.01042
co2 <- read.table("2024_09_06/co2.txt", h=TRUE)
#
lme1 <- lme(co2 ~ 1 + purchases + heating + flights, random = ~1|as.factor(IDCity), data=co2)
summary(lme1)
# pvre:
var_b <- get_variance_random(lme1)
var_eps <- get_variance_residual(lme1)
var_b # 39.01042
var_eps # 2.917392
PVRE <- var_b/(var_b+var_eps) # we can take them from summary as ^2 of StdDev
PVRE # 0.9304187 pretty high pvre, there is some random effect
lme2 <- lme(co2 ~ 1 + purchases + heating + flights,
random = ~1|as.factor(IDCity),
weights = varPower(form = ~purchases),
data=co2)
summary(lme2)
var_b2 <- get_variance_random(lme2)
var_eps2 <- get_variance_residual(lme2)
var_b2 # 39.37599
var_eps2 # 1.011572
PVRE2 <- var_b2/(var_b2+var_eps2) # we can take them from summary as ^2 of StdDev
PVRE2 # 0.9749534 almost didn't change, so probably there are no reason to use heteroscedastic residuals
PVRE # 0.9304187
# Perform a likelihood ratio test to compare M1 and M2. Which model would you choose?
anova(lme1, lme2) # p-value < 0.05 so we can reject H0 of zero not importance of heteroscedastic
ranef(summary(lme2))
max(ranef(summary(lme2)))
which(max(ranef(summary(lme2))))
which(ranef(summary(lme2)), max(ranef(summary(lme2))))
where(ranef(summary(lme2)), max(ranef(summary(lme2))))
max(ranef(summary(lme2))))
max(ranef(summary(lme2)))
ranef(summary(lme2))
ranef(summary(lme2))$Intercept #[max(ranef(summary(lme2)))
ranef(summary(lme2))$intercept #[max(ranef(summary(lme2)))
ranef(summary(lme2))[, 1] #[max(ranef(summary(lme2)))
ranef(summary(lme2))[, 1] == [max(ranef(summary(lme2)))
ranef(summary(lme2))[, 1] == max(ranef(summary(lme2)))
ranef(summary(lme2))[ranef(summary(lme2))[, 1] == max(ranef(summary(lme2))),]
ranef(summary(lme2))[ranef(summary(lme2))[, 1] == max(ranef(summary(lme2)))]
ranef(summary(lme2))[ranef(summary(lme2))[, 1] == max(ranef(summary(lme2))), ]
max(ranef(summary(lme2)))
stelvio <- read.table("2024_09_06/stelvio.txt", h=TRUE)
stelvio
plot(stelvio$distance, stelvio$altitude, xlab="ascent distance",ylab="altitude")
help(create.bspline.basis)
m <- 4 # "using a basis of CUBIC B-splines", then we have to use norder = 4
nrow(stelvio) # 126 points
# nbasis = norder + number_of_knots
# number_of_knots = 126 - 2 (knot divide space)
# => 124 knots, norder = 124 + 4 = 128
nbasis <- 128
basis <- create.bspline.basis(rangeval=c(0,25), nbasis=nbasis, norder=m)
basis$nbasis
abscissa <- stelvio$distance # seq(0,25,0.2)
# but, we can use "breaks" to automaticall indicate number of knots (nbasis)
breaks <- abscissa
basis_breaks <- create.bspline.basis(rangeval=c(0,25), breaks=breaks, norder=m)
library(fda)
stelvio <- read.table("2024_09_06/stelvio.txt", h=TRUE)
stelvio
plot(stelvio$distance, stelvio$altitude, xlab="ascent distance",ylab="altitude")
help(create.bspline.basis)
m <- 4 # "using a basis of CUBIC B-splines", then we have to use norder = 4
nrow(stelvio) # 126 points
# nbasis = norder + number_of_knots
# number_of_knots = 126 - 2 (knot divide space)
# => 124 knots, norder = 124 + 4 = 128
nbasis <- 128
basis <- create.bspline.basis(rangeval=c(0,25), nbasis=nbasis, norder=m)
basis$nbasis
abscissa <- stelvio$distance # seq(0,25,0.2)
# but, we can use "breaks" to automaticall indicate number of knots (nbasis)
breaks <- abscissa
basis_breaks <- create.bspline.basis(rangeval=c(0,25), breaks=breaks, norder=m)
basis_breaks$nbasis
functionalPar <- fdPar(fdobj=basis, Lfdobj=m-2, lambda=1) # Lfdobj=m-2 penalise second derivative
Xobs0 <- stelvio$altitude
Xss <- smooth.basis(abscissa, Xobs0, functionalPar)
Xss0 <- eval.fd(abscissa, Xss$fd, Lfd=0) # zero derivative (fit to data)
Xss1 <- eval.fd(abscissa, Xss$fd, Lfd=1) # first derivative (fit to first derivative)
Xss2 <- eval.fd(abscissa, Xss$fd, Lfd=2)
gcv <- Xss$gcv  #  the value of the GCV statistic (Generalisd cross validation)
gcv # 1490.361
plot(abscissa,Xobs0, type = "l")
plot(basis)
df <- Xss$df   #  number of parameters (so dimension of space)
df  # 14.31518
cat("The fitted curve lives in a functional space of approximately", round(df,3), " dimensions.")
par(mfrow=c(1,2))
plot(abscissa, Xss0, type = "l", lwd = 2, col="red", ylab="fitted curve (RED)")
points(abscissa, Xobs0)
plot(abscissa, Xss1, type = "l", lwd = 2, col="blue", ylab="first derivative")
# generalized cross-validation
lambda <- 10^seq(-1,3, by = 0.5)
gcv <- numeric(length(lambda)) # GCV - general cross validation
for (i in 1:length(lambda)){
functionalPar <- fdPar(fdobj=basis, Lfdobj=m-2, lambda=lambda[i])
gcv[i] <- smooth.basis(abscissa, Xobs0, functionalPar)$gcv
}
par(mfrow=c(1,1))
plot(log10(lambda),gcv)
lambda[which.min(gcv)] # 3.162278
abline(v = log10(lambda[which.min(gcv)]), col = 2)
opt_lambda <- lambda[which.min(gcv)]
cat("Optimal lambda:", opt_lambda, "\nGCV error:", min(gcv), "\n")
# Refit
functionalPar_refit <- fdPar(fdobj=basis, Lfdobj=m-2, lambda=3.162278)
Xss_refit <- smooth.basis(abscissa, Xobs0, functionalPar_refit)
Xss0_refit <- eval.fd(abscissa, Xss_refit$fd, Lfd=0) # zero derivative (fit to data)
Xss1_refit <- eval.fd(abscissa, Xss_refit$fd, Lfd=1) # first derivative (fit to first derivative)
Xss2_refit <- eval.fd(abscissa, Xss_refit$fd, Lfd=2)
par(mfrow=c(1,2))
plot(abscissa, Xss0_refit, type = "l", lwd = 2, col="red", ylab="fitted curve (RED)")
points(abscissa, Xobs0)
plot(abscissa, Xss1_refit, type = "l", lwd = 2, col="blue", ylab="first derivative")
# d) Calculate the slope at the steepest point of the ascent.
max_slope <- max(Xss1_refit)
max_index <- which.max(Xss1_refit)
max_distance <- abscissa[max_index]
max_distance # 11.2
load <- read.table("2024_06_13/load.txt", h=TRUE)
load
View(load)
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
data <- t(load[, 1:25]) # we have to plot in hours domain, so need to transopse data
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
plot.fd(data_W.fd)
plot.fd(data_W.fd)
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
nbasis <- 13
time <- seq(0,24, by=1)
basis <- create.fourier.basis(rangeval = c(0,24), nbasis = nbasis)
plot(basis)
data_W.fd <- Data2fd(y=data, argvals=time, basisobj=basis)
plot.fd(data_W.fd)
pca_W <- pca.fd(data_W.fd, nharm=5, centerfns=TRUE)
# scree plot
# we had 13 basis function -> we will have 13 components
# pca.fd computes all the 13 eigenvalues,
plot(pca_W$values[1:35],xlab='j',ylab='Eigenvalues')
plot(cumsum(pca_W$values)[1:35]/sum(pca_W$values),xlab='j',ylab='CPV',ylim=c(0.8,1))
# scree plot
# we had 13 basis function -> we will have 13 components
# pca.fd computes all the 13 eigenvalues,
plot(pca_W$values[1:35],xlab='j',ylab='Eigenvalues')
plot(cumsum(pca_W$values)[1:35]/sum(pca_W$values),xlab='j',ylab='CPV',ylim=c(0.8,1))
# we have 35 data, (features), so we can not plot more principal components
pca_W$values
cumsum(pca_W$values)[1:35]/sum(pca_W$values) # we have 13 PC, because we used basis of 13 function
layout(cbind(1,2))
plot(pca_W$harmonics[1,],col=1,ylab='FPC1',ylim=c(0.0,0.4))
abline(h=0,lty=2)
plot(pca_W$harmonics[2,],col=2,ylab='FPC2',ylim=c(-0.5,0.5))
layout(cbind(1,2))
plot(pca_W$harmonics[1,],col=1,ylab='FPC1',ylim=c(0.0,0.4))
abline(h=0,lty=2)
plot(pca_W$harmonics[2,],col=2,ylab='FPC2',ylim=c(-0.5,0.5))
# plot of the FPCs as perturbation of the mean
media <- mean.fd(data_W.fd)
plot(media,lwd=2,ylim=c(0,60),ylab='temperature',main='FPC1')
lines(media+pca_W$harmonics[1,]*sqrt(pca_W$values[1]), col=2)
lines(media-pca_W$harmonics[1,]*sqrt(pca_W$values[1]), col=3)
plot(media,lwd=2,ylim=c(0,60),ylab='temperature',main='FPC2')
lines(media+pca_W$harmonics[2,]*sqrt(pca_W$values[2]), col=2)
lines(media-pca_W$harmonics[2,]*sqrt(pca_W$values[2]), col=3)
# USE THIS TO EXPLAIN COMPONENTS
# Command of the library fda that automatically does these plots
par(mfrow=c(1,2))
plot(pca_W, nx=100, pointplot=TRUE, harm=c(1,2), expand=0, cycle=FALSE)
# USE THIS TO EXPLAIN COMPONENTS
# Command of the library fda that automatically does these plots
par(mfrow=c(1,2))
plot(pca_W, nx=100, pointplot=TRUE, harm=c(1,2), expand=0, cycle=FALSE)
# By LDA
lda.loads <- lda(load$daytype ~ pca_W$scores[,1:5]) # using first 5 components
lda.scores <- predict(lda.loads)$x # 1D projection
# Plot the LDA 1D scores
boxplot(lda.scores ~ load$daytype,
main="LDA 1D projection",
xlab="Day type", ylab="LDA score")
# By FDA
scores <- pca_W$scores
plot(scores[, 1], scores[, 2], col = ifelse(load$daytype == "Working day", "blue", "red"),
pch = 19, xlab = "PC1", ylab = "PC2", main = "Scores of First Two Principal Components")
legend("topright", legend = c("Working day", "Holiday"), col = c("blue", "red"), pch = 19)
data_pca <- data.frame(
pc1=scores[, 1],
pc2=scores[, 2]
)
g <- 2
i1 <- which(load$daytype=='Holiday')
i2 <- which(load$daytype=='Working day')
n1 <- length(i1)
# By FDA
scores <- pca_W$scores
plot(scores[, 1], scores[, 2], col = ifelse(load$daytype == "Working day", "blue", "red"),
pch = 19, xlab = "PC1", ylab = "PC2", main = "Scores of First Two Principal Components")
pc.athlete_stats$loadings
load.athlete_stats.sd
athlete_stats <- read.table("2025_01_17/athlete_stats.txt", h=TRUE)
n <- dim(athlete_stats)[1]
p <- dim(athlete_stats)[2]
# Exploration
# I would not standartise variabels, due to each feature have almost same sample variance
boxplot(athlete_stats, col = 'gold', main = 'Original Variables')
pc.athlete_stats <- princomp(athlete_stats, scores=T)
pc.athlete_stats
summary(pc.athlete_stats)
pc.athlete_stats$loadings
head(pc.athlete_stats$scores)
help(princomp)
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
barplot(pc.athlete_stats$sdev^2, las = 2, main = 'Principal Components', ylim = c(0, 1), ylab = 'Variances')
abline(h = 1, col = 'blue')
barplot(sapply(athlete_stats, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 1),
ylab = 'Variances')
plot(cumsum(pc.athlete_stats$sdev^2) / sum(pc.athlete_stats$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of Components', ylab = 'Contribution to the Total Variance', ylim = c(0, 1))
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(athlete_stats), labels = 1:ncol(athlete_stats), las = 2)
# Loadings
n_lod_show <- 3
load.athlete_stats <- pc.athlete_stats$loadings
par(mar = c(1, n_lod_show, 0, 2), mfrow = c(n_lod_show, 1))
for (i in 1:n_lod_show) {
barplot(load.athlete_stats[, i], ylim = c(-1, 1))
}
pc.athlete_stats$loadings
wheelworks <- read.table("2023_09_04/wheelworks.txt", h=TRUE)
cyclecraft <- read.table("2023_09_04/cyclecraft.txt", h=TRUE)
boxplot(wheelworks[,2:3])
boxplot(cyclecraft[,2:3])
data_difference <- data.frame(
delta = wheelworks[,2:3] - cyclecraft[,2:3]
)
View(data_difference)
wheelworks <- read.table("2023_09_04/wheelworks.txt", h=TRUE)
View(wheelworks)
load <- read.table("2024_06_13/load.txt", h=TRUE)
load
load <- read.table("2024_06_13/load.txt", h=TRUE)
load
data <- t(load[, 1:25]) # we have to plot in hours domain, so need to transopse data
View(load)
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
basis <- create.bspline.basis(rangeval=c(0,25), nbasis=nbasis, norder=m)
basis$nbasis
basis <- create.bspline.basis(rangeval=c(0,25), nbasis=nbasis, norder=m)
m <- 4 # "using a basis of CUBIC B-splines", then we have to use norder = 4
nrow(stelvio) # 126 points
# nbasis = norder + number_of_knots
# number_of_knots = 126 - 2 (knot divide space)
# => 124 knots, norder = 124 + 4 = 128
nbasis <- 128
basis <- create.bspline.basis(rangeval=c(0,25), nbasis=nbasis, norder=m)
basis$nbasis
abscissa <- stelvio$distance # seq(0,25,0.2)
stelvio <- read.table("2024_09_06/stelvio.txt", h=TRUE)
stelvio
View(stelvio)
plot(stelvio$distance, stelvio$altitude, xlab="ascent distance",ylab="altitude")
help(create.bspline.basis)
m <- 4 # "using a basis of CUBIC B-splines", then we have to use norder = 4
nrow(stelvio) # 126 points
# nbasis = norder + number_of_knots
# number_of_knots = 126 - 2 (knot divide space)
# => 124 knots, norder = 124 + 4 = 128
nbasis <- 128
basis <- create.bspline.basis(rangeval=c(0,25), nbasis=nbasis, norder=m)
basis$nbasis
abscissa <- stelvio$distance # seq(0,25,0.2)
functionalPar <- fdPar(fdobj=basis, Lfdobj=m-2, lambda=1) # Lfdobj=m-2 penalise second derivative
Xobs0 <- stelvio$altitude
Xss <- smooth.basis(abscissa, Xobs0, functionalPar)
Xss0 <- eval.fd(abscissa, Xss$fd, Lfd=0) # zero derivative (fit to data)
Xss1 <- eval.fd(abscissa, Xss$fd, Lfd=1) # first derivative (fit to first derivative)
Xss2 <- eval.fd(abscissa, Xss$fd, Lfd=2)
gcv <- Xss$gcv  #  the value of the GCV statistic (Generalisd cross validation)
gcv # 1490.361
plot(abscissa,Xobs0, type = "l")
plot(basis)
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
nbasis <- 13
time <- seq(0,24, by=1)
basis <- create.fourier.basis(rangeval = c(0,24), nbasis = nbasis)
plot(basis)
data_W.fd <- Data2fd(y=data, argvals=time, basisobj=basis)
plot.fd(data_W.fd)
data_W.fd$coefs[1:3, 1] # 116.344769 -12.747130  -2.863551
data_W.fd$coefs[, 1]
# Same procedure:
Xsp <- smooth.basis(argvals=time, y=data, fdParobj=basis) #
Xsp0 = eval.fd(time, Xsp$fd)
plot(Xsp) # same as previously
load <- read.table("2024_06_13/load.txt", h=TRUE)
load
data <- t(load[, 1:25]) # we have to plot in hours domain, so need to transopse data
matplot(data, type='l',main='Kwat consumption', xlab='Hours', ylab='365 days')
nbasis <- 13
