set.seed(1)
iris2 <- iris[,1:2] + cbind(rnorm(150, sd=0.025))    # jittering
# setosa VS versicolor+virginica
y <- rep(0,150)
y[which(species.name=='setosa')] <- 1
plot(iris2[,1], iris2[,2], xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
col=as.character(y+1))
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel='linear', cost=100, scale=FALSE )
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor (y))
svmfit <- svm(y~., data=dat , kernel ='linear', cost =1, scale =FALSE )
summary(svmfit)
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
species.name <- factor(iris$Species, labels=c('setosa','versicolor','virginica'))
set.seed(1)
iris2 <- iris[,1:2] + cbind(rnorm(150, sd=0.025))    # jittering
# setosa VS versicolor+virginica
y <- rep(0,150)
y[which(species.name=='setosa')] <- 1
plot(iris2[,1], iris2[,2], xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
col=as.character(y+1))
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel='linear', cost=100, scale=FALSE )
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor (y))
svmfit <- svm(y~., data=dat , kernel ='linear', cost =1, scale =FALSE )
summary(svmfit)
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
## Linear case --------------------------------------------------------------------------------
# Generate the data
set.seed(123)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
# The classes are not separable
plot(x, col =ifelse(y==1, 'blue', 'red'),
pch=19, xlab='x1', ylab='x2', asp=1)
# Fit the Support Vector Classifier (kernel = "linear")
# given a cost C
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel ='linear', cost=10, scale=FALSE)
# in general case - recommended to scale data
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit, dat, col =c('salmon', 'light blue'), pch=19, asp=1)
par(mfrow=c(1,1))
# support vectors are indicated with crosses
# they are:
svmfit$index
# If we try to change the cost parameter we get more support points
# (higher bias, lower variance)
svmfit <- svm(y~., data=dat , kernel ='linear', cost=0.1, scale=FALSE )
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# To set the parameter C we can use the function tune(),
# which is based on cross-validation (10-fold)
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel = 'linear',
ranges = list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
# Extract the best model from the result of tune
# when we increase cost - we look fever (меньше) support vector,
# and make boundary depended only on closees vectors
# lower cost - bigger number of support vectors
bestmod <- tune.out$best.model
summary(bestmod)
plot(bestmod , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# Prediction for a new observation (command predict())
xtest <- matrix(rnorm (20*2) , ncol =2)
ytest <- sample(c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,] <- xtest[ytest ==1,] + 1
testdat <- data.frame(x=xtest , y=as.factor(ytest))
plot(xtest, col =ifelse(ytest==1, 'light blue', 'salmon'),
pch=19, xlab='x1', ylab='x2', asp=1)
ypred <- predict(bestmod, testdat)
table(true.label=testdat$y, assigned.label=ypred)
species.name <- factor(iris$Species, labels=c('setosa','versicolor','virginica'))
set.seed(1)
iris2 <- iris[,1:2] + cbind(rnorm(150, sd=0.025))    # jittering
# setosa VS versicolor+virginica
y <- rep(0,150)
y[which(species.name=='setosa')] <- 1
plot(iris2[,1], iris2[,2], xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
col=as.character(y+1))
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel='linear', cost=100, scale=FALSE )
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor (y))
svmfit <- svm(y~., data=dat , kernel ='linear', cost =1, scale =FALSE )
summary(svmfit)
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
# Generate the data
set.seed (1)
x <- matrix (rnorm (200*2) , ncol =2)
x[1:100 ,] <- x[1:100 ,]+2
x[101:150 ,] <- x[101:150 ,] -2
y <- c(rep (1 ,150) ,rep (2 ,50) )
dat <- data.frame(x=x,y=as.factor (y))
# The classes are not separable
par(mfrow=c(1,1))
plot(x, col =ifelse(y==1, 'salmon', 'light blue'), pch=19, xlab='x1', ylab='x2', asp=1)
# Randomly split in train and test
train <- sample (200 ,100)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
(3+4)/100
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(3+10)/100
# Increasing the cost decreases the errors on the training set,
# at the expense of a more irregular boundary
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost=1e5)
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
0
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(5+20)/100
# Set parameters via CV:
set.seed (1)
tune.out <- tune(svm , y~., data=dat[train ,], kernel ='radial',
ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
gamma=c(0.5,1,2,3,4) ))
summary(tune.out)
# Misclassification error with best model on the training set
table(true=dat[train ,"y"], pred=predict (tune.out$best.model ,
newdata =dat[train ,]))
(2+4)/100
# Misclassification error with best model on the test set
table(true=dat[-train ,"y"], pred=predict (tune.out$best.model ,
newdata =dat[-train ,]))
(2+10)/100
# Application to Gene Expression Data (multiclass classification)
library(ISLR)
help(Khan)
is(Khan)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length (Khan$ytrain)
length (Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
dat <- data.frame(x=Khan$xtrain , y=as.factor(Khan$ytrain))
out <- svm(y~., data=dat , kernel ="linear",cost =10)
summary (out)
table(out$fitted , dat$y)
dat.te <- data.frame(x=Khan$xtest , y=as.factor (Khan$ytest ))
pred.te <- predict (out , newdata =dat.te)
table(pred.te , dat.te$y)
help(svm)
library(e1071)
help(svm)
## Linear case --------------------------------------------------------------------------------
# Generate the data
set.seed(123)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
# The classes are not separable
plot(x, col =ifelse(y==1, 'blue', 'red'),
pch=19, xlab='x1', ylab='x2', asp=1)
# Fit the Support Vector Classifier (kernel = "linear")
# given a cost C
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel ='linear', cost=10, scale=FALSE)
# in general case - recommended to scale data
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit, dat, col =c('salmon', 'light blue'), pch=19, asp=1)
par(mfrow=c(1,1))
# support vectors are indicated with crosses
# they are:
svmfit$index
# If we try to change the cost parameter we get more support points
# (higher bias, lower variance)
svmfit <- svm(y~., data=dat , kernel ='linear', cost=0.1, scale=FALSE )
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# To set the parameter C we can use the function tune(),
# which is based on cross-validation (10-fold)
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel = 'linear',
ranges = list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
# Extract the best model from the result of tune
# when we increase cost - we look fever (меньше) support vector,
# and make boundary depended only on closees vectors
# lower cost - bigger number of support vectors
bestmod <- tune.out$best.model
summary(bestmod)
plot(bestmod , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# Prediction for a new observation (command predict())
xtest <- matrix(rnorm (20*2) , ncol =2)
ytest <- sample(c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,] <- xtest[ytest ==1,] + 1
testdat <- data.frame(x=xtest , y=as.factor(ytest))
plot(xtest, col =ifelse(ytest==1, 'light blue', 'salmon'),
pch=19, xlab='x1', ylab='x2', asp=1)
ypred <- predict(bestmod, testdat)
table(true.label=testdat$y, assigned.label=ypred)
species.name <- factor(iris$Species, labels=c('setosa','versicolor','virginica'))
set.seed(1)
iris2 <- iris[,1:2] + cbind(rnorm(150, sd=0.025))    # jittering
# setosa VS versicolor+virginica
y <- rep(0,150)
y[which(species.name=='setosa')] <- 1
plot(iris2[,1], iris2[,2], xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
col=as.character(y+1))
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel='linear', cost=100, scale=FALSE )
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor (y))
svmfit <- svm(y~., data=dat , kernel ='linear', cost =1, scale =FALSE )
summary(svmfit)
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
# Generate the data
set.seed (1)
x <- matrix (rnorm (200*2) , ncol =2)
x[1:100 ,] <- x[1:100 ,]+2
x[101:150 ,] <- x[101:150 ,] -2
y <- c(rep (1 ,150) ,rep (2 ,50) )
dat <- data.frame(x=x,y=as.factor (y))
# The classes are not separable
par(mfrow=c(1,1))
plot(x, col =ifelse(y==1, 'salmon', 'light blue'), pch=19, xlab='x1', ylab='x2', asp=1)
# Randomly split in train and test
train <- sample (200 ,100)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
library(e1071)
help(svm)
## Linear case --------------------------------------------------------------------------------
# Generate the data
set.seed(123)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
# The classes are not separable
plot(x, col =ifelse(y==1, 'blue', 'red'),
pch=19, xlab='x1', ylab='x2', asp=1)
# Fit the Support Vector Classifier (kernel = "linear")
# given a cost C
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel ='linear', cost=10, scale=FALSE)
# in general case - recommended to scale data
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit, dat, col =c('salmon', 'light blue'), pch=19, asp=1)
par(mfrow=c(1,1))
# support vectors are indicated with crosses
# they are:
svmfit$index
# If we try to change the cost parameter we get more support points
# (higher bias, lower variance)
svmfit <- svm(y~., data=dat , kernel ='linear', cost=0.1, scale=FALSE )
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# Generate the data
set.seed (1)
x <- matrix (rnorm (200*2) , ncol =2)
x[1:100 ,] <- x[1:100 ,]+2
x[101:150 ,] <- x[101:150 ,] -2
y <- c(rep (1 ,150) ,rep (2 ,50) )
dat <- data.frame(x=x,y=as.factor (y))
# The classes are not separable
par(mfrow=c(1,1))
plot(x, col =ifelse(y==1, 'salmon', 'light blue'), pch=19, xlab='x1', ylab='x2', asp=1)
# Randomly split in train and test
train <- sample (200 ,100)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
(3+4)/100
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(3+10)/100
# Increasing the cost decreases the errors on the training set,
# at the expense of a more irregular boundary
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost=1e5)
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
svmfit$nSV
svmfit$nSV
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 1)
svmfit$nSV
svmfit$SV
nrow(svmfit$SV)
# Increasing the cost decreases the errors on the training set,
# at the expense of a more irregular boundary
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost=1e5)
svmfit$nSV
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
svmfit$nSV
nrow(svmfit$SV)
svmfit$nSV
nrow(svmfit$SV) / nrow(dat[train,]) # proportion of support vectors among all data points
## Comparison with k-Nearest Neighbor (k-NN) classifier ----------------------------------------
library(class)
help(knn)
cyto.knn <- knn(train = Infg, test = x, cl = group, k = 3, prob = T)
cyto.knn.class <- cyto.knn == 'B'
cyto.knn.B <- ifelse(cyto.knn.class==1,
attributes(cyto.knn)$prob,
1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x', ylab='estimated posterior')
points(x[,1], cyto.knn.B, type='l', col='black', lty=1)
abline(h = 0.5)
legend(-10, 0.75, legend=c('LDA','knn'), lty=c(2,1), col=c('red','black'))
## Univariate Binary (p=1, g=2) ---------------------------------------------------------------
cyto <- read.table('cytokines.txt', header=T)
cyto
attach(cyto)
# Group A: favorable clinical outcome (the treatment has effect)
A <- which(group == 'A')
# Group B: unfavorable clinical outcome (the treatment has no effect)
B <- which(group == 'B')
plot(cyto[,1], cyto[,2], pch=19, col=c(rep('blue',8), rep('red',5)), xlab='Inf-g', ylab='IL-5')
# verify assumptions 1) and 2):
# 1) normality (univariate) within the groups
shapiro.test(cyto[A,1])
shapiro.test(cyto[B,1])
# 2) equal variance (univariate)
# var.test is used instead of bartlett.test when there are only 2 groups
var.test(cyto[A,1], cyto[B,1])
# Recall
# the classification region is obtained by comparing pA*f.A and pB*f.B
nA <- length(A)
nB <- length(B)
n  <- nA + nB
# Prior probabilities (estimated from the data, no prior knowledge)
PA <- nA / n
PB <- nB / n
MA <- mean(Infg[A])
MB <- mean(Infg[B])
SA <- var(Infg[A])
SB <- var(Infg[B])
S  <- ((nA-1) * SA + (nB-1) * SB) / (nA + nB - 2)  # pooled estimate
x <- seq(-10, 35, 0.5) # include the range of Infg
par(mfrow = c(2, 1))
plot(
x,
PA * dnorm(x, MA, sqrt(S)),
type = 'l',
col = 'blue',
ylab = expression(paste('estimated ', p[i] * f[i], '(x)')),
main = 'LDA'
)
points(x, PB * dnorm(x, MB, sqrt(S)), type = 'l', col = 'red')
points(Infg[A], rep(0, length(A)), pch = 16, col = 'blue')
points(Infg[B], rep(0, length(B)), pch = 16, col = 'red')
legend(
-10,
0.03,
legend = c(expression(paste('P(A)', f[A], '(x)')),
expression(paste('P(B)', f[B], '(x)'))),
col = c('blue', 'red'),
lty = 1,
cex = 0.7
)
plot(
x,
PA * dnorm(x, MA, sqrt(S)) / (PA * dnorm(x, MA, sqrt(S)) + PB * dnorm(x, MB, sqrt(S))),
type = 'l',
col = 'blue',
ylab = 'estimated posterior'
)
points(
x,
PB * dnorm(x, MB, sqrt(S)) / (PA * dnorm(x, MA, sqrt(S)) + PB * dnorm(x, MB, sqrt(S))),
type = 'l',
col = 'red'
)
points(Infg[A], rep(0, length(A)), pch = 16, col = 'blue')
points(Infg[B], rep(0, length(B)), pch = 16, col = 'red')
legend(
-10,
0.9,
legend = c('P(A|X=x)', 'P(B|X=x)'),
col = c('blue', 'red'),
lty = 1,
cex = 0.7
)
par(mfrow = c(1, 1))
# R LDA function
library(MASS)
help(lda)
cyto.lda <- lda(group ~ Infg)
cyto.lda
# posterior probability and classification for x=0
x <- data.frame(Infg = 0)
# The command predict() returns a list containing (see the help of predict.lda):
# - the class associated with the highest posterior probability
predict(cyto.lda, x)$class
# - the posterior probabilities for the classes
predict(cyto.lda, x)$posterior
# - in lda: the coordinates of the canonical analysis of Fisher
#           (Fisher's discriminant scores)
predict(cyto.lda, x)$x
# posterior probability for a grid of x's
x <- data.frame(Infg = seq(-10, 35, 0.5))
head(predict(cyto.lda, x)$posterior)
cyto.LDA.A <-
predict(cyto.lda, x)$posterior[, 1] # posterior probability for class A
cyto.LDA.B <-
predict(cyto.lda, x)$posterior[, 2] # posterior probability for class B
plot.pred.lda <- function() {
plot(
Infg[A],
rep(0, length(A)),
pch = 16,
col = 'blue',
ylim = c(0, 1),
xlab = 'x',
ylab = 'estimated posterior',
main = "LDA",
xlim = range(Infg)
)
points(Infg[B], rep(0, length(B)), pch = 16, col = 'red')
abline(v = 0, col = 'grey')
points(
c(0, 0),
c(predict(cyto.lda, data.frame(Infg = 0))$posterior),
col = c('blue', 'red'),
pch = '*',
cex = 2.5
)
predict(cyto.lda, x)$class
lines(
x[, 1],
cyto.LDA.A,
type = 'l',
col = 'blue',
xlab = 'x',
ylab = 'estimated posterior',
main = "LDA"
)
lines(x[, 1], cyto.LDA.B, type = 'l', col = 'red')
abline(h = 0.5)
legend(
-10,
0.9,
legend = c('P(A|X=x)', 'P(B|X=x)'),
fill = c('blue', 'red'),
cex = 0.7
)
}
plot.pred.lda()
# set prior probabilities
# Pay attention to the order in which the levels appear in factor(group)!
cyto.lda.1 <- lda(group ~ Infg, prior=c(0.95, 0.05))
cyto.lda.1
x <- data.frame(Infg=seq(-10, 35, 0.5))
cyto.LDA.A.1 <- predict(cyto.lda.1, x)$posterior[,1] # posterior probability for class A
cyto.LDA.B.1 <- predict(cyto.lda.1, x)$posterior[,2] # posterior probability for class B
plot  (x[,1], cyto.LDA.A.1, type='l', col='blue', xlab='x', ylab='estimated posterior', main="LDA", ylim=c(0,1))
points(x[,1], cyto.LDA.B.1, type='l', col='red')
abline(h = 0.5)
legend(-10, 0.9, legend=c('P(A|X=x)', 'P(B|X=x)'), fill=c('blue','red'), cex = 0.7)
points(Infg[A], rep(0, length(A)), pch=16, col='blue')
points(Infg[B], rep(0, length(B)), pch=16, col='red')
points(x[,1], cyto.LDA.A, type='l', col='grey')
points(x[,1], cyto.LDA.B, type='l', col='grey')
## Comparison with k-Nearest Neighbor (k-NN) classifier ----------------------------------------
library(class)
help(knn)
cyto.knn <- knn(train = Infg, test = x, cl = group, k = 3, prob = T)
cyto.knn.class <- cyto.knn == 'B'
cyto.knn.B <- ifelse(cyto.knn.class==1,
attributes(cyto.knn)$prob,
1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x', ylab='estimated posterior')
points(x[,1], cyto.knn.B, type='l', col='black', lty=1)
abline(h = 0.5)
legend(-10, 0.75, legend=c('LDA','knn'), lty=c(2,1), col=c('red','black'))
# let's change k
par(mfrow=c(3, 4))
for(k in 1:12)
{
cyto.knn <- knn(train = Infg, test = x, cl = group, k = k, prob=T)
cyto.knn.class <- (cyto.knn == 'B')+0
cyto.knn.B <- ifelse(cyto.knn.class==1, attributes(cyto.knn)$prob, 1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x', ylab='estimated posterior', main=k)
points(x[,1], cyto.knn.B, type='l', col='black', lty=1, lwd=2)
abline(h = 0.5)
}
par(mfrow=c(1, 1))
