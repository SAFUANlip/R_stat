TP_lda <- sum(predictions == "Atlantica" & labels_all == "Atlantica")
FP_lda <- sum(predictions == "Atlantica" & labels_all == "Iberica")
FN_lda <- sum(predictions == "Iberica" & labels_all == "Atlantica")
TN_lda <- sum(predictions == "Iberica" & labels_all == "Iberica")
AER <-
lda_model
AER <-
lda_model
AER <-
lda_model
lda_model
AER <- 0.75 * FN_lda/500 + 0.25 * FP_lda/500
AER
AER
# check
AER == 0.25*(1 - pnorm(R, mean(sardinai), SD)) + 0.75*pnorm(R, mean(sardinaa), SD)
# check
AER == 0.25*(1 - pnorm(R, mean(sardinai), SD)) + 0.75*pnorm(R, mean(sardinaa), SD)
sardine <- read.table('sardine.txt', header=T)
head(sardine)
# question a)
sardinaa <- sardine[[1]]
sardinai <- sardine[[2]]
shapiro.test(sardinaa)
shapiro.test(sardinai)
var.test(sardinaa, sardinai)
t.test(sardinaa, sardinai, var.eq=T)
MA <- mean(sardinaa)
MI <- mean(sardinai)
SD <- sqrt((var(sardinaa) + var(sardinai))/2)
# Analitically
misclass <- function(x){
0.25*(1 - pnorm(x, MI, SD)) + 0.75*pnorm(x, MA, SD)
}
optimize(f=misclass, lower=min(sardine), upper=max(sardine))
R   <- optimize(f=misclass, lower=min(sardine), upper=max(sardine))$minimum
AER <- optimize(f=misclass, lower=min(sardine), upper=max(sardine))$objective
# check
AER == 0.25*(1 - pnorm(R, mean(sardinai), SD)) + 0.75*pnorm(R, mean(sardinaa), SD)
# some plots
specie <- factor(rep(c('Atlantica', 'Iberica'), each=500))
lunghezza <- c(sardine[[1]], sardine[[2]])
fit <- lda(specie ~ lunghezza, prior=c(0.75, 0.25))
x <- data.frame(lunghezza=seq(min(sardine), max(sardine), 0.05))
LDA <- predict(fit, x)$posterior[,2] # classe iberica
x11()
par(mfrow=c(2,1))
plot(x[,1], 0.25*(dnorm(x[,1], MI, SD)), type='l', col='red', lty=1, xlab='x', ylab='density * prior', ylim=c(0,0.6))
lines(x[,1], 0.75*(dnorm(x[,1], MA, SD)), type='l', col='blue', lty=1, xlab='x', ylab='density * prior')
abline(v=R, lty=2)
points(sardinaa, rep(0, 500), pch=3, col='blue')
points(sardinai, rep(0, 500), pch=3, col='red')
legend(7,.5,legend=c('Atlantic','Iberian'),fill=c('blue','red'),cex=.7)
plot(x[,1], LDA, type='l', col='red', lty=1, xlab='x', ylab='estimated posterior')# rosso = iberica
lines(x[,1], 1 - LDA, type='l', col='blue', lty=1, xlab='x', ylab='estimated posterior')# blu = atlantica
abline(h = 0.5)
abline(v=R, lty=2)
points(sardinaa, rep(0, 500), pch=3, col='blue')
points(sardinai, rep(0, 500), pch=3, col='red')
dev.off()
# Compute the APER
prior <- c(0.75,0.25)
G <- 2
misc <- table(classe.vera=specie, classe.allocata=predict(fit)$class)
misc
APER <- 0
for(g in 1:G)
APER <- APER + sum(misc[g,-g])/sum(sum(misc[g,])) * prior[g]
APER
AER
sardine <- read.table('sardine.txt', header=T)
head(sardine)
# a) normality test - (shapiro for p=1), ---------------------------------------
# mean equality - anova, t.test,
# homogenity - var.test, bartlett.test
help(t.test)
Atalantica_data <- sardine$Atlantica
Iberica_data <- sardine$Iberica
help(shapiro.test)
normality_test_Atalantica <- shapiro.test(Atalantica_data)
normality_test_Iberica <- shapiro.test(Iberica_data)
# Normality
normality_test_Atalantica$p.value # p-value > 0.05 can not reject H0 of normality
normality_test_Iberica$p.value # p-value > 0.05 can not reject H0 of normality
data_all <- c(Atalantica_data, Iberica_data)
labels_all <- vector(length = 1000)
for (i in 1:length(labels_all)){
if (i <= 500){
labels_all[i] <- "Atlantica"
}
else{
labels_all[i] <- "Iberica"
}
}
Atalantica_index <- which(labels_all == "Atlantica")
Iberica_index <- which(labels_all == "Iberica")
# equality of variances
bartlett.test(data_all ~ labels_all) # p-value > 0.05 => so we can not reject H0 of equal variances
var.test(data_all ~ labels_all) # p-value > 0.05 => so we can not reject H0 of equal variances
# equality of means
t.test(data_all ~ labels_all) # p-value < 0.05, so we can reject H0 of equal means
fit.aov <- aov(data_all ~ labels_all) # p-value < 0.05, so we can reject H0 of equal means
lda_model <- lda(as.matrix(data_all), labels_all, prior=c(75, 25)/100)
lda_model
# Projection of LD1 ------------------------------------------------------------
# 1. Get direction, where we have to project our data, to maximize
# variability between/variability within - ? as in FDA?
w <- lda_model$scaling[, 1] # solve(S) %*% (M1 - M0)
# 2. Проецируем все точки
lda_proj <- as.vector(as.matrix(data_all) %*% w)
# 3. Получаем проекции по группам
proj_atlantica <- lda_proj[Atalantica_index]
proj_iberica <- lda_proj[Iberica_index]
# 4. Оценим среднее и дисперсию на проекции
M_atlantica_proj <- mean(proj_atlantica)
M_iberica_proj <- mean(proj_iberica)
S_atlantica_proj <- cov(as.matrix(proj_atlantica))
S_iberica_proj <- cov(as.matrix(proj_iberica))
S_proj <- ((500-1) * S_atlantica_proj + (500-1) * S_iberica_proj) / (500 + 500 - 2)  # т.к. на проекции — одно измерение
# 5. Plot graphs
x <- seq(min(lda_proj), max(lda_proj), length.out = 500)
par(mfrow = c(2,1))
# Gaussian
plot(
x,
0.75 * dnorm(x, M_atlantica_proj, sqrt(S_proj)),
type = 'l',
col = 'blue',
ylab = expression(paste('estimated ', p[i] * f[i], '(x)')),
main = 'LDA (projection)'
)
lines(x, 0.25 * dnorm(x, M_iberica_proj, sqrt(S_proj)), col = 'red')
points(proj_atlantica, rep(0, length(proj_atlantica)), col = 'blue', pch = 16)
points(proj_iberica, rep(0, length(proj_iberica)), col = 'red', pch = 16)
legend("topright", legend=c("Atlantica", "Iberica"), col=c("blue", "red"), lty=1)
# posterior probability
posterior_atlantica <- 0.75 * dnorm(x, M_atlantica_proj, sqrt(S_proj)) / (
0.75 * dnorm(x, M_atlantica_proj, sqrt(S_proj)) + 0.25 * dnorm(x, M_iberica_proj, sqrt(S_proj))
)
posterior_iberica <- 1 - posterior_atlantica
par(mfrow = c(2,1))
# Gaussian
plot(
x,
0.75 * dnorm(x, M_atlantica_proj, sqrt(S_proj)),
type = 'l',
col = 'blue',
ylab = expression(paste('estimated ', p[i] * f[i], '(x)')),
main = 'LDA (projection)'
)
lines(x, 0.25 * dnorm(x, M_iberica_proj, sqrt(S_proj)), col = 'red')
points(proj_atlantica, rep(0, length(proj_atlantica)), col = 'blue', pch = 16)
points(proj_iberica, rep(0, length(proj_iberica)), col = 'red', pch = 16)
legend("topright", legend=c("Atlantica", "Iberica"), col=c("blue", "red"), lty=1)
# posterior probability
posterior_atlantica <- 0.75 * dnorm(x, M_atlantica_proj, sqrt(S_proj)) / (
0.75 * dnorm(x, M_atlantica_proj, sqrt(S_proj)) + 0.25 * dnorm(x, M_iberica_proj, sqrt(S_proj))
)
posterior_iberica <- 1 - posterior_atlantica
plot(x, posterior_atlantica, type = 'l', col = 'blue', ylab = 'estimated posterior')
lines(x, posterior_iberica, col = 'red')
legend("topright", legend=c("P(Class 0 | x)", "P(Class 1 | x)"), col=c("blue", "red"), lty=1)
par(mfrow = c(1,1))
# c)
predictions <- predict(lda_model, as.matrix(data_all))$class
predictions
# Confusion matrix components
TP_lda <- sum(predictions == "Atlantica" & labels_all == "Atlantica")
FP_lda <- sum(predictions == "Atlantica" & labels_all == "Iberica")
FN_lda <- sum(predictions == "Iberica" & labels_all == "Atlantica")
TN_lda <- sum(predictions == "Iberica" & labels_all == "Iberica")
AER <- 0.75 * FN_lda/500 + 0.25 * FP_lda/500
AER
# APER
Qda.m <- predict(qda.m)
true <- read.table('moneytrue.txt', header=TRUE)
false <- read.table('moneyfalse.txt', header=TRUE)
banknotes <- rbind(true,false)
vf <- factor(rep(c('true','false'),each=100), levels=c('true','false'))
plot(banknotes[,1:2], main='Banknotes', xlab='V1', ylab='V2', pch=20)
points(false, col='red', pch=20)
points(true, col='blue', pch=20)
legend('bottomleft', legend=levels(vf), fill=c('blue','red'), cex=.7)
# question a)
library(MVN)
mvn(true)$multivariateNormality
mvn(false)$multivariateNormality
# misclassification costs
c.tf <- 10
c.ft <- 0.05
#prior probabilities
pf <- 0.001
pt <- 1-0.001
prior = c(pt, pf)
prior
# Prior modified to account for the misclassification costs
prior.c <- c(pt*c.ft/(c.tf*pf+c.ft*pt), pf*c.tf/(c.tf*pf+c.ft*pt))
prior.c
# QDA
# Due to each group have their own covariance matrix, we can not project
# data on common vector and plot prob distribution
qda.m <- qda(banknotes, vf, prior=prior.c)
# R LDA function
library(MASS)
## Comparison with k-Nearest Neighbor (k-NN) classifier ----------------------------------------
library(class)
help(qda)
# QDA
# Due to each group have their own covariance matrix, we can not project
# data on common vector and plot prob distribution
qda.m <- qda(banknotes, vf, prior=prior.c)
qda.m
plot(banknotes[,1:2], main='Banknotes', xlab='V1', ylab='V2', pch=20)
points(false, col='red', pch=20)
points(true, col='blue', pch=20)
legend('bottomleft', legend=levels(vf), fill=c('blue','red'), cex=.7)
points(qda.m$means, pch=4,col=c('red','blue') , lwd=2, cex=1.5)
x  <- seq(min(banknotes[,1]), max(banknotes[,1]), length=200)
y  <- seq(min(banknotes[,2]), max(banknotes[,2]), length=200)
xy <- expand.grid(V1=x, V2=y)
z  <- predict(qda.m, xy)$post
z1 <- z[,1] - z[,2]
z2 <- z[,2] - z[,1]
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
m1 <- colMeans(true)
m2 <- colMeans(false)
S1 <- cov(true)
S2 <- cov(false)
open3d()
points3d(true[,1], true[,2], 0, col='blue', pch=15)
# APER
Qda.m <- predict(qda.m)
table(class.true=vf, class.assigned=Qda.m$class)
# APER
Qda.m <- predict(qda.m)
table(class.true=vf, class.assigned=Qda.m$class)
pt
pf
true <- read.table('moneytrue.txt', header=TRUE)
false <- read.table('moneyfalse.txt', header=TRUE)
true
length(true)
nrow(true)
nrow(false)
table(class.true=vf, class.assigned=Qda.m$class)
c.ft
# question a)
neve <- read.table('neve.txt', header=T)
good<-neve[neve[,3]=='good',1:2]
bad<-neve[neve[,3]=='bad',1:2]
mcshapiro.test(good)$pvalue
mcshapiro.test(bad)$pvalue
prior <- c(dim(bad)[1]/(sum(dim(bad)[1],dim(good)[1])),dim(good)[1]/(sum(dim(bad)[1],dim(good)[1])))
pb <- prior[1]
pg <- prior[2]
c.bg <- 2000
c.gb <- 3000
# Modified prior to account for the misclassification costs
prior.c <- c(bad=pb*c.gb/(c.bg*pg+c.gb*pb),good=pg*c.bg/(c.bg*pg+c.gb*pb))
prior.c
qda.m <- qda(giudizio ~ quantita + temperatura, data=neve, prior=prior.c)
library(MASS)
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
# question b) [APER]
Qda.m <- predict(qda.m)
table(classe.vera=neve[,3], classe.allocata=Qda.m$class)
APER  <- (2+3)/(46+14)
APER
# question c) [Expected economic loss]
(3*c.bg+2*c.gb)/60
# question d) [Classification of 2012-2013]
z0.new <- data.frame(quantita=200, temperatura=-4)
# Modified prior to account for the misclassification costs
prior.c <- c(bad=pb*c.gb/(c.bg*pg+c.gb*pb),good=pg*c.bg/(c.bg*pg+c.gb*pb))
prior.c
qda.m <- qda(giudizio ~ quantita + temperatura, data=neve, prior=prior.c)
qda.m
x11()
points(bad, col='red', pch=20)
points(good, col='blue', pch=20)
legend('bottomleft', legend=levels(neve[,3]), fill=c('red','blue'), cex=.7)
points(qda.m$means, pch=4,col=c('red','blue') , lwd=2, cex=1.5)
x  <- seq(min(neve[,1]), max(neve[,1]), length=200)
y  <- seq(min(neve[,2]), max(neve[,2]), length=200)
xy <- expand.grid(quantita=x, temperatura=y)
z  <- predict(qda.m, xy)$post
z1 <- z[,1] - z[,2]
z2 <- z[,2] - z[,1]
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
# question b) [APER]
Qda.m <- predict(qda.m)
table(classe.vera=neve[,3], classe.allocata=Qda.m$class)
APER  <- (2+3)/(46+14)
APER
# question c) [Expected economic loss]
(3*c.bg+2*c.gb)/60
## Linear case --------------------------------------------------------------------------------
# Generate the data
set.seed(123)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
# The classes are not separable
plot(x, col =ifelse(y==1, 'blue', 'red'),
pch=19, xlab='x1', ylab='x2', asp=1)
# Fit the Support Vector Classifier (kernel = "linear")
# given a cost C
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel ='linear', cost=10, scale=FALSE)
# in general case - recommended to scale data
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit, dat, col =c('salmon', 'light blue'), pch=19, asp=1)
par(mfrow=c(1,1))
# support vectors are indicated with crosses
# they are:
svmfit$index
# If we try to change the cost parameter we get more support points
# (higher bias, lower variance)
svmfit <- svm(y~., data=dat , kernel ='linear', cost=0.1, scale=FALSE )
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# To set the parameter C we can use the function tune(),
# which is based on cross-validation (10-fold)
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel = 'linear',
ranges = list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
# Extract the best model from the result of tune
# when we increase cost - we look fever support vector, and make boundary depended only on closees vectors
# lower cost - bigger number of support vectors
bestmod <- tune.out$best.model
summary(bestmod)
plot(bestmod , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# Prediction for a new observation (command predict())
xtest <- matrix(rnorm (20*2) , ncol =2)
ytest <- sample(c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,] <- xtest[ytest ==1,] + 1
testdat <- data.frame(x=xtest , y=as.factor(ytest))
plot(xtest, col =ifelse(ytest==1, 'light blue', 'salmon'),
pch=19, xlab='x1', ylab='x2', asp=1)
ypred <- predict(bestmod, testdat)
table(true.label=testdat$y, assigned.label=ypred)
species.name <- factor(iris$Species, labels=c('setosa','versicolor','virginica'))
set.seed(1)
iris2 <- iris[,1:2] + cbind(rnorm(150, sd=0.025))    # jittering
# setosa VS versicolor+virginica
y <- rep(0,150)
y[which(species.name=='setosa')] <- 1
plot(iris2[,1], iris2[,2], xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
col=as.character(y+1))
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel='linear', cost=100, scale=FALSE )
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor (y))
svmfit <- svm(y~., data=dat , kernel ='linear', cost =1, scale =FALSE )
summary(svmfit)
library(e1071)
help(svm)
## Linear case --------------------------------------------------------------------------------
# Generate the data
set.seed(123)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
# The classes are not separable
plot(x, col =ifelse(y==1, 'blue', 'red'),
pch=19, xlab='x1', ylab='x2', asp=1)
# Fit the Support Vector Classifier (kernel = "linear")
# given a cost C
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel ='linear', cost=10, scale=FALSE)
# in general case - recommended to scale data
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit, dat, col =c('salmon', 'light blue'), pch=19, asp=1)
par(mfrow=c(1,1))
# support vectors are indicated with crosses
# they are:
svmfit$index
# If we try to change the cost parameter we get more support points
# (higher bias, lower variance)
svmfit <- svm(y~., data=dat , kernel ='linear', cost=0.1, scale=FALSE )
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# To set the parameter C we can use the function tune(),
# which is based on cross-validation (10-fold)
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel = 'linear',
ranges = list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
# Extract the best model from the result of tune
# when we increase cost - we look fever support vector, and make boundary depended only on closees vectors
# lower cost - bigger number of support vectors
bestmod <- tune.out$best.model
summary(bestmod)
plot(bestmod , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
# Prediction for a new observation (command predict())
xtest <- matrix(rnorm (20*2) , ncol =2)
ytest <- sample(c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,] <- xtest[ytest ==1,] + 1
testdat <- data.frame(x=xtest , y=as.factor(ytest))
plot(xtest, col =ifelse(ytest==1, 'light blue', 'salmon'),
pch=19, xlab='x1', ylab='x2', asp=1)
ypred <- predict(bestmod, testdat)
table(true.label=testdat$y, assigned.label=ypred)
species.name <- factor(iris$Species, labels=c('setosa','versicolor','virginica'))
set.seed(1)
iris2 <- iris[,1:2] + cbind(rnorm(150, sd=0.025))    # jittering
# setosa VS versicolor+virginica
y <- rep(0,150)
y[which(species.name=='setosa')] <- 1
plot(iris2[,1], iris2[,2], xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
col=as.character(y+1))
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel='linear', cost=100, scale=FALSE )
summary(svmfit)
par(mfrow=c(1,2))
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
dat <- data.frame(x=iris2[,c(2,1)], y=as.factor (y))
svmfit <- svm(y~., data=dat , kernel ='linear', cost =1, scale =FALSE )
summary(svmfit)
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19)
# Generate the data
set.seed (1)
x <- matrix (rnorm (200*2) , ncol =2)
x[1:100 ,] <- x[1:100 ,]+2
x[101:150 ,] <- x[101:150 ,] -2
y <- c(rep (1 ,150) ,rep (2 ,50) )
dat <- data.frame(x=x,y=as.factor (y))
# The classes are not separable
par(mfrow=c(1,1))
plot(x, col =ifelse(y==1, 'salmon', 'light blue'), pch=19, xlab='x1', ylab='x2', asp=1)
# Randomly split in train and test
train <- sample (200 ,100)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost =1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
(3+4)/100
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(3+10)/100
# Increasing the cost decreases the errors on the training set,
# at the expense of a more irregular boundary
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost=1e5)
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
0
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(5+20)/100
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost =1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost =100000)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost =0)
summary(svmfit)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost =0.00001)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost =0.001)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Misclassification error on the training set
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 100)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 10000)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
# Fit a Support Vector Machine (kernel = "radial") given a cost C
svmfit <- svm(y~., data=dat [train ,], kernel ='radial', gamma =1, cost = 1)
summary(svmfit)
# Plot the SVM
plot(svmfit , dat[train,], col =c('salmon', 'light blue'), pch=19, asp=1)
