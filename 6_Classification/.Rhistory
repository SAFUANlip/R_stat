cyto.LDA.A.1 <- predict(cyto.lda.1, x)$posterior[,1] # posterior probability for class A
cyto.LDA.B.1 <- predict(cyto.lda.1, x)$posterior[,2] # posterior probability for class B
plot  (x[,1], cyto.LDA.A.1, type='l', col='blue', xlab='x', ylab='estimated posterior', main="LDA", ylim=c(0,1))
points(x[,1], cyto.LDA.B.1, type='l', col='red')
abline(h = 0.5)
legend(-10, 0.9, legend=c('P(A|X=x)', 'P(B|X=x)'), fill=c('blue','red'), cex = 0.7)
points(Infg[A], rep(0, length(A)), pch=16, col='blue')
points(Infg[B], rep(0, length(B)), pch=16, col='red')
points(x[,1], cyto.LDA.A, type='l', col='grey')
points(x[,1], cyto.LDA.B, type='l', col='grey')
## Comparison with k-Nearest Neighbor (k-NN) classifier ----------------------------------------
library(class)
help(knn)
cyto.knn <- knn(train = Infg, test = x, cl = group, k = 3, prob = T)
cyto.knn.class <- cyto.knn == 'B'
cyto.knn.B <- ifelse(cyto.knn.class==1,
attributes(cyto.knn)$prob,
1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x', ylab='estimated posterior')
points(x[,1], cyto.knn.B, type='l', col='black', lty=1)
abline(h = 0.5)
legend(-10, 0.75, legend=c('LDA','knn'), lty=c(2,1), col=c('red','black'))
# let's change k
par(mfrow=c(3, 4))
for(k in 1:12)
{
cyto.knn <- knn(train = Infg, test = x, cl = group, k = k, prob=T)
cyto.knn.class <- (cyto.knn == 'B')+0
cyto.knn.B <- ifelse(cyto.knn.class==1, attributes(cyto.knn)$prob, 1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x', ylab='estimated posterior', main=k)
points(x[,1], cyto.knn.B, type='l', col='black', lty=1, lwd=2)
abline(h = 0.5)
}
par(mfrow=c(1, 1))
detach(cyto)
help(iris)
attach(iris)
species.name <- factor(Species)
g = 3
i1 <- which(species.name == 'setosa')
i2 <- which(species.name == 'versicolor')
i3 <- which(species.name == 'virginica')
n1 <- length(i1)
n2 <- length(i2)
n3 <- length(i3)
n <- n1 + n2 + n3
detach(iris)
iris2 <- iris[,1:2]
# Jittering
set.seed(1)
iris2 <- iris2 + cbind(rnorm(150, sd=0.025))    # jittering
# Plot the data to verify assumptions
plot(iris2, main='Iris Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=19)
points(iris2[i1,], col='red', pch=19)
points(iris2[i2,], col='green', pch=19)
points(iris2[i3,], col='blue', pch=19)
legend("topright", legend=levels(species.name), fill=c('red','green','blue'))
m <-  colMeans(iris2)
m1 <- colMeans(iris2[i1, ])
m2 <- colMeans(iris2[i2, ])
m3 <- colMeans(iris2[i3, ])
S1 <- cov(iris2[i1, ])
S2 <- cov(iris2[i2, ])
S3 <- cov(iris2[i3, ])
Sp  <- ((n1 - 1) * S1 + (n2 - 1) * S2 + (n3 - 1) * S3) / (n - g)
# One-way MANOVA
fit <- manova(as.matrix(iris2) ~ species.name)
summary.manova(fit, test="Wilks")
# Linear Discriminant Analysis (LDA)
lda.iris <- lda(iris2, species.name)
lda.iris
Lda.iris <- predict(lda.iris, iris2)
names(Lda.iris)
# 1) Compute the APER
Lda.iris$class   # assigned classes
species.name     # true labels
table(class.true=species.name, class.assigned=Lda.iris$class)
errors <- (Lda.iris$class != species.name)
errors
sum(errors)
length(species.name)
APER   <- sum(errors)/length(species.name)
APER
(1+14+15)/150
# Recall: LOO CV
errors_CV <- 0
for(i in 1:150){
LdaCV.i <- lda(iris2[-i,], species.name[-i], prior=c(50, 50, 50) / 150)
errors_CV <- errors_CV + as.numeric(predict(LdaCV.i,iris2[i,])$class != species.name[i])
}
errors_CV
AERCV   <- sum(errors_CV)/length(species.name)
AERCV
# with R:
LdaCV.iris <- lda(iris2, species.name, CV=TRUE)  # specify the argument CV
LdaCV.iris$class
species.name
table(class.true=species.name, class.assignedCV=LdaCV.iris$class)
errorsCV <- (LdaCV.iris$class != species.name)
errorsCV
sum(errorsCV)
AERCV   <- sum(errorsCV)/length(species.name)
AERCV
x  <- seq(min(iris[,1]), max(iris[,1]), length=200)
y  <- seq(min(iris[,2]), max(iris[,2]), length=200)
xy <- expand.grid(Sepal.Length=x, Sepal.Width=y)
# Plot the partition induced by LDA
plot.partition.lda <- function() {
plot(iris2, main='Iris Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=20)
points(iris2[i1,], col='red', pch=20)
points(iris2[i2,], col='green', pch=20)
points(iris2[i3,], col='blue', pch=20)
legend("topright", legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
points(lda.iris$means, pch=4,col=c('red','green','blue') , lwd=2, cex=1.5)
z  <- predict(lda.iris, xy)$post  # these are P_i*f_i(x,y)
z1 <- z[,1] - pmax(z[,2], z[,3])  # P_1*f_1(x,y)-max{P_j*f_j(x,y)}
z2 <- z[,2] - pmax(z[,1], z[,3])  # P_2*f_2(x,y)-max{P_j*f_j(x,y)}
z3 <- z[,3] - pmax(z[,1], z[,2])  # P_3*f_3(x,y)-max{P_j*f_j(x,y)}
# Plot the contour line of level (levels=0) of z1, z2, z3:
# P_i*f_i(x,y)-max{P_j*f_j(x,y)}=0 i.e., boundary between R.i and R.j
# where j realizes the max.
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z3, 200), levels=0, drawlabels=F, add=T)
}
plot.partition.lda()
library(rgl)
library(mvtnorm)
open3d()
points3d(iris2[i1,1], iris2[i1,2], 0, col='red', pch=15)
points3d(iris2[i2,1], iris2[i3,2], 0, col='green', pch=15)
points3d(iris2[i3,1], iris2[i2,2], 0, col='blue', pch=15)
surface3d(x, y, dmvnorm(xy, m1, Sp) / 3, alpha=0.4, color='red')
surface3d(x, y, dmvnorm(xy, m2, Sp) / 3, alpha=0.4, color='green', add=T)
surface3d(x, y, dmvnorm(xy, m3, Sp) / 3, alpha=0.4, color='blue', add=T)
box3d()
help(qda)
qda.iris <- qda(iris2, species.name)
qda.iris
Qda.iris <- predict(qda.iris, iris2)
# compute the APER
Qda.iris$class
species.name
table(class.true=species.name, class.assigned=Qda.iris$class)
errorsq <- (Qda.iris$class != species.name)
errorsq
APERq   <- sum(errorsq)/length(species.name)
APERq
(15+13+1)/150
# Compute the estimate of the AER by leave-one-out cross-validation
QdaCV.iris <- qda(iris2, species.name, CV=T)
QdaCV.iris$class
species.name
table(class.true=species.name, class.assignedCV=QdaCV.iris$class)
errorsqCV <- (QdaCV.iris$class != species.name)
errorsqCV
AERqCV   <- sum(errorsqCV)/length(species.name)
AERqCV
# Plot the partition induced by QDA
plot.partition.qda <- function() {
plot(iris2, main='Iris Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=20)
points(iris2[i1,], col='red', pch=20)
points(iris2[i2,], col='green', pch=20)
points(iris2[i3,], col='blue', pch=20)
legend("topright", legend=levels(species.name), fill=c('red','green','blue'))
points(qda.iris$means, col=c('red','green','blue'), pch=4, lwd=2, cex=1.5)
x  <- seq(min(iris[,1]), max(iris[,1]), length=200)
y  <- seq(min(iris[,2]), max(iris[,2]), length=200)
xy <- expand.grid(Sepal.Length=x, Sepal.Width=y)
z  <- predict(qda.iris, xy)$post
z1 <- z[,1] - pmax(z[,2], z[,3])
z2 <- z[,2] - pmax(z[,1], z[,3])
z3 <- z[,3] - pmax(z[,1], z[,2])
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z3, 200), levels=0, drawlabels=F, add=T)
}
plot.partition.qda()
open3d()
points3d(iris2[i1,1], iris2[i1,2], 0, col='red', pch=15)
points3d(iris2[i2,1], iris2[i3,2], 0, col='green', pch=15)
points3d(iris2[i3,1], iris2[i2,2], 0, col='blue', pch=15)
surface3d(x, y, dmvnorm(xy, m1, S1) / 3, alpha=0.4, color='red')
surface3d(x, y, dmvnorm(xy, m2, S2) / 3, alpha=0.4, color='green', add=T)
surface3d(x, y, dmvnorm(xy, m3, S3) / 3, alpha=0.4, color='blue', add=T)
box3d()
k <- 12
plot(iris2, main='Iris.Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=20)
points(iris2[i1,], col=2, pch=20)
points(iris2[i3,], col=4, pch=20)
points(iris2[i2,], col=3, pch=20)
legend("topright", legend=levels(species.name), fill=c(2,3,4))
x  <- seq(min(iris[,1]), max(iris[,1]), length=200)
y  <- seq(min(iris[,2]), max(iris[,2]), length=200)
xy <- expand.grid(Sepal.Length=x, Sepal.Width=y)
iris.knn <- knn(train = iris2, test = xy, cl = iris$Species, k = k)
z  <- as.numeric(iris.knn)
contour(x, y, matrix(z, 200), levels=c(1.5, 2.5), drawlabels=F, add=T)
true <- read.table('moneytrue.txt', header=TRUE)
false <- read.table('moneyfalse.txt', header=TRUE)
banknotes <- rbind(true,false)
vf <- factor(rep(c('true','false'),each=100), levels=c('true','false'))
plot(banknotes[,1:2], main='Banknotes', xlab='V1', ylab='V2', pch=20)
points(false, col='red', pch=20)
points(true, col='blue', pch=20)
legend('bottomleft', legend=levels(vf), fill=c('blue','red'), cex=.7)
# question a)
library(MVN)
mvn(true)$multivariateNormality
mvn(false)$multivariateNormality
# misclassification costs
c.tf <- 10
c.ft <- 0.05
#prior probabilities
pf <- 0.001
pt <- 1-0.001
prior = c(pt, pf)
prior
# Prior modified to account for the misclassification costs
prior.c <- c(pt*c.ft/(c.tf*pf+c.ft*pt), pf*c.tf/(c.tf*pf+c.ft*pt))
prior.c
# QDA
# Due to each group have their own covariance matrix, we can not project
# data on common vector and plot prob distribution
qda.m <- qda(banknotes, vf, prior=prior.c)
qda.m
plot(banknotes[,1:2], main='Banknotes', xlab='V1', ylab='V2', pch=20)
points(false, col='red', pch=20)
points(true, col='blue', pch=20)
legend('bottomleft', legend=levels(vf), fill=c('blue','red'), cex=.7)
points(qda.m$means, pch=4,col=c('red','blue') , lwd=2, cex=1.5)
x  <- seq(min(banknotes[,1]), max(banknotes[,1]), length=200)
y  <- seq(min(banknotes[,2]), max(banknotes[,2]), length=200)
xy <- expand.grid(V1=x, V2=y)
z  <- predict(qda.m, xy)$post
z1 <- z[,1] - z[,2]
z2 <- z[,2] - z[,1]
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
m1 <- colMeans(true)
m2 <- colMeans(false)
S1 <- cov(true)
S2 <- cov(false)
open3d()
points3d(true[,1], true[,2], 0, col='blue', pch=15)
points3d(false[,1], false[,2], 0, col='red', pch=15)
surface3d(x,y,matrix(dmvnorm(xy, m1, S1)*prior.c[1]/100 , 200), alpha=0.6, color='blue')
surface3d(x,y,matrix(dmvnorm(xy, m2, S2)*prior.c[2]/100 , 200), alpha=0.6, color='red', add=T)
box3d()
# APER
Qda.m <- predict(qda.m)
table(class.true=vf, class.assigned=Qda.m$class)
APER  <- (2*pt+80*pf)/100 # prior NON adjusted! p(true)p(miscl.|true) + p(false)p(miscl.|false)
APER
# Expected economic loss: # |E(EL) = c.vf * p(EL = c.vf) + c.fv * p(EL = c.fv) + 0 * p(EL = 0)
2 / 100 * pt * c.ft + 80 / 100 * pf * c.tf
# question c)
# P[rejected] = P[rejected | true]P[true] + P[rejected | false]P[false]
2/100*pt + 20/100*pf
# Let's consider again the iris dataset
attach(iris)
species.name <- factor(Species, labels=c('setosa','versicolor','virginica'))
g <- 3
i1 <- which(species.name=='setosa')
i2 <- which(species.name=='versicolor')
i3 <- which(species.name=='virginica')
n1 <- length(i1)
n2 <- length(i2)
n3 <- length(i3)
n <- n1+n2+n3
detach(iris)
iris2 <- iris[,1:2]
head(iris2)
set.seed(1)
iris2 <- iris2 + cbind(rnorm(150, sd=0.025))    # jittering
m <-  colMeans(iris2)
m1 <- colMeans(iris2[i1,])
m2 <- colMeans(iris2[i2,])
m3 <- colMeans(iris2[i3,])
S1 <- cov(iris2[i1,])
S2 <- cov(iris2[i2,])
S3 <- cov(iris2[i3,])
Sp  <- ((n1-1)*S1+(n2-1)*S2+(n3-1)*S3)/(n-g)
# covariance between groups (estimate) # should be here 1/(g-1)?
B <- 1/(g-1)*(cbind(m1 - m) %*% rbind(m1 - m) +
cbind(m2 - m) %*% rbind(m2 - m) +
cbind(m3 - m) %*% rbind(m3 - m))
B
# covariance within groups (estimate)
Sp
# how many coordinates?
g <- 3
p <- 2
s <- min(g-1,p)
s
# system on which we project the data (in PCA it's orhogonal basis, but here no)
# Matrix Sp^(-1/2)
val.Sp <- eigen(Sp)$val
vec.Sp <- eigen(Sp)$vec
invSp.2 <- 1/sqrt(val.Sp[1])*vec.Sp[,1]%*%t(vec.Sp[,1]) + 1/sqrt(val.Sp[2])*vec.Sp[,2]%*%t(vec.Sp[,2])
invSp.2
# ration variability between groups compared to variability within groups
# spectral decomposition of Sp^(-1/2) B Sp^(-1/2)
spec.dec <- eigen(invSp.2 %*% B %*% invSp.2)
# first canonical coordinate
a1 <- invSp.2 %*% spec.dec$vec[,1] # not exact eigen vector, but multiplyed by invSp
a1
# second canonical coordinate
a2 <- invSp.2 %*% spec.dec$vec[,2]
a2
# compare with the output of lda():
lda.iris <- lda(iris2, species.name)
lda.iris
a1
a2 # here we found it by hand, but lda give same vectors
# (when equal miss classification cost and prior probability = lda, when something different != lda)
spec.dec$val/sum(spec.dec$val) # ratio of eigen values, first component of LDA holds 0.9644 of sepearability between groups
### How are the data classified?
# Compute the canonical coordinates of the data
# we want project data to canonical coordinates
cc1.iris <- as.matrix(iris2)%*%a1
cc2.iris <- as.matrix(iris2)%*%a2
coord.cc <- cbind(cc1.iris,cc2.iris)
# Compute the coordinates of the mean within groups along the canonical directions
# here we use our manually extracted vectors
cc.m1 <- c(m1%*%a1, m1%*%a2)
cc.m2 <- c(m2%*%a1, m2%*%a2)
cc.m3 <- c(m3%*%a1, m3%*%a2)
# Assign data to groups
f.class=rep(0, n)
for(i in 1:n) # for each datum
{
# Compute the Euclidean distance of the i-th datum from mean within the groups
dist.m=c(d1=sqrt(sum((coord.cc[i,]-cc.m1)^2)),
d2=sqrt(sum((coord.cc[i,]-cc.m2)^2)),
d3=sqrt(sum((coord.cc[i,]-cc.m3)^2)))
# Assign the datum to the group whose mean is the nearest
f.class[i]=which.min(dist.m)
}
f.class
table(class.true=species.name, class.assigned=f.class)
errors <- 150 - sum(diag(table(class.true=species.name, class.assigned=f.class)))
errors
length(species.name)
APERf   <- errors/length(species.name)
APERf
### How do I classify a new observation?
x.new <- c(5.85, 2.90)
# compute the canonical coordinates
cc.new <- c(x.new%*%a1, x.new%*%a2)
# compute the distance from the means
dist.m <- c(d1=sqrt(sum((cc.new-cc.m1)^2)),
d2=sqrt(sum((cc.new-cc.m2)^2)),
d3=sqrt(sum((cc.new-cc.m3)^2)))
# assign to the nearest mean
which.min(dist.m)
color.species=rep(c('red','green','blue'), each=50)
# look like we just flipped vertically and horizontally (rotated)
# visually
plot.fisher.score <- function() {
par(mfrow=c(1,2))
plot(iris2[,1], iris2[,2], main='Plane of original coordinates',
xlab='Sepal.Length', ylab='Sepal.Width', pch=20, col=as.character(color.species))
legend("topleft", legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
points(x.new[1], x.new[2], col='gold', pch=19)
points(m1[1], m1[2], pch=4,col='red' , lwd=2, cex=1.5)
points(m2[1], m2[2], pch=4,col='green' , lwd=2, cex=1.5)
points(m3[1], m3[2], pch=4,col='blue' , lwd=2, cex=1.5)
plot(cc1.iris, cc2.iris, main='Plane of canonical coordinates', xlab='first canonical coordinate', ylab='second canonical coordinate', pch=20, col=as.character(color.species))
legend("topleft", legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
points(cc.m1[1], cc.m1[2], pch=4,col='red' , lwd=2, cex=1.5)
points(cc.m2[1], cc.m2[2], pch=4,col='green' , lwd=2, cex=1.5)
points(cc.m3[1], cc.m3[2], pch=4,col='blue' , lwd=2, cex=1.5)
points(cc.new[1], cc.new[2], col='gold', pch=19)
segments(cc.m1[1], cc.m1[2], cc.new[1], cc.new[2])
segments(cc.m2[1], cc.m2[2], cc.new[1], cc.new[2])
segments(cc.m3[1], cc.m3[2], cc.new[1], cc.new[2])
par(mfrow=c(1,1))
}
plot.fisher.score()
# equal to vorronity...
# crosses - just means of our data (for eqch class)
# We plot the partition generated by the canonical coordinates
plot.partition.fisher <- function() {
color.species <- species.name
levels(color.species) <- c('red','green','blue')
plot(cc1.iris, cc2.iris, main='Fisher discriminant analysis', xlab='first canonical coordinate', ylab='second canonical coordinate', pch=20, col=as.character(color.species))
legend("topleft", legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
points(cc.m1[1], cc.m1[2], pch=4,col='red' , lwd=2, cex=1.5)
points(cc.m2[1], cc.m2[2], pch=4,col='green' , lwd=2, cex=1.5)
points(cc.m3[1], cc.m3[2], pch=4,col='blue' , lwd=2, cex=1.5)
x.cc  <- seq(min(cc1.iris),max(cc1.iris),len=200)
y.cc  <- seq(min(cc2.iris),max(cc2.iris),len=200)
xy.cc <- expand.grid(cc1=x.cc, cc2=y.cc)
z  <- cbind( sqrt(rowSums(scale(xy.cc,cc.m1,scale=FALSE)^2)), sqrt(rowSums(scale(xy.cc,cc.m2,scale=FALSE)^2)), sqrt(rowSums(scale(xy.cc,cc.m3,scale=FALSE)^2)))
z1.cc <- z[,1] - pmin(z[,2], z[,3])
z2.cc <- z[,2] - pmin(z[,1], z[,3])
z3.cc <- z[,3] - pmin(z[,1], z[,2])
contour(x.cc, y.cc, matrix(z1.cc, 200), levels=0, drawlabels=F, add=T)
contour(x.cc, y.cc, matrix(z2.cc, 200), levels=0, drawlabels=F, add=T)
contour(x.cc, y.cc, matrix(z3.cc, 200), levels=0, drawlabels=F, add=T)
}
plot.partition.fisher()
# In practical it's not popular, But here we didn't assume normality of data
# But we assume - equal miss classification cost and prior probability
plot.partition.lda()
plot.fisher.directions <- function() {
plot(iris2, main='Projection on the canonical directions', xlab='Sepal.Length', ylab='Sepal.Width', pch=20, xlim=c(-3,8), ylim=c(-3,7))
points(iris2[i1,], col='red', pch=20)
points(iris2[i2,], col='green', pch=20)
points(iris2[i3,], col='blue', pch=20)
legend('topleft', legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
points(rbind(m1,m2,m3), pch=4,col=c('red','green','blue') , lwd=2, cex=1.5)
abline(h=0,v=0, col='grey35')
arrows(x0=0, y0=0, x1=a1[1], y1=a1[2], length=.1)
arrows(x0=0, y0=0, x1=a2[1], y1=a2[2], length=.1)
text(a1[1], a1[2], 'a1',pos=3)
text(a2[1], a2[2], 'a2',pos=2)
abline(coef=c(0,(a1[2]/a1[1])), col='grey55',lty=2)
abline(coef=c(0,(a2[2]/a2[1])), col='grey55',lty=2)
points(cc1.iris*a1[1]/(sum(a1^2)),cc1.iris*a1[2]/(sum(a1^2)),col=as.character(color.species))
points(cc2.iris*a2[1]/(sum(a2^2)),cc2.iris*a2[2]/(sum(a2^2)),col=as.character(color.species))
#plot(cc1.iris, cc2.iris, main='Coordinate system of the canonical coordinates', xlab='first canonical coordinate', ylab='second canonical coordinate', pch=20, col=as.character(color.species))
#legend('topleft', legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
#
#cc.m1 <- c(m1%*%a1, m1%*%a2)
#cc.m2 <- c(m2%*%a1, m2%*%a2)
#cc.m3 <- c(m3%*%a1, m3%*%a2)
#
#points(cc.m1[1], cc.m1[2], pch=4,col='red' , lwd=2, cex=1.5)
#points(cc.m2[1], cc.m2[2], pch=4,col='green' , lwd=2, cex=1.5)
#points(cc.m3[1], cc.m3[2], pch=4,col='blue' , lwd=2, cex=1.5)
}
# and our a1, a2 vectors not orthogonal
plot.fisher.directions()
library(e1071)
help(svm)
# and our a1, a2 vectors not orthogonal
plot.fisher.directions()
# In practical it's not popular, But here we didn't assume normality of data
# But we assume - equal miss classification cost and prior probability
plot.partition.lda()
plot.partition.fisher()
# verify assumptions 1) and 2):
# 1) normality (univariate) within the groups
shapiro.test(cyto[A,1])
shapiro.test(cyto[B,1])
# 2) equal variance (univariate)
# var.test is used instead of bartlett.test when there are only 2 groups
var.test(cyto[A,1], cyto[B,1])
# Recall
# the classification region is obtained by comparing pA*f.A and pB*f.B
nA <- length(A)
nB <- length(B)
n  <- nA + nB
# Prior probabilities (estimated from the data, no prior knowledge)
PA <- nA / n
PB <- nB / n
MA <- mean(Infg[A])
MB <- mean(Infg[B])
SA <- var(Infg[A])
SB <- var(Infg[B])
S  <- ((nA-1) * SA + (nB-1) * SB) / (nA + nB - 2)  # pooled estimate
## Univariate Binary (p=1, g=2) ---------------------------------------------------------------
cyto <- read.table('cytokines.txt', header=T)
cyto
attach(cyto)
# Group A: favorable clinical outcome (the treatment has effect)
A <- which(group == 'A')
# Group B: unfavorable clinical outcome (the treatment has no effect)
B <- which(group == 'B')
plot(cyto[,1], cyto[,2], pch=19, col=c(rep('blue',8), rep('red',5)), xlab='Inf-g', ylab='IL-5')
# verify assumptions 1) and 2):
# 1) normality (univariate) within the groups
shapiro.test(cyto[A,1])
shapiro.test(cyto[B,1])
# 2) equal variance (univariate)
# var.test is used instead of bartlett.test when there are only 2 groups
var.test(cyto[A,1], cyto[B,1])
# Recall
# the classification region is obtained by comparing pA*f.A and pB*f.B
nA <- length(A)
nB <- length(B)
n  <- nA + nB
# Prior probabilities (estimated from the data, no prior knowledge)
PA <- nA / n
PB <- nB / n
MA <- mean(Infg[A])
MB <- mean(Infg[B])
SA <- var(Infg[A])
SB <- var(Infg[B])
S  <- ((nA-1) * SA + (nB-1) * SB) / (nA + nB - 2)  # pooled estimate
x <- seq(-10, 35, 0.5) # include the range of Infg
par(mfrow = c(2, 1))
plot(
x,
PA * dnorm(x, MA, sqrt(S)),
type = 'l',
col = 'blue',
ylab = expression(paste('estimated ', p[i] * f[i], '(x)')),
main = 'LDA'
)
points(x, PB * dnorm(x, MB, sqrt(S)), type = 'l', col = 'red')
points(Infg[A], rep(0, length(A)), pch = 16, col = 'blue')
# compare with the output of lda():
lda.iris <- lda(iris2, species.name)
lda.iris
a1
a2 # here we found it by hand, but lda give same vectors
# (when equal miss classification cost and prior probability = lda, when something different != lda)
spec.dec$val/sum(spec.dec$val) # ratio of eigen values, first component of LDA holds 0.9644 of sepearability between groups
spec.dec$val
