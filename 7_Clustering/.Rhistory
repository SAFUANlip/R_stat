for(j in 1:n)
cluster[j] <- which.min(Dist[j,])
plot(x, col = colplot[cluster],pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
}
## Example: Earthquakes dataset --------------------------------------------------------------
# in automatic, command kmeans()
help(kmeans)
result.k <- kmeans(Q, centers=2) # Centers: fixed number of clusters
names(result.k)
result.k$cluster      # labels of clusters
result.k$centers      # centers of the clusters
result.k$totss        # tot. sum of squares
result.k$withinss     # sum of squares within clusters
result.k$tot.withinss # sum(sum of squares within cluster)
result.k$betweenss    # sum of squares between clusters
result.k$size         # dimension of the clusters
plot(Q, col = result.k$cluster+1)
open3d()
plot3d(Q, size=3, col=result.k$cluster+1, aspect = F)
points3d(result.k$centers,size=10)
# method 1)
b <- NULL
w <- NULL
for(k in 1:10){
result.k <- kmeans(Q, k)
w <- c(w, sum(result.k$wit))
b <- c(b, result.k$bet)
}
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
par(mfrow=c(1,1))
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
# this method seems to suggest k = 2 or 4
# let's try also k=4:
result.k <- kmeans(Q, 4)
plot(Q, col = result.k$cluster+1)
open3d()
plot3d(Q, size=3, col=result.k$cluster+1, aspect = F)
# Simulate the data
set.seed(2)
n <- 400
x <- cbind(x = runif(4) + rnorm(n, sd = 0.1), y = runif(4) + rnorm(n, sd = 0.1))
true_clusters <- rep(1:4, time = 100)
plot(x, col = true_clusters, pch = 19)
plot(x, pch = 19)
# How to choose eps from minPts?
# Plot of the distances to the minPts-1 nearest neighbor
kNNdistplot(x, minPts = minPts)
library(dbscan)
# Choice of hyperparameters for DBSCAN
# Rule of thumb, minPts should be at least p + 1 = 3 here
# Can be eventually increased
minPts <- 3
# How to choose eps from minPts?
# Plot of the distances to the minPts-1 nearest neighbor
kNNdistplot(x, minPts = minPts)
# Taking eps = 0.05 seems to be a good threshold
abline(h = 0.05, col = "red", lty = 2)
# Run the dbscan
dbs <- dbscan(x, eps = 0.05, minPts = 3)
dbs
# Plot of the resulting clustering
plot(x, col = dbs$cluster + 1L, pch=19)
# Silhouette score (from the package "cluster")
help(silhouette)
library(cluster)
# Silhouette score (from the package "cluster")
help(silhouette)
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)
sil_score <- function(labels, dist) {
# Compute the average of the silhouette widths
sil <- silhouette(labels, dist)
sil_widths <- sil[,"sil_width"]
mean(sil_widths)
}
sil_score(clustered_labels, dist(clustered_points))
# Grid Search
minPts_grid <- 1:20
eps_grid <- seq(0.01, 0.2, by = 0.01)
max_share_noise <- 0.2
dbscan_perf <- function(minPts, eps) {
# Compute the silhouette score resulting from dbscan clustering
dbs <- dbscan(x, eps, minPts) # Run dbscan
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
nb_clusters <- length(unique(clustered_labels))
if ((nb_clusters > 1 & nb_clusters < n) & (length(which(dbs$cluster == 0))/n < max_share_noise)) {
# Silhouette score is defined only if 2 <= nb_clusters <= n-1
sil_score(clustered_labels, dist(clustered_points))
}
else {
# otherwise we return 0 which would be the approx. value of the silhouette
# score if the clusters were completely overlapping
0
}
}
# We compute the silhouette score for all combinations of minPts and eps
perf_grid <- outer(minPts_grid, eps_grid, FUN = Vectorize(dbscan_perf))
dimnames(perf_grid) <- list(minPts_grid, eps_grid)
# Histogram of the Silhouette scores
hist(perf_grid, breaks = 20, xlab = "Silhouette score", xlim = c(-1, 1), main = NULL)
max_score <- max(perf_grid)
min_score <- min(perf_grid)
max_abs <- max(abs(max_score), abs(min_score))
image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))
library(fields)
image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))
# Retrieve best parameter values
max_score <- max(perf_grid)
argmax_score <- which(perf_grid == max_score, arr.ind = TRUE)
best_eps <- eps_grid[argmax_score[2]]
best_minPts <- minPts_grid[argmax_score[1]]
best_eps
best_minPts
max_score
# Run the dbscan
dbs <- dbscan(x, best_eps, best_minPts)
dbs
plot(x, col = dbs$cluster + 1L, pch=19)
# Let's try now with eps = 0.09 and minPts = 15
dbs <- dbscan(x, eps = 0.09, minPts = 15)
dbs
# Recovered the original clusters!
plot(x, col = dbs$cluster + 1L, pch=19)
# Example of dataset where DBSCAN succeed and k-means & hierarchical fail
data("moons")
plot(moons, pch=19)
minPts = 3 # Dimensionality + 1
kNNdistplot(moons, minPts = 3)
abline(h = 0.27, col = "red", lty = 2)
eps <- 0.3
minPts <- 3
dbs <- dbscan(moons, eps, minPts)
dbs
plot(moons, col = dbs$cluster + 1L, pch=19)
# Let's try the other algorithms that we saw:
hclust.s <- hclust(dist(moons), method='single')
hclust.a <- hclust(dist(moons), method='average')
hclust.c <- hclust(dist(moons), method='complete')
# plot of the dendrograms
par(mfrow=c(1,3))
plot(hclust.s, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.c, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.a, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
cluster.c <- cutree(hclust.c, k=2) # euclidean-complete:
cluster.s <- cutree(hclust.s, k=3) # euclidean-single
cluster.a <- cutree(hclust.a, k=3) # euclidean-average
par(mfrow=c(1,3))
plot(moons, col=cluster.c + 1L, pch=19, main='Complete')
plot(moons, col=cluster.a + 1L, pch=19, main='Average')
plot(moons, col=cluster.s + 1L, pch=19, main='Single')
# Choose k -> elbow method
b <- NULL
w <- NULL
for(k in 1:10){
k.means <- kmeans(moons, k)
w <- c(w, sum(k.means$wit))
b <- c(b, k.means$bet)
}
par(mfrow=c(1,1))
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
k.means <- kmeans(moons, centers = 4, nstart = 25)
plot(moons, col=k.means$cluster + 1L, pch=19, main='k-means')
# Example : European cities
help(eurodist)
eurodist
# R function for multidimensional scaling: cmdscale
help(cmdscale)
location <- cmdscale(eurodist, k=2)
location
# I have to set asp=1 (equal scales on the two axes)
# to correctly represent Euclidean distances
plot(location[,1], location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# change the sign to get the North in the upper part of the plot
plot(location[,1], -location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], -location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# compare the original matrix d_ij = d(x_i,x_j) and delta_ij = d(y_i,y_j)
plot(eurodist, dist(location))
# visualize the most different distances
par(cex = 0.75, mar = c(10,10,2,2))
image(1:21, 1:21, asp=1, abs(as.matrix(dist(location)) - as.matrix(eurodist)), axes = F, xlab = '', ylab ='')
axis(1, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 2, cex = 0.75)
axis(2, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 1, cex = 0.75)
box()
# Rome-Athens
as.matrix(eurodist)[19,1]
as.matrix(dist(location))[19,1]
# Cologne-Geneve
as.matrix(eurodist)[6,8]
as.matrix(dist(location))[6,8]
Stressk <- NULL
for(k in 1:4)
{
location.k <- cmdscale(eurodist, k)
Stress <- (sum( (as.vector(eurodist) - as.vector(dist(location.k)))^2)  /
sum( as.vector(location.k)^2))^(1/2)
Stressk <- c(Stressk, Stress)
}
plot(1:4,Stressk,xlab='k',ylab='Stress',lwd=2)
set.seed(2)
n <- 400
x <- cbind(x = runif(4) + rnorm(n, sd = 0.1), y = runif(4) + rnorm(n, sd = 0.1))
true_clusters <- rep(1:4, time = 100)
plot(x, col = true_clusters, pch = 19)
plot(x, pch = 19)
# Choice of hyperparameters for DBSCAN
# Rule of thumb, minPts should be at least p + 1 = 3 here
# Can be eventually increased
minPts <- 3
library(dbscan)
# How to choose eps from minPts?
# Plot of the distances to the minPts-1 nearest neighbor
kNNdistplot(x, minPts = minPts)
# Taking eps = 0.05 seems to be a good threshold
abline(h = 0.05, col = "red", lty = 2)
# Run the dbscan
dbs <- dbscan(x, eps = 0.05, minPts = 3)
dbs
# Plot of the resulting clustering
plot(x, col = dbs$cluster + 1L, pch=19)
# Silhouette score (from the package "cluster")
help(silhouette)
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)
sil_score <- function(labels, dist) {
# Compute the average of the silhouette widths
sil <- silhouette(labels, dist)
sil_widths <- sil[,"sil_width"]
mean(sil_widths)
}
sil_score(clustered_labels, dist(clustered_points))
# Grid Search
minPts_grid <- 1:20
eps_grid <- seq(0.01, 0.2, by = 0.01)
max_share_noise <- 0.2
max_share_noise <- 0.2
dbscan_perf <- function(minPts, eps) {
# Compute the silhouette score resulting from dbscan clustering
dbs <- dbscan(x, eps, minPts) # Run dbscan
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
nb_clusters <- length(unique(clustered_labels))
if ((nb_clusters > 1 & nb_clusters < n) & (length(which(dbs$cluster == 0))/n < max_share_noise)) {
# Silhouette score is defined only if 2 <= nb_clusters <= n-1
sil_score(clustered_labels, dist(clustered_points))
}
else {
# otherwise we return 0 which would be the approx. value of the silhouette
# score if the clusters were completely overlapping
0
}
}
# We compute the silhouette score for all combinations of minPts and eps
perf_grid <- outer(minPts_grid, eps_grid, FUN = Vectorize(dbscan_perf))
dimnames(perf_grid) <- list(minPts_grid, eps_grid)
# Histogram of the Silhouette scores
hist(perf_grid, breaks = 20, xlab = "Silhouette score", xlim = c(-1, 1), main = NULL)
max_score <- max(perf_grid)
min_score <- min(perf_grid)
max_abs <- max(abs(max_score), abs(min_score))
max_abs
max_score
image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))
# Retrieve best parameter values
max_score <- max(perf_grid)
argmax_score <- which(perf_grid == max_score, arr.ind = TRUE)
best_eps <- eps_grid[argmax_score[2]]
best_minPts <- minPts_grid[argmax_score[1]]
best_eps
best_minPts
max_score
# Run the dbscan
dbs <- dbscan(x, best_eps, best_minPts)
dbs
plot(x, col = dbs$cluster + 1L, pch=19)
# Let's try now with eps = 0.09 and minPts = 15
dbs <- dbscan(x, eps = 0.09, minPts = 15)
dbs
# Recovered the original clusters!
plot(x, col = dbs$cluster + 1L, pch=19)
# Example of dataset where DBSCAN succeed and k-means & hierarchical fail
data("moons")
plot(moons, pch=19)
minPts = 3 # Dimensionality + 1
kNNdistplot(moons, minPts = 3)
abline(h = 0.27, col = "red", lty = 2)
eps <- 0.3
minPts <- 3
dbs <- dbscan(moons, eps, minPts)
dbs
plot(moons, col = dbs$cluster + 1L, pch=19)
# Let's try the other algorithms that we saw:
hclust.s <- hclust(dist(moons), method='single')
hclust.a <- hclust(dist(moons), method='average')
hclust.c <- hclust(dist(moons), method='complete')
# plot of the dendrograms
par(mfrow=c(1,3))
plot(hclust.s, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.c, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.a, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
cluster.c <- cutree(hclust.c, k=2) # euclidean-complete:
cluster.s <- cutree(hclust.s, k=3) # euclidean-single
cluster.a <- cutree(hclust.a, k=3) # euclidean-average
par(mfrow=c(1,3))
plot(moons, col=cluster.c + 1L, pch=19, main='Complete')
plot(moons, col=cluster.a + 1L, pch=19, main='Average')
plot(moons, col=cluster.s + 1L, pch=19, main='Single')
# Choose k -> elbow method
b <- NULL
w <- NULL
for(k in 1:10){
k.means <- kmeans(moons, k)
w <- c(w, sum(k.means$wit))
b <- c(b, k.means$bet)
}
par(mfrow=c(1,1))
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
k.means <- kmeans(moons, centers = 4, nstart = 25)
plot(moons, col=k.means$cluster + 1L, pch=19, main='k-means')
# Example : European cities
help(eurodist)
eurodist
# R function for multidimensional scaling: cmdscale
help(cmdscale)
location <- cmdscale(eurodist, k=2)
location
# I have to set asp=1 (equal scales on the two axes)
# to correctly represent Euclidean distances
plot(location[,1], location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# change the sign to get the North in the upper part of the plot
plot(location[,1], -location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], -location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# compare the original matrix d_ij = d(x_i,x_j) and delta_ij = d(y_i,y_j)
plot(eurodist, dist(location))
# visualize the most different distances
par(cex = 0.75, mar = c(10,10,2,2))
image(1:21, 1:21, asp=1, abs(as.matrix(dist(location)) - as.matrix(eurodist)), axes = F, xlab = '', ylab ='')
axis(1, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 2, cex = 0.75)
axis(2, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 1, cex = 0.75)
box()
# Rome-Athens
as.matrix(eurodist)[19,1]
as.matrix(dist(location))[19,1]
# Cologne-Geneve
as.matrix(eurodist)[6,8]
as.matrix(dist(location))[6,8]
Stressk <- NULL
for(k in 1:4)
{
location.k <- cmdscale(eurodist, k)
Stress <- (sum( (as.vector(eurodist) - as.vector(dist(location.k)))^2)  /
sum( as.vector(location.k)^2))^(1/2)
Stressk <- c(Stressk, Stress)
}
plot(1:4,Stressk,xlab='k',ylab='Stress',lwd=2)
options(rgl.printRglwidget = TRUE)
library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)
install.packages("fields")
species.name <- iris[,5]
iris4        <- iris[,1:4]
pairs(iris4, pch=19)
# compute the dissimilarity matrix of the data
# we choose the Euclidean metric (and then we look at other metrics)
help(dist)
iris.e <- dist(iris4, method='euclidean')
image(1:150,1:150,as.matrix(iris.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
# with other metrics:
iris.m <- dist(iris4, method='manhattan')
iris.c <- dist(iris4, method='canberra')
par(mfrow=c(1,3))
image(1:150,1:150,as.matrix(iris.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.c), main='metrics: Canberra', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.m), main='metrics: Manhattan', asp=1, xlab='i', ylab='j' )
par(mfrow=c(1,1))
# actually, the data are never ordered according to (unknown) labels
misc <- sample(150)
iris4 <- iris4[misc,]
iris.e <- dist(iris4, method='euclidean')
iris.m <- dist(iris4, method='manhattan')
iris.c <- dist(iris4, method='canberra')
par(mfrow=c(1,3))
image(1:150,1:150,as.matrix(iris.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.c), main='metrics: Canberra', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.m), main='metrics: Manhattan', asp=1, xlab='i', ylab='j' )
par(mfrow=c(1,1))
# Command hclust()
help(hclust)
iris.es <- hclust(iris.e, method='single')
iris.ea <- hclust(iris.e, method='average')
iris.ec <- hclust(iris.e, method='complete')
iris.es <- hclust(iris.e, method='single') # distance between clusters -
#smallest amogn all pairs
iris.ea <- hclust(iris.e, method='average') # average distance among all pairs
iris.ec <- hclust(iris.e, method='complete') # maximum distance between two points
# if we want more detailed information on euclidean-complete
# clustering:
names(iris.ec)
iris.ec$merge  # order of aggregation of statistical units / clusters
iris.ec$height # distance at which we have aggregations
iris.ec$order  # ordering that allows to avoid intersections in the dendrogram
# plot of the dendrograms
par(mfrow=c(1,3))
plot(iris.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(iris.ec, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(iris.ea, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
par(mfrow=c(1,1))
# plot dendrograms (2 clusters)
par(mfrow=c(1,3))
plot(iris.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.es, k=2)
plot(iris.ec, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.ec, k=2)
plot(iris.ea, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.ea, k=2)
par(mfrow=c(1,1))
# plot dendrograms (3 clusters)
par(mfrow=c(1,3))
plot(iris.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
#rect.hclust(iris.es, k=2)
rect.hclust(iris.es, k=3)
plot(iris.ec, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
#rect.hclust(iris.ec, k=2)
rect.hclust(iris.ec, k=3)
plot(iris.ea, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
#rect.hclust(iris.ea, k=2)
rect.hclust(iris.ea, k=3)
par(mfrow=c(1,1))
# How to cut a dendrogram?
# We generate vectors of labels through the command cutree()
help(cutree)
# Fix k=2 clusters:
cluster.ec <- cutree(iris.ec, k=2) # euclidean-complete:
cluster.ec
cluster.es <- cutree(iris.es, k=2) # euclidean-single
cluster.ea <- cutree(iris.ea, k=2) # euclidean-average
# interpret the clusters
table(label.true = species.name[misc], label.cluster = cluster.es)
table(label.true = species.name[misc], label.cluster = cluster.ec)
table(label.true = species.name[misc], label.cluster = cluster.ea)
plot(iris4, col=ifelse(cluster.es==1,'red','blue'), pch=19, main='Simple')
plot(iris4, col=ifelse(cluster.ec==1,'red','blue'), pch=19, main='Complete')
plot(iris4, col=ifelse(cluster.ea==1,'red','blue'), pch=19, main='Average')
# compute the cophenetic matrices
coph.es <- cophenetic(iris.es)
coph.ec <- cophenetic(iris.ec)
coph.ea <- cophenetic(iris.ea)
# compare with dissimilarity matrix (Euclidean distance)
layout(rbind(c(0,1,0),c(2,3,4)))
image(as.matrix(iris.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
image(as.matrix(coph.ec), main='Complete', asp=1 )
image(as.matrix(coph.ea), main='Average', asp=1 )
par(mfrow=c(1,1))
# compute cophenetic coefficients
es <- cor(iris.e, coph.es)
ec <- cor(iris.e, coph.ec)
ea <- cor(iris.e, coph.ea)
c("Eucl-Single"=es,"Eucl-Compl."=ec,"Eucl-Ave."=ea)
### Univariate case ---------------------------------------------------------------------------
set.seed(123)
# x : vector of NON clustered data
x <- 0:124/124 + rnorm(125, sd=0.01)
# y : vector of clustered data (5 clusters)
y <- c(rnorm(25, mean=0, sd=0.01), rnorm(25, mean=0.25, sd=0.01),
rnorm(25, mean=0.5, sd=0.01), rnorm(25, mean=0.75, sd=0.01),
rnorm(25, mean=1, sd=0.01))
x <- sample(x)
y <- sample(y)
par(mfrow=c(1,2))
plot(rep(0,125),x, main='data: no clust',xlab='')
plot(rep(0,125),y, main='data: clust',xlab='')
par(mfrow=c(1,1))
dx <- dist(x)
dy <- dist(y)
hcx<- hclust(dx, method='single')
hcy<- hclust(dy, method='single')
par(mfrow=c(1,2))
plot(hcx, labels=F, cex=0.5, hang=-0.1, xlab='', sub='x')
plot(hcy, labels=F, cex=0.5, hang=-0.1, xlab='', sub='y')
