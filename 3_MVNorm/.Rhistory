M <- colMeans(stiff)
S <- cov(stiff)
d2 <- matrix(mahalanobis(stiff, M, S))
stiff_wo_outliers <- stiff[which(d2 <= 8),]
stiff_wo_outliers <- stiff[which(d2 <= 8),]
result <- mvn(data = stiff_wo_outliers)
result$multivariateNormality
plot(stiff_wo_outliers, asp = 1, pch = 19)
# Normality of the components
par(mfcol = c(2, 4))
for (i in 1:4) {
hist(
stiff_wo_outliers[, i],
prob = T,
main = paste('Histogram of V', i, sep = ''),
xlab = paste('V', i, sep = '')
)
lines(900:2800,
dnorm(900:2800, mean(stiff_wo_outliers[, i]), sd(stiff_wo_outliers[, i])),
col = 'blue',
lty = 2)
qqnorm(stiff_wo_outliers[, i], main = paste('QQplot of V', i, sep = ''))
qqline(stiff_wo_outliers[, i])
print(shapiro.test(stiff_wo_outliers[, i])$p)
}
# Let's plot the transformations for some values of lambda
box_cox <- function(x, lambda) {
if (lambda != 0)
return((x ^ lambda - 1) / lambda)
return(log(x))
}
# Uniform sequence between 0 and 25
x <- seq(0, 25, by = 0.01)
# Plot of the box cox transformation applied to the uniform sequence
plot.box.cox.transformation <- function() {
par(mfrow = c(1, 1))
plot(
x,
box_cox(x, 0),
col = 'gold',
type = 'l',
xlab = 'x',
ylab = expression(x[lambda]),
ylim = c(-20, 30),
xlim = c(-5, 25),
asp = 1
)
title(main = 'Box-Cox transformations')
curve(
box_cox(x, -1),
from = 0,
to = 25,
col = 'blue',
add = TRUE
)
curve(
box_cox(x, 1),
from = 0,
to = 25,
col = 'red',
add = TRUE
)
curve(
box_cox(x, 2),
from = 0,
to = 25,
col = 'springgreen',
add = TRUE
)
points(1, 0, pch = 19, col = 'black')
abline(v = 0, lty = 2, col = 'grey')
legend(
'topright',
c(
expression(paste(lambda, '=-1')),
expression(paste(lambda, '=0')),
expression(paste(lambda, '=1')),
expression(paste(lambda, '=2'))
),
col = c('blue', 'gold', 'red', 'springgreen'),
lty = c(1, 1, 1, 1, 1)
)
xx=seq(0.01, 25.01, .05)
par(cex = .5)
points(xx, rep(0, length(xx)), col = 'grey', pch = 19)
points(-.5 + rep(0, length(xx)),
box_cox(xx,-1),
col = 'blue',
pch = 19)
points(-.5 + rep(-.5, length(xx)),
log(xx),
col = 'gold',
pch = 19)
points(-.5 + rep(-1, length(xx)),
box_cox(xx, 1),
col = 'red',
pch = 19)
points(-.5 + rep(-1.5, length(xx)),
box_cox(xx, 2),
col = 'springgreen',
pch = 19)
points(1, 0, pch = 19, col = 'black')
}
# Plot of the box cox transformation applied to the uniform sequence
plot.box.cox.transformation <- function() {
par(mfrow = c(1, 1))
plot(
x,
box_cox(x, 0),
col = 'gold',
type = 'l',
xlab = 'x',
ylab = expression(x[lambda]),
ylim = c(-20, 30),
xlim = c(-5, 25),
asp = 1
)
title(main = 'Box-Cox transformations')
curve(
box_cox(x, -1),
from = 0,
to = 25,
col = 'blue',
add = TRUE
)
curve(
box_cox(x, 1),
from = 0,
to = 25,
col = 'red',
add = TRUE
)
curve(
box_cox(x, 2),
from = 0,
to = 25,
col = 'springgreen',
add = TRUE
)
points(1, 0, pch = 19, col = 'black')
abline(v = 0, lty = 2, col = 'grey')
legend(
'topright',
c(
expression(paste(lambda, '=-1')),
expression(paste(lambda, '=0')),
expression(paste(lambda, '=1')),
expression(paste(lambda, '=2'))
),
col = c('blue', 'gold', 'red', 'springgreen'),
lty = c(1, 1, 1, 1, 1)
)
xx=seq(0.01, 25.01, .05)
par(cex = .5)
points(xx, rep(0, length(xx)), col = 'grey', pch = 19)
points(-.5 + rep(0, length(xx)),
box_cox(xx,-1),
col = 'blue',
pch = 19)
points(-.5 + rep(-.5, length(xx)),
log(xx),
col = 'gold',
pch = 19)
points(-.5 + rep(-1, length(xx)),
box_cox(xx, 1),
col = 'red',
pch = 19)
points(-.5 + rep(-1.5, length(xx)),
box_cox(xx, 2),
col = 'springgreen',
pch = 19)
points(1, 0, pch = 19, col = 'black')
}
plot.box.cox.transformation()
## Example 1: Simulated chi-squared -----------------------------------------------------------
n <- 100
# Sample from Chi-squared with 1 degree of freedom
set.seed(20032023)
x <- rnorm(n) ^ 2
hist(
x,
col = 'grey',
prob = T,
xlab = 'x',
main = 'Histogram of X'
)
points(x, rep(0, n), pch = 19)
points(x, rep(0, n), pch = 19)
# Univariate Box-Cox transformation
# We compute the optimal lambda of the univariate Box-Cox transformation
# (command powerTransform of library car)
lambda.x <- powerTransform(x) # automatically find best lambda
lambda.x
# Transformed sample with the optimal lambda (command bcPower of library car)
bc.x <- bcPower(x, lambda.x$lambda)      # it transforms the data of the first argument through the
# Box-Cox transformation with lambda given as second argument
hist(
bc.x,
col = 'grey',
prob = T,
main = 'Histogram of BC(X)',
xlab = 'BC(x)'
)
points(bc.x, rep(0, n), pch = 19)
shapiro.test(x)
shapiro.test(bc.x)
### Multivariate Box-Cox transformation (J-W, Ch. 4.8)
# Similar to univariate transformation, but jointly on all the variables
rm(x)
## Example 2: Unknown distribution -------------------------------------------------------------
b <- read.table('data_sim.txt')
head(b)
dim(b)
attach(b)
plot.hist.qq <- function() {
plot(b, pch=19,main='Data', xlim=c(1,7), ylim=c(-20,800))
points(x, rep(-20,dim(b)[1]), col='red', pch=19)
points(rep(1,dim(b)[1]), y, col='blue', pch=19)
par(mfrow=c(2,2))
hist(x, prob=T,col='grey85')
lines(0:1000/100, dnorm(0:1000/100, mean(x), sd(x)), col='blue', lty=2)
hist(y, prob=T,col='grey85')
lines(0:1000, dnorm(0:1000, mean(y), sd(y)), col='blue', lty=2)
qqnorm(x, main='QQplot of x')
qqline(x)
qqnorm(y, main='QQplot of y')
qqline(y)
par(mfrow=c(1,1))
}
plot.hist.qq()
shapiro.test(x)
shapiro.test(y)
mvn(b)$multivariateNormality
# Bivariate Box-Cox transformation
# Compute the optimal lambda of a bivariate Box-Cox transformation
# (command powerTransform, with a multivariate input)
lambda <- powerTransform(cbind(x, y))
lambda
# Compute the transformed data with optimal lambda (of the bivariate transf.)
# (command bcPower)
BC.x <- bcPower(x, lambda$lambda[1])
BC.y <- bcPower(y, lambda$lambda[2])
xx <- seq(0, 7, by=0.01)
# Compute the transformed data with optimal lambda (of the bivariate transf.)
# (command bcPower)
BC.x <- bcPower(x, lambda$lambda[1])
BC.y <- bcPower(y, lambda$lambda[2])
xx <- seq(0, 7, by=0.01)
plot.bivariate.bc <- function() {
par(mfrow=c(1,3))
plot(xx, box_cox(x=xx, lambda=lambda$lambda[1]), col='red', lty=1, type='l',
xlab=expression(x), ylab=expression(x[lambda]), ylim=c(-5,10), asp=1)
title(main='Box-Cox transformation')
lines(xx, box_cox(x=xx, lambda=lambda$lambda[2]), col='blue')
points(1, 0, pch=19, col='black')
abline(a=-1, b=1, lty=2, col='grey')
legend('bottomright', c(expression(lambda[x]),expression(lambda[y]),
expression(paste(lambda,'=1'))),
col=c('blue','red','grey'),lty=c(1,1,1))
plot(b, pch=19, main='Data', xlim=c(1,7))
points(rep(1,200), y, pch=19, col='blue') # projection on y
points(x, rep(-20,200), pch=19, col='red')  # projection on x
plot(BC.x, BC.y, pch=19, main='Bivariate BC', xlim=c(0,5))
points(rep(0,200), BC.y, pch=19, col='blue') # projection on y
points(BC.x, rep(0.7,200), pch=19, col='red')  # projection on x
}
plot.bivariate.bc()
# Let's formulate an hypothesis of transformation:
# since we get lambda[1]~1 and lambda[2]~0, we could reasonably consider:
hyp.x <- x
hyp.y <- log(y)
plot.bivariate.bc.hyp <- function() {
par(mfrow=c(3,3))
plot(b, pch=19, main='Data', xlim=c(1,7))
points(rep(1,200), y, pch=19, col='blue')   # projection on y
points(x, rep(-20,200), pch=19, col='red')  # projection on x
plot(BC.x, BC.y, pch=19, main='BC Bivariate', xlim=c(0,7), ylim=c(0.5,7.5))
points(rep(0,200), BC.y, pch=19, col='blue')   # projection on y
points(BC.x, rep(0.5,200), pch=19, col='red')  # projection on x
plot(hyp.x, hyp.y, pch=19, main='According to hyp.', xlim=c(0,7), ylim=c(0.5,7.5))
points(rep(0,200), hyp.y, pch=19, col='blue')  # projection on y
points(hyp.x, rep(0.5,200), pch=19, col='red') # projection on x
qqnorm(x, main="x", col='red')
qqnorm(BC.x, main="BC.x", col='red')
qqnorm(hyp.x, main="hyp.x", col='red')
qqnorm(y, main="y", col='blue')
qqnorm(BC.y, main="BC.y", col='blue')
qqnorm(hyp.y, main="hyp.y", col='blue')
}
plot.bivariate.bc.hyp()
# Univariate Shapiro-Wilk (H0: univariate gaussianity along a given direction)
shapiro.test(x)$p
shapiro.test(BC.x)$p
shapiro.test(hyp.x)$p
shapiro.test(y)$p
shapiro.test(BC.y)$p
shapiro.test(hyp.y)$p
# Multivariate normality test (H0: multivariate gaussianity)
mvn(cbind(x, y))$multivariateNormality
mvn(cbind(BC.x, BC.y))$multivariateNormality
mvn(cbind(hyp.x, hyp.y))$multivariateNormality
dim(stiff)
lambda.mult <- powerTransform(stiff)
lambda.mult
BC.x <- bcPower(stiff[, 1], lambda.mult$lambda[1])
BC.y <- bcPower(stiff[, 2], lambda.mult$lambda[2])
BC.z <- bcPower(stiff[, 3], lambda.mult$lambda[3])
BC.w <- bcPower(stiff[, 4], lambda.mult$lambda[4])
# Plot of the original variables
attach(stiff)
plot.hist.qq <- function(stiff) {
par(mfrow=c(2,4))
xx<-seq(1320, 3000, length=100)
hist(V1, prob=T, breaks=8, col='grey85')
lines(xx, dnorm(xx,mean(V1), sd(V1)), col='blue', lty=2)
yy<-seq(1150, 2800, length=100)
hist(V2, prob=T,breaks=8,col='grey85')
lines(yy, dnorm(yy,mean(V2), sd(V2)), col='blue', lty=2)
zz<-seq(1000, 2420, length=100)
hist(V3, prob=T, breaks=8, col='grey85')
lines(zz, dnorm(zz,mean(V3), sd(V3)), col='blue', lty=2)
ww<-seq(1100, 2600, length=100)
hist(V4, prob=T, breaks=8, col='grey85')
lines(ww, dnorm(ww, mean(V4), sd(V4)), col='blue', lty=2)
qqnorm(V1, main='QQplot of V1')
qqline(V1)
qqnorm(V2, main='QQplot of V2')
qqline(V2)
qqnorm(V3, main='QQplot of V3')
qqline(V3)
qqnorm(V4, main='QQplot of V4')
qqline(V4)
}
plot.hist.qq(stiff)
detach(stiff)
# Plot of the transformed variables
plot.hist.qq.bc <- function() {
par(mfrow=c(2,4))
xx<-seq(10, 12, length=100)
hist(BC.x, prob=T, breaks=8, col='grey85')
lines(xx, dnorm(xx, mean(BC.x), sd(BC.x)), col='blue', lty=2)
yy<-seq(3, 3.2, length=100)
hist(BC.y, prob=T, breaks=8, col='grey85')
lines(yy, dnorm(yy, mean(BC.y), sd(BC.y)), col='blue', lty=2)
zz<-seq(12, 15, length=100)
hist(BC.z, prob=T, breaks=8, col='grey85')
lines(zz, dnorm(zz, mean(BC.z), sd(BC.z)), col='blue', lty=2)
ww<-seq(250, 500, length=100)
hist(BC.w, prob=T, breaks=8, col='grey85')
lines(ww, dnorm(ww, mean(BC.w), sd(BC.w)), col='blue', lty=2)
qqnorm(BC.x, main='QQplot of BC.x')
qqline(BC.x)
qqnorm(BC.y, main='QQplot of BC.y')
qqline(BC.y)
qqnorm(BC.z, main='QQplot of BC.z')
qqline(BC.z)
qqnorm(BC.w, main='QQplot of BC.w')
qqline(BC.w)
par(mfrow=c(1,1))
}
plot.hist.qq.bc()
# One transformed variable at a time
shapiro.test(BC.x)
shapiro.test(BC.y)
shapiro.test(BC.z)
shapiro.test(BC.w)
# All together
mvn(cbind(BC.x, BC.y, BC.z, BC.w))$multivariateNormality
## Example with simulated data from a bivariate Gaussian --------------------------------------
mu <- c(1, 0)
## Example with simulated data from a bivariate Gaussian --------------------------------------
mu <- c(1, 0)
## Example with simulated data from a bivariate Gaussian --------------------------------------
mu <- c(1, 0)
sig <- matrix(c(1, 1, 1, 2), nrow = 2)
set.seed(123)
x <- rmvnorm(n = 30, mean = mu, sigma = sig)
x <- data.frame(X.1 = x[, 1], X.2 = x[, 2])
plot(x, asp = 1, pch = 19)
n <- dim(x)[1]
p <- dim(x)[2]
x.mean   <- sapply(x, mean)
x.cov    <- cov(x)
x.invcov <- solve(x.cov)
# Test on the mean of level alpha=1%
# H0: mu == mu0 vs H1: mu != mu0
# with mu0 = c(1, 0)
mvn(x)$multivariateNormality
alpha <- .01
mu0 <- c(1, 0)
### Inference relying on normality -------------------------------------------------------------
# T2 Statistics
x.T2       <- n * (x.mean - mu0) %*% x.invcov %*% (x.mean - mu0)
# Radius of the ellipsoid
cfr.fisher <- ((n - 1) * p / (n - p)) * qf(1 - alpha, p, n - p)
# Test:
x.T2 < cfr.fisher   # no statistical evidence to reject H0 at level alpha
# Compute the p-value
P <- 1 - pf(x.T2 * (n - p) / ((n - 1) * p), p, n - p)
P
xx <- seq(0, 40, by = 0.05)
plot(
xx,
df(xx * (n - p) / ((n - 1) * p), p, n - p),
type = "l",
lwd = 2,
main = 'Density F(p,n-p)',
xlab = 'x*(n-p)/((n-1)*p)',
ylab = 'Density'
)
abline(
h = 0,
v = x.T2 * (n - p) / ((n - 1) * p),
col = c('grey', 'red'),
lwd = 2,
lty = c(2, 1)
)
# Region of rejection (centered in mu0)
plot(x, asp = 1)
ellipse(
mu0,
shape = x.cov / n,
sqrt(cfr.fisher),
col = 'blue',
lty = 2,
center.pch = 16
)
# We add a red point in correspondence of the sample mean
points(x.mean[1], x.mean[2], pch = 16, col ='red', cex = 1.5)
ellipse(x.mean, x.cov/n, sqrt(cfr.fisher), col = 'red', lty = 2, lwd=2, center.cex=1)
# Note: we don't need to verify the Gaussian assumption!
# Warning: we are going to use an asymptotic test, but we only have n = 30 data!
# As a rule of thumb, you should consider that you don't have enough data for asymptotic test
# when n is below 30 * p^2
mu0   <- c(1, 0)
# Note: we don't need to verify the Gaussian assumption!
# Warning: we are going to use an asymptotic test, but we only have n = 30 data!
# As a rule of thumb, you should consider that you don't have enough data for asymptotic test
# when n is below 30 * p^2
mu0   <- c(1, 0)
x.T2A   <- n * (x.mean - mu0) %*%  x.invcov  %*% (x.mean - mu0)
cfr.chisq <- qchisq(1 - alpha, p)
x.T2A < cfr.chisq # no statistical evidence to reject H0 at level alpha
# Compute the p-value
PA <- 1 - pchisq(x.T2A, p)
PA
plot.exact.asympt <- function() {
par(mfrow=c(1,2))
plot(x, asp = 1,main='Comparison rejection regions')
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 1,
center.pch = 4, center.cex=1.5, lwd=2)
ellipse(mu0, x.cov/n, sqrt(cfr.chisq), col = 'lightblue', lty = 1,
center.pch = 4, center.cex=1.5, lwd=2)
points(mu0[1], mu0[2], pch = 4, cex = 1.5, lwd = 2, col ='lightblue')
legend('topleft', c('Exact', 'Asymptotic'), col=c('blue', 'lightblue'),
lty=c(1), lwd=2)
plot(x, asp = 1,main='Comparison of confidence regions')
ellipse(x.mean, x.cov/n, sqrt(cfr.fisher), col = 'red', lty = 1, center.pch = 4,
center.cex=1.5, lwd=2)
ellipse(x.mean, x.cov/n, sqrt(cfr.chisq), col = 'orange', lty = 1, center.pch = 4,
center.cex=1.5, lwd=2)
points(x.mean[1], x.mean[2], pch = 4, cex = 1.5, lwd = 2, col ='orange')
legend('topleft', c('Exact', 'Asymptotic'), col=c('red','orange'),
lty=c(1), lwd=2)
par(mfrow = c(1, 1))
}
plot.exact.asympt()
#  We change the null Hypothesis
mu0 <- c(1.5, -0.5)
mu
# Ellipsoidal region
x.T2 <- n * t(x.mean - mu0) %*% x.invcov %*% (x.mean - mu0)
x.T2 < cfr.fisher
# Compute the p-value
P <- 1 - pf(x.T2 * (n - p) / ((n - 1) * p), p, n - p)
P
plot(x, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
points(x.mean[1], x.mean[2], pch = 16, col = 'red', cex=1.5)
T2 <- cbind(inf = x.mean - sqrt(cfr.fisher*diag(x.cov)/n),
center = x.mean,
sup = x.mean + sqrt(cfr.fisher*diag(x.cov)/n))
T2
par(mfrow=c(1,1))
# Plot of the acceptance region and the sample mean
plot(x, asp = 1,main='Confidence and rejection regions')
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
points(x.mean[1], x.mean[2], pch = 16, col = 'red', cex=1.5)
# Plot of the confidence region and the region defined by the cartesian product
# of the sim. confidence intervals in direction of x1 and x2
ellipse(x.mean, shape=x.cov/n, sqrt(cfr.fisher), col = 'red', lty = 2, center.pch = 16)
rect(T2[1,1], T2[2,1], T2[1,3], T2[2,3], border='red', lwd=2)
# Let's try with Bonferroni intervals
k <- p # number of intervals I want to compute (set in advance)
cfr.t <- qt(1-alpha/(2*k), n-1)
Bf <- cbind(inf = x.mean - cfr.t*sqrt(diag(x.cov)/n),
center = x.mean,
sup = x.mean + cfr.t*sqrt(diag(x.cov)/n))
Bf
# we add the Bonferroni intervals to the plot
rect(Bf[1,1], Bf[2,1], Bf[1,3], Bf[2,3], border='orange', lwd=2)
legend('topleft', c('Rej. Reg.', 'Conf. Reg','T2-sim', 'Bonferroni'),
col=c('blue','red','red','orange'), lty=c(2,2,1,1), lwd=2)
## Example with board stiffness dataset -------------------------------------------------------
stiff <- read.table('stiff.dat')
head(stiff)
dim(stiff)
n <- dim(stiff)[1]
p <- dim(stiff)[2]
plot(stiff, pch = 19)
# Normality test
mvn(stiff)$multivariateNormality
