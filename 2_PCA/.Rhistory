# Explained variance
x11()
layout(matrix(c(2,3,1,3),2,byrow=T))
plot(pc.runrec, las=2, main='Principal components', ylim=c(0,3.5e6))
barplot(sapply(runrec,sd)^2, las=2, main='Original Variables', ylim=c(0,3.5e6), ylab='Variances')
plot(cumsum(pc.runrec$sd^2)/sum(pc.runrec$sd^2), type='b', axes=F, xlab='number of components',
ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(runrec),labels=1:ncol(runrec),las=2)
# scores
scores.runrec <- pc.runrec$scores
layout(matrix(c(1,2),2))
boxplot(runrec, las=2, col='red', main='Original variables')
scores.runrec <- data.frame(scores.runrec)
boxplot(scores.runrec, las=2, col='red', main='Principal components')
biplot(pc.runrec, scale=0, cex=.7)
graphics.off()
# We compute the standardized variables
runrec.sd <- scale(runrec)
runrec.sd <- data.frame(runrec.sd)
pc.runrec <- princomp(runrec.sd, scores=T)
pc.runrec
summary(pc.runrec)
# Explained variance
x11()
layout(matrix(c(2,3,1,3),2,byrow=T))
plot(pc.runrec, las=2, main='Principal Components', ylim=c(0,6))
abline(h=1, col='blue')
barplot(sapply(runrec.sd,sd)^2, las=2, main='Original Variables', ylim=c(0,6), ylab='Variances')
plot(cumsum(pc.runrec$sde^2)/sum(pc.runrec$sde^2), type='b', axes=F, xlab='Number of components', ylab='Contribution to the total variance', ylim=c(0,1))
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(runrec.sd),labels=1:ncol(runrec.sd),las=2)
# loadings
load.rec <- pc.runrec$loadings
load.rec
x11()
par(mar = c(2,2,2,1), mfrow=c(3,1))
for(i in 1:3)barplot(load.rec[,i], ylim = c(-1, 1), main=paste('Loadings PC ',i,sep=''))
# scores
scores.runrec <- pc.runrec$scores
scores.runrec
x11()
plot(scores.runrec[,1:2])
x11()
layout(matrix(c(1,2),2))
boxplot(runrec.sd, las=2, col='red', main='Original variables')
scores.runrec <- data.frame(scores.runrec)
boxplot(scores.runrec, las=2, col='red', main='Principal components')
x11()
layout(matrix(c(1,2),1))
plot(runrec.sd[,'m100'],runrec.sd[,'Marathon'],type="n",xlab="m100",ylab="Marathon", asp=1)
text(runrec.sd[,'m100'],runrec.sd[,'Marathon'],dimnames(runrec)[[1]],cex=.7)
plot(scores.runrec[,1],scores.runrec[,2],type="n",xlab="pc1",ylab="pc2", asp=1)
text(scores.runrec[,1],scores.runrec[,2],dimnames(runrec)[[1]],cex=.7)
x11()
biplot(pc.runrec)
graphics.off()
# Import the data
NO <- read.table('NO.txt', header=T)
NO
dim(NO)
dimnames(NO)
NO <- data.frame(NO)
var.names <- c("I Control Unit","II Control Unit","III Control Unit","IV Control Unit")
dimnames(NO)[[2]] <- var.names
## DATA EXPLORATION ##
# Scatter plot
pairs(NO, col=rainbow(dim(NO)[1]), pch=16, main='Scatter plot')
# This is confirmed by quantitative analyses
# we compute the sample mean, sample covariance matrix and sample correlation matrix
M <- sapply(NO,mean)
M
S <- cov(NO)
S
R <- cor(NO)
R
# Boxplot
x11()
boxplot(NO, las=1, col='red', main='Boxplot',grid=T)
# Matplot + boxplot
x11()
matplot(t(NO), type='l', axes=F)
box()
boxplot(NO, add=T, boxwex=0.1, col='red')
## PRINCIPAL COMPONENT ANALYSIS ##
## COMMENT ##
# The original variability along the 3 variables is similar; the units of
# measure of the variables are homogeneous. We thus perform a PCA based on S
pca.NO <- princomp(NO, scores=T)
pca.NO
summary(pca.NO)
x11()
layout(matrix(c(2,3,1,3),2,byrow=T))
barplot(pca.NO$sdev^2, las=2, main='Principal components', ylim=c(0,10), ylab='Variances')
barplot(sapply(NO,sd)^2, las=2, main='Original variables', ylim=c(0,5), ylab='Variances')
plot(cumsum(pca.NO$sdev^2)/sum(pca.NO$sde^2), type='b', axes=F, xlab='number of components',
ylab='contribution to the total variability', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(NO),labels=1:ncol(NO),las=2)
## COMMENT ##
# Dimensionality reduction: the first two components explain 73.6%<80% of the total
# variability. However, we do not see an elbow in the proportion of explained variability
# in correspondence of the III or IV PC. It could be thus sufficient to analyse the
# sample through the first 2 PCs.
# Note. The results of the PCA on the correlation matrix would not give very different results
NO.sd <- scale(NO)
pca.NO.std <- princomp(NO.sd, scores=T)
pca.NO.std
summary(pca.NO.std) # PCA on R
summary(pca.NO)     # PCA on S
# SCORES AND LOADINGS (PCA on S)#
# Scores
scores.NO <- pca.NO$scores
scores.NO
layout(matrix(c(1,2),1,2))
boxplot(NO, las=2, col='red', main='Variabili originarie')
scores.NO <- data.frame(scores.NO)
boxplot(scores.NO, las=2, col='red', main='Componenti principali')
# We plot the outlying data for the first 2 PCs
color.outliers <- NULL
for (i in 1:365){
color.outliers[i] <- 'blue'
}
color.outliers[5] <- 'red'
color.outliers[217] <- 'red'
color.outliers[3] <- 'green'
color.outliers[125] <- 'green'
color.outliers[59] <- 'green'
color.outliers[308] <- 'green'
pairs(NO, col=color.outliers, pch=16, main='Scatter plot - Outliers')
# Loadings
load.NO    <- pca.NO$loadings
load.NO
x11()
par(mar = c(1,4,0,2), mfrow = c(4,1))
for(i in 1:4)
barplot(load.NO[,i], ylim = c(-1, 1))
library(rgl)
M <- sapply(NO[,1:3],mean)
S <- cov(NO[,1:3])
open3d()
points3d(NO[,1:3], col=rainbow(dim(NO)[1]), asp=1, size=5)
axes3d()
plot3d(ellipse3d(S, centre=M, level= 9/10), alpha=0.25, add = TRUE)
title3d(xlab="I CU",ylab="II CU",zlab="III CU")
pca.NO1 <- NULL
for(i in 1:365)
pca.NO1 <- rbind(pca.NO1, pca.NO$loadings[1:3,1]*pca.NO$scores[i,1] + M)
points3d(pca.NO1, col='black', size=6)
lines3d(rbind(M + 4*pca.NO$sdev[1] * pca.NO$loadings[1:3,1], M - 4*pca.NO$sdev[1] * pca.NO$loadings[1:3,1]), col='black')
color=rainbow(dim(NO)[1])
for(i in 1:365)
lines3d(rbind(NO[i,], pca.NO1[i,]),col=color[i])
open3d()
points3d(NO[,1:3], col=color.outliers, asp=1, size=5)
axes3d()
plot3d(ellipse3d(S, centre=M, level= 9/10), alpha=0.25, add = TRUE)
title3d(xlab="I CU",ylab="II CU",zlab="III CU")
pca.NO1 <- NULL
for(i in 1:365)
pca.NO1 <- rbind(pca.NO1, pca.NO$loadings[1:3,1]*pca.NO$scores[i,1] + M)
points3d(pca.NO1, col='black', size=5)
x11()
plot(NO[,4],scores.NO[,2],col=rainbow(n),pch=19,xlab='IV CU',ylab='Comp.2')
##TRY TO REMOVE THE OUTLIERS##
NO.outliers <- NO[which(color.outliers=='blue'),]
dim(NO.outliers)
pca.NO.outliers <- princomp(NO.outliers, scores=T)
pca.NO.outliers
summary(pca.NO.outliers)
# SCORES AND LOADINGS #
# Scores
scores.NO.outliers <- pca.NO.outliers$scores
scores.NO.outliers
layout(matrix(c(1,2),1,2))
boxplot(NO.outliers, las=2, col='red', main='Original Variables')
scores.NO.outliers <- data.frame(scores.NO.outliers)
boxplot(scores.NO.outliers, las=2, col='red', main='Principal Components')
# Loadings
load.NO.outliers    <- pca.NO.outliers$loadings
load.NO.outliers
x11()
par(mar = c(1,4,0,2), mfrow = c(4,1))
for(i in 1:4)
barplot(load.NO.outliers[,i], ylim = c(-1, 1))
data.3jul <- c(13, 10, 11, 13)
scores.3jul <- t(pca.NO$loadings)%*%(data.3jul-colMeans(NO))
scores.3jul
x11()
pairs(data.frame(rbind(NO,data.3jul)), col=c(rep(1,n),2), pch=16, main='Scatter plot - Data 3rd July')
x11()
plot(scores.NO[,1],scores.NO[,2],col='grey',pch=19,xlab='Comp.1',ylab='Comp.2')
points(scores.3jul[1],scores.3jul[2],col='black',pch=19)
# Import the data
NO <- read.table('NO.txt', header=T)
NO
dim(NO)
dimnames(NO)
NO <- data.frame(NO)
var.names <- c("I Control Unit","II Control Unit","III Control Unit","IV Control Unit")
dimnames(NO)[[2]] <- var.names
## DATA EXPLORATION ##
# Scatter plot
pairs(NO, col=rainbow(dim(NO)[1]), pch=16, main='Scatter plot')
# This is confirmed by quantitative analyses
# we compute the sample mean, sample covariance matrix and sample correlation matrix
M <- sapply(NO,mean)
M
S <- cov(NO)
S
R <- cor(NO)
R
# Boxplot
x11()
boxplot(NO, las=1, col='red', main='Boxplot',grid=T)
# Matplot + boxplot
x11()
matplot(t(NO), type='l', axes=F)
box()
boxplot(NO, add=T, boxwex=0.1, col='red')
## PRINCIPAL COMPONENT ANALYSIS ##
## COMMENT ##
# The original variability along the 3 variables is similar; the units of
# measure of the variables are homogeneous. We thus perform a PCA based on S
pca.NO <- princomp(NO, scores=T)
pca.NO
summary(pca.NO)
x11()
layout(matrix(c(2,3,1,3),2,byrow=T))
barplot(pca.NO$sdev^2, las=2, main='Principal components', ylim=c(0,10), ylab='Variances')
barplot(sapply(NO,sd)^2, las=2, main='Original variables', ylim=c(0,5), ylab='Variances')
plot(cumsum(pca.NO$sdev^2)/sum(pca.NO$sde^2), type='b', axes=F, xlab='number of components',
ylab='contribution to the total variability', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(NO),labels=1:ncol(NO),las=2)
## COMMENT ##
# Dimensionality reduction: the first two components explain 73.6%<80% of the total
# variability. However, we do not see an elbow in the proportion of explained variability
# in correspondence of the III or IV PC. It could be thus sufficient to analyse the
# sample through the first 2 PCs.
# Note. The results of the PCA on the correlation matrix would not give very different results
NO.sd <- scale(NO)
pca.NO.std <- princomp(NO.sd, scores=T)
pca.NO.std
summary(pca.NO.std) # PCA on R
summary(pca.NO.std) # PCA on R
summary(pca.NO)     # PCA on S
# SCORES AND LOADINGS (PCA on S)#
# Scores
scores.NO <- pca.NO$scores
scores.NO
layout(matrix(c(1,2),1,2))
boxplot(NO, las=2, col='red', main='Variabili originarie')
scores.NO <- data.frame(scores.NO)
boxplot(scores.NO, las=2, col='red', main='Componenti principali')
pairs(scores.NO, col=rainbow(dim(scores.NO)[1]), pch=16)
cor(scores.NO)
scores.NO
pca.NO$scores
head(scores.NO)
head(pca.NO$scores)
tail(scores.NO)
tail(pca.NO$scores)
# We plot the outlying data for the first 2 PCs
color.outliers <- NULL
for (i in 1:365){
color.outliers[i] <- 'blue'
}
color.outliers[5] <- 'red'
color.outliers[217] <- 'red'
color.outliers[3] <- 'green'
color.outliers[125] <- 'green'
color.outliers[59] <- 'green'
color.outliers[308] <- 'green'
pairs(NO, col=color.outliers, pch=16, main='Scatter plot - Outliers')
# Loadings
load.NO    <- pca.NO$loadings
load.NO
load.NO
pca.NO.std$loadings
x11()
par(mar = c(1,4,0,2), mfrow = c(4,1))
for(i in 1:4)
barplot(load.NO[,i], ylim = c(-1, 1))
library(rgl)
M <- sapply(NO[,1:3],mean)
S <- cov(NO[,1:3])
open3d()
points3d(NO[,1:3], col=rainbow(dim(NO)[1]), asp=1, size=5)
axes3d()
plot3d(ellipse3d(S, centre=M, level= 9/10), alpha=0.25, add = TRUE)
title3d(xlab="I CU",ylab="II CU",zlab="III CU")
pca.NO1 <- NULL
for(i in 1:365)
pca.NO1 <- rbind(pca.NO1, pca.NO$loadings[1:3,1]*pca.NO$scores[i,1] + M)
points3d(pca.NO1, col='black', size=6)
lines3d(rbind(M + 4*pca.NO$sdev[1] * pca.NO$loadings[1:3,1], M - 4*pca.NO$sdev[1] * pca.NO$loadings[1:3,1]), col='black')
color=rainbow(dim(NO)[1])
for(i in 1:365)
lines3d(rbind(NO[i,], pca.NO1[i,]),col=color[i])
open3d()
points3d(NO[,1:3], col=color.outliers, asp=1, size=5)
axes3d()
plot3d(ellipse3d(S, centre=M, level= 9/10), alpha=0.25, add = TRUE)
title3d(xlab="I CU",ylab="II CU",zlab="III CU")
pca.NO1 <- NULL
for(i in 1:365)
pca.NO1 <- rbind(pca.NO1, pca.NO$loadings[1:3,1]*pca.NO$scores[i,1] + M)
points3d(pca.NO1, col='black', size=5)
x11()
plot(NO[,4],scores.NO[,2],col=rainbow(n),pch=19,xlab='IV CU',ylab='Comp.2')
##TRY TO REMOVE THE OUTLIERS##
NO.outliers <- NO[which(color.outliers=='blue'),]
dim(NO.outliers)
pca.NO.outliers <- princomp(NO.outliers, scores=T)
pca.NO.outliers
summary(pca.NO.outliers)
# SCORES AND LOADINGS #
# Scores
scores.NO.outliers <- pca.NO.outliers$scores
scores.NO.outliers
layout(matrix(c(1,2),1,2))
boxplot(NO.outliers, las=2, col='red', main='Original Variables')
scores.NO.outliers <- data.frame(scores.NO.outliers)
boxplot(scores.NO.outliers, las=2, col='red', main='Principal Components')
# Loadings
load.NO.outliers    <- pca.NO.outliers$loadings
load.NO.outliers
x11()
par(mar = c(1,4,0,2), mfrow = c(4,1))
for(i in 1:4)
barplot(load.NO.outliers[,i], ylim = c(-1, 1))
data.3jul <- c(13, 10, 11, 13)
data.3jul <- c(13, 10, 11, 13)
scores.3jul <- t(pca.NO$loadings)%*%(data.3jul-colMeans(NO))
scores.3jul
x11()
pairs(data.frame(rbind(NO,data.3jul)), col=c(rep(1,n),2), pch=16, main='Scatter plot - Data 3rd July')
x11()
plot(scores.NO[,1],scores.NO[,2],col='grey',pch=19,xlab='Comp.1',ylab='Comp.2')
points(scores.3jul[1],scores.3jul[2],col='black',pch=19)
# Import the data
NO <- read.table('NO.txt', header=T)
NO
dim(NO)
dimnames(NO)
NO <- data.frame(NO)
var.names <- c("I Control Unit","II Control Unit","III Control Unit","IV Control Unit")
dimnames(NO)[[2]] <- var.names
## DATA EXPLORATION ##
# Scatter plot
pairs(NO, col=rainbow(dim(NO)[1]), pch=16, main='Scatter plot')
# This is confirmed by quantitative analyses
# we compute the sample mean, sample covariance matrix and sample correlation matrix
M <- sapply(NO,mean)
M
S <- cov(NO)
S
R <- cor(NO)
R
# Boxplot
x11()
boxplot(NO, las=1, col='red', main='Boxplot',grid=T)
# Matplot + boxplot
x11()
matplot(t(NO), type='l', axes=F)
box()
boxplot(NO, add=T, boxwex=0.1, col='red')
## PRINCIPAL COMPONENT ANALYSIS ##
## COMMENT ##
# The original variability along the 3 variables is similar; the units of
# measure of the variables are homogeneous. We thus perform a PCA based on S
pca.NO <- princomp(NO, scores=T)
pca.NO
## PRINCIPAL COMPONENT ANALYSIS ##
## COMMENT ##
# The original variability along the 3 variables is similar; the units of
# measure of the variables are homogeneous. We thus perform a PCA based on S
pca.NO <- princomp(NO, scores=T)
pca.NO
summary(pca.NO)
x11()
layout(matrix(c(2,3,1,3),2,byrow=T))
barplot(pca.NO$sdev^2, las=2, main='Principal components', ylim=c(0,10), ylab='Variances')
barplot(sapply(NO,sd)^2, las=2, main='Original variables', ylim=c(0,5), ylab='Variances')
plot(cumsum(pca.NO$sdev^2)/sum(pca.NO$sde^2), type='b', axes=F, xlab='number of components',
ylab='contribution to the total variability', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(NO),labels=1:ncol(NO),las=2)
## COMMENT ##
# Dimensionality reduction: the first two components explain 73.6%<80% of the total
# variability. However, we do not see an elbow in the proportion of explained variability
# in correspondence of the III or IV PC. It could be thus sufficient to analyse the
# sample through the first 2 PCs.
# Note. The results of the PCA on the correlation matrix would not give very different results
NO.sd <- scale(NO)
pca.NO.std <- princomp(NO.sd, scores=T)
## COMMENT ##
# Dimensionality reduction: the first two components explain 73.6%<80% of the total
# variability. However, we do not see an elbow in the proportion of explained variability
# in correspondence of the III or IV PC. It could be thus sufficient to analyse the
# sample through the first 2 PCs.
# Note. The results of the PCA on the correlation matrix would not give very different results
NO.sd <- scale(NO)
pca.NO.std <- princomp(NO.sd, scores=T)
pca.NO.std
summary(pca.NO.std) # PCA on R
summary(pca.NO)     # PCA on S
# SCORES AND LOADINGS (PCA on S)#
# Scores
scores.NO <- pca.NO$scores
tail(scores.NO)
tail(pca.NO$scores)
# Loadings
load.NO    <- pca.NO$loadings
load.NO
pca.NO.std$loadings
help(princomp)
data.3jul <- c(13, 10, 11, 13)
scores.3jul <- t(pca.NO$loadings)%*%(data.3jul-colMeans(NO))
scores.3jul
predict(data.3jul, pca.NO)
predict(pca.NO, data.3jul)
predict(pca.NO, as.matrixx(data.3jul))
predict(pca.NO, as.matrix(data.3jul))
View(NO)
colnames(NO)
df_data.3jul < data.frame(colnames(NO), c(13, 10, 11, 13))
df_data.3jul <- data.frame(colnames(NO), c(13, 10, 11, 13))
View(df_data.3jul)
df_data.3jul <- data.frame("I Control Unit" = 13,
"II Control Unit" = 10,
"III Control Unit" = 11,
"IV Control Unit" = 13))
df_data.3jul <- data.frame("I Control Unit" = 13,
"II Control Unit" = 10,
"III Control Unit" = 11,
"IV Control Unit" = 13)
predict(pca.NO, data.3jul)
predict(pca.NO, as.matrix(data.3jul))
pca.NO
View(df_data.3jul)
data.3jul <- c(13, 10, 11, 13)
df_data.3jul <- as.data.frame(t(data.3jul))
scores.3jul <- t(pca.NO$loadings)%*%(data.3jul-colMeans(NO))
scores.3jul
df_data.3jul <- as.data.frame(t(data.3jul))
colnames(df_data.3jul) <- colnames(NO)
predict(pca.NO, as.matrix(data.3jul))
# Import the data
NO <- read.table('NO.txt', header=T)
NO
dim(NO)
dimnames(NO)
NO <- data.frame(NO)
var.names <- c("I Control Unit","II Control Unit","III Control Unit","IV Control Unit")
dimnames(NO)[[2]] <- var.names
## DATA EXPLORATION ##
# Scatter plot
pairs(NO, col=rainbow(dim(NO)[1]), pch=16, main='Scatter plot')
# This is confirmed by quantitative analyses
# we compute the sample mean, sample covariance matrix and sample correlation matrix
M <- sapply(NO,mean)
M
S <- cov(NO)
S
R <- cor(NO)
R
# Boxplot
x11()
boxplot(NO, las=1, col='red', main='Boxplot',grid=T)
# Matplot + boxplot
x11()
matplot(t(NO), type='l', axes=F)
box()
boxplot(NO, add=T, boxwex=0.1, col='red')
## PRINCIPAL COMPONENT ANALYSIS ##
## COMMENT ##
# The original variability along the 3 variables is similar; the units of
# measure of the variables are homogeneous. We thus perform a PCA based on S
pca.NO <- princomp(NO, scores=T)
pca.NO
summary(pca.NO)
data.3jul <- c(13, 10, 11, 13)
df_data.3jul <- as.data.frame(t(data.3jul))
colnames(df_data.3jul) <- colnames(NO)
scores.3jul <- t(pca.NO$loadings)%*%(data.3jul-colMeans(NO))
scores.3jul
predict(pca.NO, as.matrix(data.3jul))
View(df_data.3jul)
predict(pca.NO, data.3jul)
predict(pca.NO, df_data.3jul)
scores.3jul
as.amatrix(c(1,0, 1,0))
as.matrix(c(1,0, 1,0))
as.matrix(c(1,0,) c(1,0))
as.matrix(c(1,0,), c(1,0))
as.matrix(c(1,0), c(1,0))
as.matrix(c(1,0), c(0,1))
