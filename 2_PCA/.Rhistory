points3d(t(M), col='red', size=6)
for(i in 1:100)
lines3d(rbind(X[i,], M))
## Space spanned by the first PC --------------------------------------------------------------
# I principal component: the best approximation of dimension 1 (a line)
open3d()
points3d(X, asp=1, size=4)
axes3d()
PC1 <- NULL
for(i in 1:nobs) {
PC1 <- rbind(PC1, PC$loadings[,1]*PC$scores[i,1] + M)
}
points3d(PC1, col='red', size=6)
for(i in 1:nobs) {
lines3d(rbind(X[i,], PC1[i,]), col='blue')
}
lines3d(rbind(M + 2*PC$sdev[1] * PC$loadings[,1], M - 2*PC$sdev[1] * PC$loadings[,1]),
col='forestgreen',lwd=2)
## Space spanned by the first two PCs ----------------------------------------------------------
# I and II principal components: the best approximation of dimension 2 (a plane)
open3d()
points3d(X, asp=1, size=4)
axes3d()
PC12 <- NULL
for(i in 1:nobs) {
PC12 <- rbind(PC12, PC$loadings[,1]*PC$scores[i,1] + PC$loadings[,2]*PC$scores[i,2] + M)
}
points3d(PC12, col='red', size=6)
for(i in 1:nobs) {
lines3d(rbind(X[i,], PC12[i,]),col='blue')
}
lines3d(rbind(M + 2*PC$sdev[1] * PC$loadings[,1], M - 2*PC$sdev[1] * PC$loadings[,1]),
col='forestgreen',lwd=2)
lines3d(rbind(M + 2*PC$sdev[2] * PC$loadings[,2], M - 2*PC$sdev[2] * PC$loadings[,2]),
col='forestgreen',lwd=2)
## Space spanned by all PCs -------------------------------------------------------------------
# I, II and III principal components: the best approximation of dimension 3 (the entire space)
open3d()
points3d(X, asp=1, size=4)
axes3d()
PC123 <- NULL
for(i in 1:nobs) {
PC123 <- rbind(PC123, PC$loadings[,1]*PC$scores[i,1] + PC$loadings[,2]*PC$scores[i,2]
+ PC$loadings[,3]*PC$scores[i,3] + M)
}
points3d(PC123, col='red', size=6)
for(i in 1:nobs) {
lines3d(rbind(X[i,], PC123[i,]),col='blue')
}
lines3d(rbind(M + 2*PC$sdev[1] * PC$loadings[,1], M - 2*PC$sdev[1] * PC$loadings[,1]),
col='forestgreen',lwd=2)
lines3d(rbind(M + 2*PC$sdev[2] * PC$loadings[,2], M - 2*PC$sdev[2] * PC$loadings[,2]),
col='forestgreen',lwd=2)
lines3d(rbind(M + 2*PC$sdev[3] * PC$loadings[,3], M - 2*PC$sdev[3] * PC$loadings[,3]),
col='forestgreen',lwd=2)
tourists <- read.table('tourists.txt', header=T)
head(tourists)
dim(tourists)
tourists.label <- tourists[, 1:2]
tourists <- tourists[, -(1:2)]
n <- dim(tourists)[1]
p <- dim(tourists)[2]
# Visualize the original data using boxplots
# We observe that the variability of the number of nights in 3, 4-star hotels, and residences
# is higher than that of the others. This may influence the PCA.
boxplot(tourists, las = 2, col = 'gold')
# Note: PCA is not about the mean, it is about the variability
boxplot(scale(x = tourists, center = T, scale = F), las = 2, col = 'gold')
## PCA on the original data -------------------------------------------------------------------
pc.tourists <- princomp(tourists, scores = T)
pc.tourists
summary(pc.tourists)
# To obtain the rows of the summary:
# Standard deviation of the components
pc.tourists$sd
# Proportion of variance explained by each PC
pc.tourists$sd^2 / sum(pc.tourists$sd^2)
# Cumulative proportion of explained variance
cumsum(pc.tourists$sd^2) / sum(pc.tourists$sd^2)
# Get loadings (coefficients of the linear combination of the original variables that defines each
# principal component)
load.tour <- pc.tourists$loadings
load.tour
load.tour[, 1:8]
# graphical representation of the loadings of the first three principal components
par(mfrow = c(3,1))
for(i in 1:3) barplot(load.tour[,i], ylim = c(-1, 1))
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
# Variance explained by the principal components
plot(pc.tourists, las = 2, main = 'Principal components', ylim = c(0, 4.5e7))
# Variances of original variables
barplot(sapply(tourists, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 4.5e7),
ylab = 'Variances')
# Plot contribution to the total variance by number of components
plot(cumsum(pc.tourists$sd^2) / sum(pc.tourists$sd^2), type = 'b', axes = FALSE,
xlab = 'Number of components', ylab = 'Contribution to the total variance', ylim = c(0, 1))
abline(h = 1, col = 'blue')
abline(h = 0.8, lty = 2, col = 'blue')
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(tourists), labels = 1:ncol(tourists), las = 2)
## PCA on the standardized variables ----------------------------------------------------------
tourists.sd <- scale(tourists)
tourists.sd <- data.frame(tourists.sd)
head(tourists.sd)
# Boxplot
par(mfrow = c(1, 1))
boxplot(tourists.sd, las = 2, col = 'gold')
pc.tourists <- princomp(tourists.sd, scores = T)
pc.tourists
summary(pc.tourists)
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
# Variance explained by the principal components
plot(pc.tourists, las = 2, main = 'Principal Components', ylim = c(0, 7))
abline(h = 1, col = 'blue')
# Variances of original variables
barplot(sapply(tourists.sd, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 7),
ylab = 'Variances')
# Plot contribution to the total variance by number of components
plot(cumsum(pc.tourists$sde^2) / sum(pc.tourists$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of components', ylab = 'Contribution to the total variance', ylim = c(0, 1))
abline(h = 1, col = 'blue')
abline(h = 0.8, lty = 2, col = 'blue')
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(tourists.sd), labels = 1:ncol(tourists.sd), las = 2)
# Loadings
load.tour <- pc.tourists$loadings
load.tour
# Graphical representation
par(mar = c(2,2,2,1), mfrow=c(3,1))
for(i in 1:3)barplot(load.tour[,i], ylim = c(-1, 1), main=paste('Loadings PC ',i,sep=''))
## Scores -------------------------------------------------------------------------------------
scores.tourists <- pc.tourists$scores
scores.tourists
# Scatter plot of the scores
par(mfrow = c(1,1))
plot(scores.tourists[, 1:2])
abline(h=0, v=0, lty=2, col='grey')
loadings_matrix <- as.matrix(pc.tourists$loadings)
first_observation <- tourists.sd[1, , drop = FALSE]  # Убеждаемся, что это матрица
first_scores <- as.matrix(first_observation) %*% loadings_matrix
# same as we get before
scores.tourists[1,]
first_scores
# Biplot
par(mfrow = c(1, 1))
biplot(pc.tourists)
# Let's use the categorical variables to further interpret the results
head(tourists.label)
# Color according to Month
# We order the labels according to time order
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sept", "Oct",
"Nov", "Dec")
tourists.label[, 1] <- factor(tourists.label[, 1], levels = months)
# Define a cyclic palette
colors <- c("#2D1857", "#4D2C8C", "#6C47C7", "#8B63FF", "#7A8CDD", "#68B5C5",
"#5ECABA", "#46AF89", "#108963", "#047138", "#025A26", "#013814")
col.lab1 <- rep(NA, n)
for(i in 1:n)
col.lab1[i] <- colors[which(tourists.label[i, 1] == levels(tourists.label[, 1]))]
plot(scores.tourists[, 1:2], col = col.lab1, pch = 19, xlim = c(-8, 25), ylim = c(-3, 3.2))
abline(h = -3, v = -8, col = 1)
points(scores.tourists[, 1], rep(-3, n), col = col.lab1, pch = 19)
points(rep(-8, n), scores.tourists[, 2], col = col.lab1, pch = 19)
abline(h = 0, v = 0, lty = 2, col = 'grey')
legend('topright', levels(tourists.label[, 1]), fill = colors, bty = 'n')
# Months of Expo 2015: May to Oct
expo.label <- factor(ifelse(tourists.label[, 1] %in% c("May", "Jun", "Jul", "Aug", "Sept", "Oct"),
'Expo', 'Non Expo'))
col.expo <- ifelse(tourists.label[, 1] %in% c("May", "Jun", "Jul", "Aug", "Sept", "Oct"),
'red', 'blue')
layout(cbind(c(2, 4), c(1, 3)), widths = c(1, 4), heights = c(4, 1))
par(mar = rep(3, 4))
plot(scores.tourists[, 1:2], col = col.expo, pch = 19, xlim = c(-8, 20), ylim = c(-3, 3.2), las = 2)
abline(h = -3, v = -8, col = 1)
points(scores.tourists[, 1], rep(-3, n), col = col.expo, pch = 19)
points(rep(-8, n), scores.tourists[, 2], col = col.expo, pch = 19)
abline(h = 0, v = 0, lty = 2, col = 'grey')
boxplot(scores.tourists[, 2] ~ expo.label, col = c('red', 'blue'), ylim = c(-3, 3.2), las = 2)
boxplot(scores.tourists[, 1] ~ expo.label, col = c('red', 'blue'), ylim = c(-8, 20), horizontal = T,
las = 2)
# Color according to Region of Origin
col.lab2 <- rainbow(length(unique(tourists.label[, 2])))
par(mfrow = c(1, 1))
plot(
scores.tourists[, 1:2],
col = col.lab2,
pch = 19,
xlim = c(-8, 30),
ylim = c(-3, 3.2)
)
abline(h = -3, v = -8, col = 1)
points(scores.tourists[, 1], rep(-3, n), col = col.lab2, pch = 19)
points(rep(-8, n), scores.tourists[, 2], col = col.lab2, pch = 19)
abline(h = 0, v = 0, lty = 2, col = 'grey')
legend('topright', levels(factor(tourists.label[, 2])), fill = col.lab2, bty = 'n')
graphics.off()
##### Homework: try to perform the PCA on the log-transformed data -------------
tourists.mod = tourists
for(i in 1:8) tourists.mod[which(tourists[,i]==0),i] = 1
tourists.log = log(tourists.mod)
head(tourists.log)
pc.tourists_log <- princomp(tourists.log, scores = T)
pc.tourists_log
summary(pc.tourists_log)
# To obtain the rows of the summary:
# Standard deviation of the components
pc.tourists_log$sd
# Proportion of variance explained by each PC
pc.tourists_log$sd^2 / sum(pc.tourists_log$sd^2)
# Cumulative proportion of explained variance
cumsum(pc.tourists_log$sd^2) / sum(pc.tourists_log$sd^2)
# Get loadings (coefficients of the linear combination of the original variables that defines each
# principal component)
load.tour_log <- pc.tourists_log$loadings
load.tour_log
load.tour_log[, 1:8]
# graphical representation of the loadings of the first three principal components
par(mfrow = c(3,1))
for(i in 1:3) barplot(load.tour_log[,i], ylim = c(-1, 1))
# PCA on the Food dataset ---------------------------------------------------------------------
food <- read.table('Food.txt', header = TRUE)
head(food)
dim(food)
n <- dim(food)[1]
p <- dim(food)[2]
# Exploration
boxplot(food, col = 'gold')
boxplot(scale(x = food, center = TRUE, scale = FALSE), col = 'gold')
# Principal component analysis of the dataset 'food', based on the correlation matrix
food.sd <- scale(food)
food.sd <- data.frame(food.sd)
head(food.sd)
sapply(food.sd, mean)
sapply(food.sd, sd)
cov(food.sd)
cor(food)
# PC on correlation matrix
pc.food <- princomp(food.sd, scores=T)
pc.food
summary(pc.food)
# Explained variance
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
barplot(pc.food$sdev^2, las = 2, main = 'Principal Components', ylim = c(0, 4), ylab = 'Variances')
abline(h = 1, col = 'blue')
barplot(sapply(food.sd, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 4),
ylab = 'Variances')
plot(cumsum(pc.food$sdev^2) / sum(pc.food$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of Components', ylab = 'Contribution to the Total Variance', ylim = c(0, 1))
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(food.sd), labels = 1:ncol(food.sd), las = 2)
# Scores
scores.food <- pc.food$scores
# Variability of the original variables / scores
layout(matrix(c(1, 2), 2))
boxplot(food.sd, las = 2, col = 'gold', main = 'Original Variables')
scores.food <- data.frame(scores.food)
boxplot(scores.food, las = 2, col = 'gold', main = 'Principal Components')
# Loadings
load.food <- pc.food$loadings
par(mar = c(1, 4, 0, 2), mfrow = c(4, 1))
for (i in 1:4) {
barplot(load.food[, i], ylim = c(-1, 1))
}
# Let's plot only the most significant loadings
par(mar = c(1, 4, 0, 2), mfrow = c(4, 1))
for (i in 1:4) {
barplot(ifelse(abs(load.food[, i]) < 0.3, 0, load.food[, i]),
ylim = c(-1, 1))
abline(h = 0)
}
# Scores
par(mfrow = c(1, 1))
plot(scores.food[, 1], scores.food[, 2], type = "n", xlab = "PC1", ylab = "PC2", asp = 1,
xlim = c(-4, 3))
text(scores.food[, 1], scores.food[, 2], dimnames(food)[[1]], cex = 0.7)
biplot(pc.food)
# Projection on the space generated by the first k principal components
par(mfrow = c(2, 5))
matplot(t(food.sd), type = 'l', main = 'Data', ylim = range(food.sd))
pc.athlete_stats$x
# Load necessary libraries
library(mvtnorm) # to deal with multivariate normal distributions
library(car) # "Companion to Applied Regression" for regression analysis
## Simulate data from a multivariate normal ---------------------------------------------------
mu  <-  c(1, 2) # mean
sig <-  cbind(c(1, 1), c(1, 4)) # covariance matrix
n   <-  100 # number of observations
set.seed(05032024) # Fix the random state to have reproducible results
X <- rmvnorm(n, mu, sig) # sample n observations from N(mu, sig)
# Plot the data
plot(X, asp = 1, xlab = 'Var 1', ylab = 'Var 2', pch = 19, xlim = c(-6, 6), ylim = c(-5, 10))
# Plot the sample mean
points(colMeans(X)[1], colMeans(X)[2], col = 'red', pch = 19, lwd = 3)
## Compute the variance along specific directions ---------------------------------------------
# Compute and plot variance of the projection of the data along the x-axis
abline(h = colMeans(X)[2], lty = 2, col = 'grey')
points(X[, 1], rep(colMeans(X)[2], n), col = 'red')
var(X[, 1])
# Compute and plot variance of the projection of the data along the y-axis
abline(v = colMeans(X)[1], lty = 2, col = 'grey')
points(rep(colMeans(X)[1], n), X[, 2], col = 'red')
var(X[, 2])
# Example: along the direction with angle pi / 6
theta <- pi / 6
# Plot the line corresponding to pi / 6
abline(a = colMeans(X)[2] - tan(theta) * colMeans(X)[1],
b = tan(theta), lty = 2)
# Compute unit vector a in direction theta[30]
a <- c(cos(theta), sin(theta))
# Project data onto direction defined by vector a
proj30 <- a %*% (t(X) - colMeans(X))
# Plot the projected points
points(colMeans(X)[1] + cos(theta) * proj30,
colMeans(X)[2] + sin(theta) * proj30, col = 'red')
# Compute variance along the direction
var(X %*% a)
## Compute variance along all directions ------------------------------------------------------
theta <- seq(0, pi, by = 2 * pi / 360)
Var <- NULL
for (i in 1:length(theta)) {
a <- c(cos(theta[i]), sin(theta[i]))  # unit vector in direction theta[i]
v <- var(X %*% a)  # sample variance of the projection of X along direction defined by vector a
Var <- c(Var, v)
}
# Compute min and max variance
max.var <- max(Var)
max.theta <- theta[which.max(Var)]
max.a <- c(cos(max.theta), sin(max.theta))
min.var <- min(Var)
min.theta <- theta[which.min(Var)]
min.a <- c(cos(min.theta), sin(min.theta))
# Graphical representation
par(mfrow = c(1, 2))
plot(X, asp = 1, xlab = 'Var 1', ylab = 'Var 2', pch = 20)
abline(a = colMeans(X)[2] - tan(max.theta) * colMeans(X)[1], b = tan(max.theta),
lty = 4, col = 'navyblue', lwd = 2)
abline(a = colMeans(X)[2] - tan(min.theta) * colMeans(X)[1], b = tan(min.theta),
lty = 4, col = 'blue', lwd = 2)
plot(theta, Var, type = 'l', col = 'dark grey', lwd = 2, ylab = 'Variance')
points(max.theta, max.var, pch = 16, col = 'navyblue')
points(min.theta, min.var, pch = 16, col = 'blue')
par(mfrow = c(1, 1))
## Confrontation with the theory --------------------------------------------------------------
# Compute sample covariance matrix
M <- colMeans(X)
S <- cov(X)
# Compute eigenvectors and eigenvalues
eigen(S) # eigen(S)$vectors returns a matrix whose columns are the eigenvectors of S
# Compare with empirically found values and directions of max/min variability
max.var
min.var
max.a
min.a
# Plot directions of max/min variability
par(mfrow = c(1, 2))
plot(X, asp = 1, xlab = 'Var 1', ylab = 'Var 2', pch = 20)
ellipse(M, S, 1, add = TRUE, lwd = 3, col = 'red')
abline(a = M[2] - eigen(S)$vectors[2, 1] / eigen(S)$vectors[1, 1] * M[1],
b = eigen(S)$vectors[2, 1] / eigen(S)$vectors[1, 1],
lty = 2, col = 'dark red', lwd = 2)
abline(a = M[2] - eigen(S)$vectors[2, 2] / eigen(S)$vectors[1, 2] * M[1],
b = eigen(S)$vectors[2, 2] / eigen(S)$vectors[1, 2],
lty = 2, col = 'red', lwd = 2)
abline(a = M[2] - tan(max.theta) * M[1], b = tan(max.theta),
lty = 4, col = 'navyblue', lwd = 2)
abline(a = M[2] - tan(min.theta) * M[1], b = tan(min.theta),
lty = 4, col = 'blue', lwd = 2)
plot(theta, Var, type = 'l', col = 'dark grey', lwd = 2, ylab = 'Variance')
points(max.theta, max.var, pch = 20, col = 'navyblue')
points(min.theta, min.var, pch = 20, col = 'blue')
points(atan(eigen(S)$vector[2, 1] / eigen(S)$vector[1, 1]), max.var, pch = 3, col = 'dark red')
points(atan(eigen(S)$vector[2, 2] / eigen(S)$vector[1, 2]) + pi, min.var, pch = 3, col = 'red')
par(mfrow = c(1, 1))
library(rgl)
options(rgl.printRglwidget = TRUE)
# theoretical mean and covariance matrix of the model
mu  <- c(0, 2, 3)
sig <- rbind(c(9, 1, 1), c(1, 4, 1), c(1, 1, 1))
nobs <- 100
X <- rmvnorm(nobs, mu, sig)
# sample mean and covariance matrix
M <- colMeans(X)
M
S <- cov(X)
S
points3d(X, asp=1, size=4)  # plot the points
axes3d()                    # add the axes
plot3d(ellipse3d(S, centre=M, level= 9/10), alpha=0.15, add=TRUE) # add the ellipsoid
# Principal components
PC <- princomp(X)
summary(PC)
# "0" principal component: the best approximation of dimension 0 (a point)
open3d()
points3d(X, asp=1, size=4)
axes3d()
points3d(t(M), col='red', size=6)
for(i in 1:100)
lines3d(rbind(X[i,], M))
## Space spanned by the first PC --------------------------------------------------------------
# I principal component: the best approximation of dimension 1 (a line)
open3d()
points3d(X, asp=1, size=4)
axes3d()
PC1 <- NULL
for(i in 1:nobs) {
PC1 <- rbind(PC1, PC$loadings[,1]*PC$scores[i,1] + M)
}
points3d(PC1, col='red', size=6)
for(i in 1:nobs) {
lines3d(rbind(X[i,], PC1[i,]), col='blue')
}
lines3d(rbind(M + 2*PC$sdev[1] * PC$loadings[,1], M - 2*PC$sdev[1] * PC$loadings[,1]),
col='forestgreen',lwd=2)
## Space spanned by the first two PCs ----------------------------------------------------------
# I and II principal components: the best approximation of dimension 2 (a plane)
open3d()
# Load necessary libraries
library(mvtnorm) # to deal with multivariate normal distributions
library(car) # "Companion to Applied Regression" for regression analysis
## Simulate data from a multivariate normal ---------------------------------------------------
mu  <-  c(1, 2) # mean
sig <-  cbind(c(1, 1), c(1, 4)) # covariance matrix
n   <-  100 # number of observations
set.seed(05032024) # Fix the random state to have reproducible results
X <- rmvnorm(n, mu, sig) # sample n observations from N(mu, sig)
# Plot the data
plot(X, asp = 1, xlab = 'Var 1', ylab = 'Var 2', pch = 19, xlim = c(-6, 6), ylim = c(-5, 10))
# Plot the sample mean
points(colMeans(X)[1], colMeans(X)[2], col = 'red', pch = 19, lwd = 3)
## Compute the variance along specific directions ---------------------------------------------
# Compute and plot variance of the projection of the data along the x-axis
abline(h = colMeans(X)[2], lty = 2, col = 'grey')
points(X[, 1], rep(colMeans(X)[2], n), col = 'red')
var(X[, 1])
# Compute and plot variance of the projection of the data along the y-axis
abline(v = colMeans(X)[1], lty = 2, col = 'grey')
points(rep(colMeans(X)[1], n), X[, 2], col = 'red')
var(X[, 2])
# Example: along the direction with angle pi / 6
theta <- pi / 6
# Plot the line corresponding to pi / 6
abline(a = colMeans(X)[2] - tan(theta) * colMeans(X)[1],
b = tan(theta), lty = 2)
# Compute unit vector a in direction theta[30]
a <- c(cos(theta), sin(theta))
# Project data onto direction defined by vector a
proj30 <- a %*% (t(X) - colMeans(X))
# Plot the projected points
points(colMeans(X)[1] + cos(theta) * proj30,
colMeans(X)[2] + sin(theta) * proj30, col = 'red')
# Compute variance along the direction
var(X %*% a)
## Compute variance along all directions ------------------------------------------------------
theta <- seq(0, pi, by = 2 * pi / 360)
Var <- NULL
for (i in 1:length(theta)) {
a <- c(cos(theta[i]), sin(theta[i]))  # unit vector in direction theta[i]
v <- var(X %*% a)  # sample variance of the projection of X along direction defined by vector a
Var <- c(Var, v)
}
# Compute min and max variance
max.var <- max(Var)
max.theta <- theta[which.max(Var)]
max.a <- c(cos(max.theta), sin(max.theta))
min.var <- min(Var)
min.theta <- theta[which.min(Var)]
min.a <- c(cos(min.theta), sin(min.theta))
# Graphical representation
par(mfrow = c(1, 2))
plot(X, asp = 1, xlab = 'Var 1', ylab = 'Var 2', pch = 20)
abline(a = colMeans(X)[2] - tan(max.theta) * colMeans(X)[1], b = tan(max.theta),
lty = 4, col = 'navyblue', lwd = 2)
abline(a = colMeans(X)[2] - tan(min.theta) * colMeans(X)[1], b = tan(min.theta),
lty = 4, col = 'blue', lwd = 2)
plot(theta, Var, type = 'l', col = 'dark grey', lwd = 2, ylab = 'Variance')
points(max.theta, max.var, pch = 16, col = 'navyblue')
points(min.theta, min.var, pch = 16, col = 'blue')
par(mfrow = c(1, 1))
## Confrontation with the theory --------------------------------------------------------------
# Compute sample covariance matrix
M <- colMeans(X)
S <- cov(X)
# Compute eigenvectors and eigenvalues
eigen(S) # eigen(S)$vectors returns a matrix whose columns are the eigenvectors of S
# Compare with empirically found values and directions of max/min variability
max.var
min.var
max.a
min.a
# Plot directions of max/min variability
par(mfrow = c(1, 2))
plot(X, asp = 1, xlab = 'Var 1', ylab = 'Var 2', pch = 20)
ellipse(M, S, 1, add = TRUE, lwd = 3, col = 'red')
abline(a = M[2] - eigen(S)$vectors[2, 1] / eigen(S)$vectors[1, 1] * M[1],
b = eigen(S)$vectors[2, 1] / eigen(S)$vectors[1, 1],
lty = 2, col = 'dark red', lwd = 2)
abline(a = M[2] - eigen(S)$vectors[2, 2] / eigen(S)$vectors[1, 2] * M[1],
b = eigen(S)$vectors[2, 2] / eigen(S)$vectors[1, 2],
lty = 2, col = 'red', lwd = 2)
abline(a = M[2] - tan(max.theta) * M[1], b = tan(max.theta),
lty = 4, col = 'navyblue', lwd = 2)
abline(a = M[2] - tan(min.theta) * M[1], b = tan(min.theta),
lty = 4, col = 'blue', lwd = 2)
plot(theta, Var, type = 'l', col = 'dark grey', lwd = 2, ylab = 'Variance')
points(max.theta, max.var, pch = 20, col = 'navyblue')
points(min.theta, min.var, pch = 20, col = 'blue')
points(atan(eigen(S)$vector[2, 1] / eigen(S)$vector[1, 1]), max.var, pch = 3, col = 'dark red')
points(atan(eigen(S)$vector[2, 2] / eigen(S)$vector[1, 2]) + pi, min.var, pch = 3, col = 'red')
par(mfrow = c(1, 1))
