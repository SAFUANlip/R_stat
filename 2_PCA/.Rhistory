boxplot(age, las=2, col='gold', main='Original variables')
scores.age <- data.frame(scores.age)
boxplot(scores.age, las=2, col='gold', main='Principal components')
load.age    <- pc.age$loadings
load.age
quartz()
par(mar = c(1,4,0,2), mfrow = c(3,1))
for(i in 1:3)barplot(load.age[,i], ylim = c(-1, 1))
quartz()
plot(scores.age[,1],scores.age[,2],type="n",xlab="pc1",ylab="pc2", asp=1)
text(scores.age[,1],scores.age[,2],dimnames(age)[[1]], cex=0.7)
biplot(pc.age)
# Projection on the space generated by the k-th principal component
x11(width=18, height=7)
par(mfrow=c(2,3))
#matplot(t(age), type='l', main = 'Data', ylim=range(age))
meanA <- colMeans(age)
matplot(meanA, type='l', main = '0 PC', lwd=2, ylim=range(age))
for(i in 1:5)
{
projection <- matrix(meanA, dim(age)[[1]], dim(age)[[2]], byrow=T) + scores.age[,i] %*% t(load.age[,i])
matplot(t(projection), type='l', main = paste(i, 'PC'), ylim=range(age))
matplot(meanA, type='l', lwd=2, add=T)
}
# Projection on the space generated by the first k principal components
x11(width=18, height=7)
par(mfrow=c(2,3))
#matplot(t(age), type='l', main = 'Data', ylim=range(age))
meanA <- colMeans(age)
matplot(meanA, type='l', main = '0 PC', lwd=2, ylim=range(age))
projection <- matrix(meanA, dim(age)[[1]], dim(age)[[2]], byrow=T)
for(i in 1:5)
{
projection <- projection + scores.age[,i] %*% t(load.age[,i])
matplot(t(projection), type='l', main = paste('First', i, 'PCs'), ylim=range(age))
matplot(meanA, type='l', lwd=2, add=T)
}
# Let's plot only the most significant loadings
par(mar = c(1, 4, 0, 2), mfrow = c(4, 1))
for (i in 1:4) {
barplot(ifelse(abs(load.scotland.sd[, i]) < 0.3, 0, load.scotland.sd[, i]),
ylim = c(-1, 1))
abline(h = 0)
}
# Scores
par(mfrow = c(1, 1))
plot(scores.scotland.sd[, 1], scores.scotland.sd[, 2], type = "n", xlab = "PC1", ylab = "PC2", asp = 1,
xlim = c(-4, 3))
text(scores.scotland.sd[, 1], scores.scotland.sd[, 2], dimnames(scotland)[[1]], cex = 0.7)
biplot(pc.scotland.sd)
scotland <- read.table('scotland.txt', header = TRUE)
n <- dim(scotland)[1]
p <- dim(scotland)[2]
# Exploration
boxplot(scotland, col = 'gold', main = 'Original Variables')
boxplot(scale(x = scotland, center = TRUE, scale = TRUE), col = 'gold')
scotland <- read.table('scotland.txt', header = TRUE)
n <- dim(scotland)[1]
p <- dim(scotland)[2]
# Exploration
boxplot(scotland, col = 'gold', main = 'Original Variables')
boxplot(scale(x = scotland, center = TRUE, scale = TRUE), col = 'gold')
scotland.sd <- scale(scotland,center=TRUE, scale=TRUE)
scotland.sd <- data.frame(scotland.sd)
boxplot(scotland.sd, col = 'gold', main = 'Standartised Variables')
sapply(scotland.sd, sd)
sapply(scotland.sd, mean)
pc.scotland.sd <- princomp(scotland.sd, scores=T)
pc.scotland.sd
summary(pc.scotland.sd)
# Explained variance
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
barplot(pc.scotland.sd$sdev^2, las = 2, main = 'Principal Components', ylim = c(0, 4), ylab = 'Variances')
abline(h = 1, col = 'blue')
barplot(sapply(scotland.sd, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 4),
ylab = 'Variances')
plot(cumsum(pc.scotland.sd$sdev^2) / sum(pc.scotland.sd$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of Components', ylab = 'Contribution to the Total Variance', ylim = c(0, 1))
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(scotland.sd), labels = 1:ncol(scotland.sd), las = 2)
# Scores
scores.scotland.sd <- pc.scotland.sd$scores
# Variability of the original variables / scores
layout(matrix(c(1, 2), 2))
boxplot(scotland.sd, las = 2, col = 'gold', main = 'Original Variables')
scores.scotland.sd <- data.frame(scores.scotland.sd)
boxplot(scores.scotland.sd, las = 2, col = 'gold', main = 'Principal Components')
# Loadings
load.scotland.sd <- pc.scotland.sd$loadings
par(mar = c(1, 4, 0, 2), mfrow = c(4, 1))
for (i in 1:4) {
barplot(load.scotland.sd[, i], ylim = c(-1, 1))
}
# Let's plot only the most significant loadings
par(mar = c(1, 4, 0, 2), mfrow = c(4, 1))
for (i in 1:4) {
barplot(ifelse(abs(load.scotland.sd[, i]) < 0.3, 0, load.scotland.sd[, i]),
ylim = c(-1, 1))
abline(h = 0)
}
# Scores
par(mfrow = c(1, 1))
plot(scores.scotland.sd[, 1], scores.scotland.sd[, 2], type = "n", xlab = "PC1", ylab = "PC2", asp = 1,
xlim = c(-4, 3))
text(scores.scotland.sd[, 1], scores.scotland.sd[, 2], dimnames(scotland)[[1]], cex = 0.7)
biplot(pc.scotland.sd)
View(scotland)
scotland == "Sutherland"
View(scotland)
scotland$"Sutherland"
scotland$Sutherland
scotland
rownames(scotland)
scotland[rownames(scotland) == "Sutherland"]
rownames(scotland)
rownames(scotland) == "Sutherland"
scotland[rownames(scotland) == "Sutherland", ]
scotland[rownames(scotland) == "Ayrshire", ]
NO <- read.table("NO.txt", header = TRUE)
View(NO)
View(NO)
n <- dim(NO)[1]
p <- dim(NO)[2]
# Exploration
boxplot(NO, col = 'gold', main = 'Original Variables')
boxplot(scale(x = scotland, center = TRUE, scale = TRUE), col = 'gold')
boxplot(scale(x = NO, center = TRUE, scale = TRUE), col = 'gold')
NO.sd <- scale(NO,center=TRUE, scale=TRUE)
NO.sd <- data.frame(NO.sd)
boxplot(scale(x = NO, center = TRUE, scale = FALSE), col = 'gold')
# Exploration
boxplot(NO, col = 'gold', main = 'Original Variables')
scotland <- read.table('scotland.txt', header = TRUE)
n <- dim(scotland)[1]
p <- dim(scotland)[2]
# Exploration
boxplot(scotland, col = 'gold', main = 'Original Variables')
boxplot(scale(x = scotland, center = TRUE, scale = TRUE), col = 'gold')
scotland.sd <- scale(scotland,center=TRUE, scale=TRUE)
scotland.sd <- data.frame(scotland.sd)
boxplot(scotland.sd, col = 'gold', main = 'Standartised Variables')
sapply(scotland.sd, sd)
sapply(scotland.sd, mean)
NO <- read.table("NO.txt", header = TRUE)
n <- dim(NO)[1]
p <- dim(NO)[2]
# Exploration
boxplot(NO, col = 'gold', main = 'Original Variables')
boxplot(scale(x = NO, center = TRUE, scale = FALSE), col = 'gold')
NO.sd <- scale(NO,center=TRUE, scale=TRUE)
NO <- read.table("NO.txt", header = TRUE)
n <- dim(NO)[1]
p <- dim(NO)[2]
# Exploration
boxplot(NO, col = 'gold', main = 'Original Variables')
boxplot(scale(x = NO, center = TRUE, scale = FALSE), col = 'gold')
NO.sd <- scale(NO,center=TRUE, scale=FALSE)
NO.sd <- data.frame(NO.sd)
boxplot(NO.sd, col = 'gold', main = 'Standartised Variables')
NO <- read.table("NO.txt", header = TRUE)
n <- dim(NO)[1]
p <- dim(NO)[2]
# Exploration
boxplot(NO, col = 'gold', main = 'Original Variables')
boxplot(scale(x = NO, center = TRUE, scale = FALSE), col = 'gold')
NO.sd <- scale(NO,center=TRUE, scale=FALSE)
NO.sd <- data.frame(NO.sd)
boxplot(NO.sd, col = 'gold', main = 'Standartised Variables')
pc.NO.sd <- princomp(NO.sd, scores=T)
pc.NO <- princomp(NO, scores=T)
summary(pc.NO)
summary(pc.NO.sd)
pc.NO$scores
pc.NO$loadings
pc.NO$loadings[1]
pc.NO$loadings[1,]
new_obs <- c(13, 10, 11, 13)
pc.NO$loadings[1,]
pc.NO.sd$loadings[1,]
View(pc.NO)
View(pc.NO.sd)
new_obs %*% pc.NO$loadings[1,]
t(new_obs) %*% pc.NO$loadings[1,]
pc.NO$loadings
pc.NO$loadings[,]
pc.NO.sd$loadings[,]
as.matrix(new_obs) %*% as.matrix(pc.NO$loadings)
as.matrix(pc.NO$loadings)
as.matrix(pc.NO$loadings
as.matrix(pc.NO$loadings)
as.matrix(new_obs) %*% as.matrix(pc.NO$loadings)
as.matrix(new_obs)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
predict(pc.NO, new_obs)
predict(pc.NO, as.matrix(new_obs))
predict(pc.NO, as.matrix(new_obs, dimnames=colnames(NO)))
as.matrix(new_obs, dimnames=colnames(NO))
colnames(NO)
predict(pc.NO, as.matrix(new_obs, dimnames=c(colnames(NO))))
pc.NO
pc.NO$loadings
new_obs_df <- as.data.frame(t(new_obs))
colnames(new_obs_df) <- colnames(NO)
predict(pc.NO, new_obs_df)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
predict(pc.NO.sd, new_obs_df)
t(as.matrix(new_obs_df)) %*% as.matrix(pc.NO$loadings)
t(as.matrix(new_obs_df)) %*% as.matrix(pc.NO.sd$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
predict(pc.NO, new_obs_df)
predict(pc.NO.sd, new_obs_df)
colMeans(N)
colMeans(N0)
colMeans(NO)
predict(pc.NO.sd, new_obs_df-colMeans(NO))
t(as.matrix(new_obs)) %*% as.matrix(pc.NO$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
predict(pc.NO, new_obs_df)
predict(pc.NO.sd, new_obs_df-colMeans(NO))
View(pc.NO.sd)
View(pc.NO)
summary(pc.NO)
summary(pc.NO.sd)
pc.NO.sd
loadings.pc.NO.sd <- pc.NO.sd$loadings
scores.pc.NO.sd <- pc.NO.sd$scores
scores.pc.NO.sd
# Explained variance
layout(matrix(c(2, 3, 1, 3), 2, byrow = TRUE))
barplot(pc.NO.sd$sdev^2, las = 2, main = 'Principal Components', ylim = c(0, 4), ylab = 'Variances')
abline(h = 1, col = 'blue')
barplot(sapply(NO.sd, sd)^2, las = 2, main = 'Original Variables', ylim = c(0, 4),
ylab = 'Variances')
plot(cumsum(pc.NO.sd$sdev^2) / sum(pc.NO.sd$sde^2), type = 'b', axes = FALSE,
xlab = 'Number of Components', ylab = 'Contribution to the Total Variance', ylim = c(0, 1))
box()
axis(2, at = 0:10/10, labels = 0:10/10)
axis(1, at = 1:ncol(NO.sd), labels = 1:ncol(NO.sd), las = 2)
# d)
new_obs <- c(13, 10, 11, 13)
new_obs_df <- as.data.frame(t(new_obs))
abline(0.8)
abline(h = 0, v = 0, lty = 2, col = 'grey')
abline(h = 0.8, v = 0, lty = 2, col = 'red')
# b)
variances.pc.sd <- pc.NO.sd$sdev^2
variances.pc.sd
# Scores
# Variability of the original variables / scores
layout(matrix(c(1, 2), 2))
boxplot(NO.sd, las = 2, col = 'gold', main = 'Original Variables')
scores.pc.NO.sd <- data.frame(scores.pc.NO.sd)
boxplot(scores.pc.NO.sd, las = 2, col = 'gold', main = 'Principal Components')
par(mar = c(1, 4, 0, 2), mfrow = c(4, 1))
for (i in 1:4) {
barplot(loadings.pc.NO.sd[, i], ylim = c(-1, 1))
}
# d) prediction
new_obs <- c(13, 10, 11, 13)
predict(pc.NO.sd, new_obs_df-colMeans(NO))
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO$loadings)
predict(pc.NO, new_obs_df)
scores.pc.NO.sd
t(as.matrix(new_obs)) %*% as.matrix(pc.NO.sd$loadings)
t(as.matrix(new_obs)) %*% as.matrix(pc.NO$loadings)
# Let's generate a sample of size 100 from a Gaussian
set.seed(14032022)
mu  <-  c(1,2)
sig <-  cbind(c(1,1), c(1,4))
n   <-  100
X <- rmvnorm(n, mu, sig)
M <- colMeans(X)
S <- cov(X)
# Let's generate a sample of size 100 from a Gaussian
set.seed(14032022)
mu  <-  c(1,2)
sig <-  cbind(c(1,1), c(1,4))
n   <-  100
X <- rmvnorm(n, mu, sig)
install.packages("rmvnorm")
X <- rmvnorm(n, mu, sig)
# Load necessary libraries
library(mvtnorm) # to deal with multivariate normal distributions
library(car) # "Companion to Applied Regression" for regression analysis
# Let's generate a sample of size 100 from a Gaussian
set.seed(14032022)
mu  <-  c(1,2)
sig <-  cbind(c(1,1), c(1,4))
n   <-  100
X <- rmvnorm(n, mu, sig)
M <- colMeans(X)
S <- cov(X)
x11(width=14, height=7)
par(mfrow=c(1,3))
plot(X, asp=1, xlab='Var 1', ylab='Var 2',pch=1)
ellipse(M, S, 1, add=T,lwd=3, col = 'red')
ellipse(mu, sig, 1, add=T,lwd=3, col='springgreen')
abline(a = M[2] - eigen(S)$vectors[2,1]/eigen(S)$vectors[1,1]*M[1], b = eigen(S)$vectors[2,1]/eigen(S)$vectors[1,1], lty = 2, col = 'red', lwd = 2)
abline(a = M[2] - eigen(S)$vectors[2,2]/eigen(S)$vectors[1,2]*M[1], b = eigen(S)$vectors[2,2]/eigen(S)$vectors[1,2], lty = 2, col = 'red', lwd = 2)
abline(a = mu[2] - eigen(sig)$vectors[2,1]/eigen(sig)$vectors[1,1]*mu[1], b = eigen(sig)$vectors[2,1]/eigen(sig)$vectors[1,1], lty = 2, col = 'springgreen', lwd = 2)
abline(a = mu[2] - eigen(sig)$vectors[2,2]/eigen(sig)$vectors[1,2]*mu[1], b = eigen(sig)$vectors[2,2]/eigen(sig)$vectors[1,2], lty = 2, col = 'springgreen', lwd = 2)
legend('topleft',c('True','Estimated'),col=c('springgreen','red'),lty=c(1,1),lwd=2)
### let's now increase n...
n <- 1000
X <- rmvnorm(n, mu, sig)
M <- colMeans(X)
S <- cov(X)
plot(X, asp=1, xlab='Var 1', ylab='Var 2',pch=1)
ellipse(M, S, 1, add=T,lwd=3, col = 'red')
ellipse(mu, sig, 1, add=T,lwd=3, col='springgreen')
abline(a = M[2] - eigen(S)$vectors[2,1]/eigen(S)$vectors[1,1]*M[1], b = eigen(S)$vectors[2,1]/eigen(S)$vectors[1,1], lty = 2, col = 'red', lwd = 2)
abline(a = M[2] - eigen(S)$vectors[2,2]/eigen(S)$vectors[1,2]*M[1], b = eigen(S)$vectors[2,2]/eigen(S)$vectors[1,2], lty = 2, col = 'red', lwd = 2)
abline(a = mu[2] - eigen(sig)$vectors[2,1]/eigen(sig)$vectors[1,1]*mu[1], b = eigen(sig)$vectors[2,1]/eigen(sig)$vectors[1,1], lty = 2, col = 'springgreen', lwd = 2)
abline(a = mu[2] - eigen(sig)$vectors[2,2]/eigen(sig)$vectors[1,2]*mu[1], b = eigen(sig)$vectors[2,2]/eigen(sig)$vectors[1,2], lty = 2, col = 'springgreen', lwd = 2)
legend('topleft',c('True','Estimated'),col=c('springgreen','red'),lty=c(1,1),lwd=2)
###
n <- 5000
X <- rmvnorm(n, mu, sig)
M <- colMeans(X)
S <- cov(X)
plot(X, asp=1, xlab='Var 1', ylab='Var 2',pch=1)
ellipse(M, S, 1, add=T,lwd=3, col = 'red')
ellipse(mu, sig, 1, add=T,lwd=3, col='springgreen')
abline(a = M[2] - eigen(S)$vectors[2,1]/eigen(S)$vectors[1,1]*M[1], b = eigen(S)$vectors[2,1]/eigen(S)$vectors[1,1], lty = 2, col = 'red', lwd = 2)
abline(a = M[2] - eigen(S)$vectors[2,2]/eigen(S)$vectors[1,2]*M[1], b = eigen(S)$vectors[2,2]/eigen(S)$vectors[1,2], lty = 2, col = 'red', lwd = 2)
abline(a = mu[2] - eigen(sig)$vectors[2,1]/eigen(sig)$vectors[1,1]*mu[1], b = eigen(sig)$vectors[2,1]/eigen(sig)$vectors[1,1], lty = 2, col = 'springgreen', lwd = 2)
abline(a = mu[2] - eigen(sig)$vectors[2,2]/eigen(sig)$vectors[1,2]*mu[1], b = eigen(sig)$vectors[2,2]/eigen(sig)$vectors[1,2], lty = 2, col = 'springgreen', lwd = 2)
legend('topleft',c('True','Estimated'),col=c('springgreen','red'),lty=c(1,1),lwd=2)
S <- cov(NO)
plot(S)
image(S, asp=1)
image(S, asp=1)
var.tot <- sum( diag(S) )
var.gen <- det(S)
S
S <- cov(NO)
heatmap(S, main = "Covariance Matrix Heatmap")
heatmap.2(S,
trace = "none",
col = heat.colors(100),
key = TRUE,            # ← это включает легенду
main = "Covariance Matrix with Color Key",
density.info = "none")
library(gplots) # pheatmat
image(S, col=heat.colors(100),main='Cov. S1', asp=1, axes = FALSE, breaks = quantile(rbind(S1,S2,S3), (0:100)/100, na.rm=TRUE))
image(S, col=heat.colors(100),main='Cov. S', asp=1, axes = FALSE, (0:100)/100, na.rm=TRUE))
image(S, col=heat.colors(100),main='Cov. S', asp=1, axes = FALSE, (0:100)/100, na.rm=TRUE)
image(S, col=heat.colors(100),main='Cov. S', asp=1, axes = FALSE, na.rm=TRUE)
pheatmap(
mat = S,
main = "Covariance Matrix",
color = my_palette,
breaks = breaks,
cluster_rows = FALSE,
cluster_cols = FALSE,
display_numbers = TRUE,
fontsize_number = 8,
border_color = NA
)
library(pheatmap) # pheatmap
pheatmap(
mat = S,
main = "Covariance Matrix",
color = my_palette,
breaks = breaks,
cluster_rows = FALSE,
cluster_cols = FALSE,
display_numbers = TRUE,
fontsize_number = 8,
border_color = NA
)
pheatmap(
mat = S,
main = "Covariance Matrix",
color = my_palette,
cluster_rows = FALSE,
cluster_cols = FALSE,
display_numbers = TRUE,
fontsize_number = 8,
border_color = NA
)
pheatmap(
mat = S,
main = "Covariance Matrix",
cluster_rows = FALSE,
cluster_cols = FALSE,
display_numbers = TRUE,
fontsize_number = 8,
border_color = NA
)
NO <- read.table("NO.txt", header = TRUE)
n <- dim(NO)[1]
p <- dim(NO)[2]
var.gen <- det(S)
View(NO)
S <- cov(NO)
boxplot(NO, col = 'gold', main = 'Original Variables')
S <- cov(NO)
S <- cov(NO)
var.gen <- det(S)
var.tot <- sum( diag(S) )
S
pheatmap(
mat = S,
main = "Covariance Matrix",
cluster_rows = FALSE,
cluster_cols = FALSE,
display_numbers = TRUE,
fontsize_number = 8,
border_color = NA
)
# Exploration
boxplot(NO, col = 'gold', main = 'Original Variables')
boxplot(scale(x = NO, center = TRUE, scale = FALSE), col = 'gold')
NO.sd <- scale(NO,center=TRUE, scale=FALSE)
NO.sd <- data.frame(NO.sd)
boxplot(NO.sd, col = 'gold', main = 'Standartised Variables')
pc.NO.sd <- princomp(NO.sd, scores=T)
pc.NO <- princomp(NO, scores=T)
summary(pc.NO)
summary(pc.NO.sd)
# b)
variances.pc.sd <- pc.NO.sd$sdev^2
variances.pc.sd
# Check Total and Generalised Variance after PCA -------------------------------
# Загружаем данные
data(iris)
# Check Total and Generalised Variance after PCA -------------------------------
# Загружаем данные
data(iris)
X <- iris[, 1:4]
# Стандартизация данных
X_scaled <- scale(X)
boxplot(X, col = 'gold', main = 'Original Variables')
# Стандартизация данных
X_scaled <- scale(X)
boxplot(X_sclaed, col = 'gold', main = 'Standartised Variabels')
boxplot(X_scaled, col = 'gold', main = 'Standartised Variabels')
cov(X_scaled)
# --- Исходная ковариационная матрица ---
cov_orig <- cov(X_scaled)
# --- Total Variance (сумма дисперсий) ---
total_var_orig <- sum(diag(cov_orig))
cat("Total variance (original):", total_var_orig, "\n")
# --- Generalized Variance (определитель) ---
gen_var_orig <- det(cov_orig)
cat("Generalized variance (original):", gen_var_orig, "\n")
# --- PCA через princomp ---
pca_model <- princomp(X_scaled, cor = FALSE, scores = TRUE)
# Преобразованные данные
X_pca <- pca_model$scores
# --- Ковариационная матрица после PCA ---
cov_pca <- cov(X_pca)
# --- Total Variance после PCA ---
total_var_pca <- sum(diag(cov_pca))
cat("Total variance (PCA):", total_var_pca, "\n")
cov_pca
# --- Total Variance после PCA ---
total_var_pca <- sum(diag(cov_pca))
cat("Total variance (PCA):", total_var_pca, "\n")
# --- Generalized Variance после PCA ---
gen_var_pca <- det(cov_pca)
cat("Generalized variance (PCA):", gen_var_pca, "\n")
# what if cetralise
# PCA без стандартизации
p1 <- princomp(X, cor = FALSE)
# PCA со стандартизацией
p2 <- princomp(X, cor = TRUE)
# Сравним долю объяснённой дисперсии
summary(p1)
summary(p2)
X_centred <- scale(X,scale = FALSE, center = TRUE)
boxplot(X_centred, col = 'gold', main = 'Centered Variabels')
p3 <- princomp(X_centred, cor = FALSE)
# Сравним долю объяснённой дисперсии
summary(p1)
summary(p2)
summary(p3)
# Check Total and Generalised Variance after PCA -------------------------------
# Загружаем данные
data(iris)
X <- iris[, 1:4]
boxplot(X, col = 'gold', main = 'Original Variables')
# Стандартизация данных
X_scaled <- scale(X)
cov(X_scaled)
boxplot(X_scaled, col = 'gold', main = 'Standartised Variabels')
# --- Исходная ковариационная матрица ---
cov_orig <- cov(X_scaled)
# --- Total Variance (сумма дисперсий) ---
total_var_orig <- sum(diag(cov_orig))
cat("Total variance (original):", total_var_orig, "\n")
cov_orig
cat("Total variance (original):", total_var_orig, "\n")
# --- Generalized Variance (определитель) ---
gen_var_orig <- det(cov_orig)
cat("Generalized variance (original):", gen_var_orig, "\n")
# --- PCA через princomp ---
pca_model <- princomp(X_scaled, cor = FALSE, scores = TRUE)
# Преобразованные данные
X_pca <- pca_model$scores
# --- Ковариационная матрица после PCA ---
cov_pca <- cov(X_pca)
cov_pca
# --- Total Variance после PCA ---
total_var_pca <- sum(diag(cov_pca))
cat("Total variance (PCA):", total_var_pca, "\n")
