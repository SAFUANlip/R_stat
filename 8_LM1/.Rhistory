bG_full <- c(0, 0, 0, 0)
linearHypothesis(fit_pb4_full, AG_full, bG_full) # т.е мы проверили что все пары коэффициентов одинаковы
AGQ <- rbind(
c(0,0,0,0,1,0),
c(0,0,0,0,0,1))
bGQ <- c(0,0)
linearHypothesis(fit_pb4, AGQ, bGQ)
#     3. the effect of the variable G onto the coefficient that multiplies
#        the regressor x^2;
linearHypothesis(fit,
rbind(c(0,0,0,0,1,0),
c(0,0,0,0,0,1)),
c(0,0))
AGQ <- rbind(
c(0,0,0,1,-1,0),
c(0,0,0,1,0,-1))
AGQ <- rbind(
c(0,0,0,0,1,0),
c(0,0,0,0,0,1))
bGQ <- c(0,0)
linearHypothesis(fit_pb4, AGQ, bGQ)
AGQ_full <- rbind(
c(0,0,0,1,-1,0),
c(0,0,0,1,0,-1))
bGQ_full <- c(0,0)
linearHypothesis(fit_pb4_full, AGQ_full, bGQ_full)
AGI <- rbind(
c(0,1,0,0,0,0),
c(0,0,1,0,0,0))
bGI <- c(0,0)
linearHypothesis(fit_pb4, AGI, bGI)
AGI <- rbind(
c(0,1,0,0,0,0),
c(0,0,1,0,0,0))
bGI <- c(0,0)
linearHypothesis(fit_pb4, AGI, bGI)
AGI_full <- rbind(
c(1,-1,0,0,0,0),
c(1,0,-1,0,0,0))
bGI_full <- c(0,0)
linearHypothesis(fit_pb4_full, AGI_full, bGI_full)
#     4. the effect of the variable G on the intercept.
linearHypothesis(fit,
rbind(c(0,1,0,0,0,0),
c(0,0,1,0,0,0)),
c(0,0))
# c) so, if no effect of group on intercept =>
fit_pb4_best <- lm(y ~ I(x^2) + I(x^2):dummy_France + I(x^2):dummy_Germany, data=pb4new)
summary(fit_pb4_best)
new_data <- data.frame(x = c(11,11,11)^2, dummy_France=c(1,0,0), dummy_Germany=c(0,1,0))
predict(fit_pb4_best, new_data, interval="prediction")
new_data <- data.frame(x = c(11,11,11), dummy_France=c(1,0,0), dummy_Germany=c(0,1,0))
predict(fit_pb4_best, new_data, interval="prediction")
new <- data.frame(speed1 = 10, speed2 = 100)
predict(fm, new, interval="confidence")  # доверительный интервал для E[Y | X]
### question (c)
fit2 <- lm(Reg ~ Year2 + Year2:dFr + Year2:dGer, data=dati)
summary(fit2)
new_data <- data.frame(Year2 = c(11,11,11)^2, dFr=c(1,0,0), dGer=c(0,1,0))
IP <- predict(fit2, newdata=new_data, interval='prediction', level=1-0.05/3)
rownames(IP) <- c('Fr','Ger','It')
IP
fit_pb4_best_full <- lm(y ~ -1 + I(x^2):dummy_France + I(x^2):dummy_Germany + I(x^2):dummy_Italy, data=pb4new_full)
summary(fit_pb4_best_full)
summary(fit_pb4_best)
new_data_full <- data.frame(x = c(11,11,11), dummy_France=c(1,0,0), dummy_Germany=c(0,1,0), dummy_Italy=c(0,0,1)) # so it automatically make x^2
predict(fit_pb4_best, new_data_full, interval="prediction")
people <- read.table("people.txt", h=TRUE)
View(people)
pi
fit_people <- lm(mese ~ rifiuti + I(1-cos(2*pi/12*rifiuti)), data=people)
summary(fit_people)
plot(people)
fit_people$coefficients
summary(fit_people)$coefficients
summary(fit_people)$coefficients$Estimate
summary(fit_people)
fit_people$coefficients
t <- c(1:30)
fit_people$terms
fit_people$effects
fitted(fit_people)
plot(fitted(fit_people))
plot(fitted(fit_people), color='blue')
t <- c(1:30)
t <- c(1:30)
fit_people$coefficients[1]
pred <- c(fit_people$coefficients[1] + t)
pred <- c(fit_people$coefficients[1] + t + (1-cos(2*pi/12*t)))
plot(pred)
t <- c(1:30:1)
t <- c(1:30)
pred <- c(fit_people$coefficients[1] + t + (1-cos(2*pi/12*t)))
plot(pred)
# b)
plot(fit_people$residuals)
# b)
plot(fit_people)
# b)
plot(fit_people)
plot(fit_people)
plot(fit_people)
# b)
par(mfrow=c(2,2))
plot(fit_people)
View(people)
# but question was about residents (not residuals)
# lets check that betta1 != 0
A_residents <- rbind(c(0,1,0))
linearHypothesis(fit_people, A_residents, b_residents)
b_residents <- c(0)
linearHypothesis(fit_people, A_residents, b_residents)
summary(fit_people)
View(people)
plot(people)
# c) effect of tourists maybe noticet if we look on preiodic part of our model
A_tourists <- rbind(c(0,0,1))
b_tourists <- c(0)
linearHypothesis(fit_people, A_tourists, b_tourists)
# d) increase by 10 tons each month => betta1 = 10?
A_10 <- rbind(c(0,1,0)) # H0: betta1 = 10, H1: betta1 != 10
b_10 <- c(10)
linearHypothesis(fit_people, A_10, b_10)
people <- read.table('people.txt', header=T)
people
plot(people,pch=20)
attach(people)
### question a)
fit <- lm(rifiuti ~ mese + I(1 - cos(2*pi/12*mese)))
summary(fit)
t <- seq(from=0,to=30,length=100)
points(t,fit$coeff[1]+fit$coeff[2]*t+fit$coeff[3]*(1-cos(2*pi/12*t)),type='l')
### question b)
par(mfrow=c(2,2))
plot(fit)
shapiro.test(residuals(fit))
dev.off()
# Test: H0: beta_1==0 vs beta_1!=0
summary(fit)
## or
linearHypothesis(fit,rbind(c(0,1,0)),0)
### question c)
# Test: H0: beta_2==0 vs beta_2!=0
summary(fit)
## or
linearHypothesis(fit,rbind(c(0,0,1)),0)
### question d)
linearHypothesis(fit,rbind(c(0,1,0)),10)
# or (from the summary)
summary(fit)
t <- (coef(fit)[2]-10)/sqrt(diag(vcov(fit))[2])
t
pval <- 2*(1-pt(t,29-(2+1)))
pval
### question d)
linearHypothesis(fit,rbind(c(0,1,0)),10)
# d) increase by 10 tons each month => betta1 = 10?
A_10 <- rbind(c(0,1,0)) # H0: betta1 = 10, H1: betta1 != 10
b_10 <- c(10)
linearHypothesis(fit_people, A_10, b_10) # so we can reject H0, and
fit_people <- lm(mese ~ rifiuti + I(1-cos(2*pi/12*rifiuti)), data=people)
# d) increase by 10 tons each month => betta1 = 10?
A_10 <- rbind(c(0,1,0)) # H0: betta1 = 10, H1: betta1 != 10
b_10 <- c(10)
linearHypothesis(fit_people, A_10, b_10) # so we can reject H0, and
### question d)
linearHypothesis(fit,rbind(c(0,1,0)),10)
View(people)
View(people)
fit_people <- lm(mese ~ mese + I(1-cos(2*pi/12*mese)), data=people)
fit_people <- lm(rifiuti ~ mese + I(1-cos(2*pi/12*mese)), data=people)
summary(fit_people)
# a)
fit_people$coefficients
t <- c(1:30)
pred <- c(fit_people$coefficients[1] + t + (1-cos(2*pi/12*t)))
plot(pred)
# b)
par(mfrow=c(2,2))
plot(fit_people) # I don't see, that residuals increase among time
# but question was about residents (not residuals)
# lets check that betta1 != 0
A_residents <- rbind(c(0,1,0))
b_residents <- c(0)
linearHypothesis(fit_people, A_residents, b_residents)
summary(fit_people)
# c) effect of tourists maybe noticet if we look on preiodic part of our model
A_tourists <- rbind(c(0,0,1))
b_tourists <- c(0)
linearHypothesis(fit_people, A_tourists, b_tourists)
# d) increase by 10 tons each month => betta1 = 10?
A_10 <- rbind(c(0,1,0)) # H0: betta1 = 10, H1: betta1 != 10
b_10 <- c(10)
linearHypothesis(fit_people, A_10, b_10) # so we can reject H0, and
summary(fit_people)
fit_people_e <- lm(rifiuti ~ I(10*mese) + I(1-cos(2*pi/12*mese)), data=people)
summary(fit_people_e)
### question e)
rifiuti.vinc <- rifiuti - 10*mese
fit2 <- lm(rifiuti.vinc ~ I(1 - cos(2*pi/12*mese)))
summary(fit2)
View(people)
C <- rbind(c(1,(1 - cos(2*pi/12*30))),   # total waste in June 2011 [mese=30]
c(1,0),                       # waste due to residents in June 2011
c(0,(1 - cos(2*pi/12*30))))   # waste due to tourists in June 2011
C
pred <- C %*% coefficients(fit2) + c(10*30, 10*30, 0)
# pred=C%*%beta.hat[fit.mod.constrained] + 10*mese[constrained part]
pred
plot(people, xlim=c(1,30), ylim=c(900,1400))
lines(mese, fitted(fit))
lines(mese, fitted(fit2) + 10*mese, col='blue')
points(c(30,30,30), pred, pch=16)
legend('bottomright',c('Model 1', 'Constrained model'), lty=1, col=c('black','blue'))
dev.off()
# pred=C%*%beta.hat[fit.mod.constrained] + 10*mese[constrained part]
pred
plot(people, xlim=c(1,30), ylim=c(900,1400))
lines(mese, fitted(fit))
lines(mese, fitted(fit2) + 10*mese, col='blue')
points(c(30,30,30), pred, pch=16)
summary(fit_people_e)
# by default will be collected
fit_people_e$coefficients[1]
# by residents
fit_people_e$coefficients[2] * 30*10
# by tourists
fit_people_e$coefficients[3] * (1-cos(2*pi/12*30))
# pred=C%*%beta.hat[fit.mod.constrained] + 10*mese[constrained part]
pred
# correct point e)
people$t_residents <- 10 * people$mese
fit_people_e <- lm(rifiuti ~ t_residents + I(1 - cos(2 * pi / 12 * mese)), data = people)
summary(fit_people_e)
t_pred <- 30
# correct point f)
t_pred <- 30
A_hat <- fit_people_e$coefficients[1]
C_hat <- fit_people_e$coefficients[3]
seasonal_component <- 1 - cos(2 * pi / 12 * t_pred)
# f1. Total waste forecast
Y_total <- A_hat + 10 * t_pred + C_hat * seasonal_component
# f2. Waste from residents
Y_residents <- 10 * t_pred
# f3. Waste from tourists
Y_tourists <- C_hat * seasonal_component
# Print all
cat("Total waste (June 2011):", Y_total, "\n")
cat("Residents' contribution:", Y_residents, "\n")
cat("Tourists' contribution:", Y_tourists, "\n")
fit_people_e <- lm(rifiuti ~ I(10*mese) + I(1-cos(2*pi/12*mese)), data=people)
summary(fit_people_e)
goat <- read.table('goat.txt', h=TRUE)
View(goat)
View(goat)
C <- length(goat)
C <- dim(goat)[1]
View(goat)
goat$gender
goat$gender == "female"
int(goat$gender == "female")
numeric(goat$gender == "female")
print(i)
for(i in 1:C){
print(i)
}
A <- repeat(0,C)
A <- repeat(c(0),C)
A <- repeat(c(0),c(C))
A <- rep(c(0),c(C))
B <- rep(c(0),c(C))
A <- rep(c(0),c(C))
B <- rep(c(0),c(C))
for(i in 1:C){
if (goat$gender[i] == "female"){
A[i] = 1
B[i] = 1
}
}
View(goat)
View(goat)
fit <- lm(height ~ A + B:(1 - exp(-age)) + I(C), data=goat)
fit <- lm(height ~ A + B:(1 - exp(-age)), data=goat)
summary(fit)
fit <- lm(height ~ gender + I(1 - exp(-age)) + I(1 - exp(-age)):gender, data=goat)
summary(fit)
summary(fit)
fit$coefficients
View(goat)
View(goat)
inf
infinity
Inf
# d) impement reduced model (we can ignore gender, due to it don't reject H0 of zero coefficient)
fit_reduced <- lm(height ~I(1 - exp(-age)) + I(1 - exp(-age)):gender, data=goat)
summary(fit_reduced)
# b) when t = 0, we have height ~ gender + eps, so we have to check if there are. significance
# of this coefficent
A0 <- rbind(
c(0,1,0,0)
)
b0 <- c(0)
linearHypothesis(fit, A0, b0)
# a)
fit <- lm(height ~ gender + I(1 - exp(-age)) + I(1 - exp(-age)):gender, data=goat)
summary(fit)
# b)
library(car)
linearHypothesis(fit,rbind(c(0,1,0,0)),0)
# я не понял как можно было догодаться до этой модели (может по числу параметров...)
fit <- lm(height ~ gender + I(1 - exp(-age)) + I(1 - exp(-age)):gender, data=goat)
summary(fit)
# a)
fit$coefficients
# b) when t = 0, we have height ~ gender + eps, so we have to check if there are. significance
# of this coefficent
A0 <- rbind(
c(0,1,0,0)
)
b0 <- c(0)
linearHypothesis(fit, A0, b0)
# c) I(1 - exp(-age)) - goes to one, I(1 - exp(-age)):gender - will depend on gender, so we have to check this and gender coefficeinte
Ainf <- rbind(
c(0,1,0,1)
)
binf <- c(0)
linearHypothesis(fit, Ainf, binf)
1
# d)
fit.red <- lm(height ~ I(1 - exp(-age)) + I(1 - exp(-age)):gender, data=goat)
summary(fit.red)
# e)
C <- rbind(c(1,0,0), c(1,1,0), c(1,1,1))
Bf <- rbind(
c((C %*% coefficients(fit.red))[1] - sqrt((C %*% vcov(fit.red) %*% t(C))[1,1]) * qt(1 - 0.10/6, n-3),
(C %*% coefficients(fit.red))[1] + sqrt((C %*% vcov(fit.red) %*% t(C))[1,1]) * qt(1 - 0.10/6, n-3)),
c((C %*% coefficients(fit.red))[2] - sqrt((C %*% vcov(fit.red) %*% t(C))[2,2]) * qt(1 - 0.10/6, n-3),
(C %*% coefficients(fit.red))[2] + sqrt((C %*% vcov(fit.red) %*% t(C))[2,2]) * qt(1 - 0.10/6, n-3)),
c((C %*% coefficients(fit.red))[3] - sqrt((C %*% vcov(fit.red) %*% t(C))[3,3]) * qt(1 - 0.10/6, n-3),
(C %*% coefficients(fit.red))[3] + sqrt((C %*% vcov(fit.red) %*% t(C))[3,3]) * qt(1 - 0.10/6, n-3))
)
n <- dim(goat)[1]
Bf <- rbind(
c((C %*% coefficients(fit.red))[1] - sqrt((C %*% vcov(fit.red) %*% t(C))[1,1]) * qt(1 - 0.10/6, n-3),
(C %*% coefficients(fit.red))[1] + sqrt((C %*% vcov(fit.red) %*% t(C))[1,1]) * qt(1 - 0.10/6, n-3)),
c((C %*% coefficients(fit.red))[2] - sqrt((C %*% vcov(fit.red) %*% t(C))[2,2]) * qt(1 - 0.10/6, n-3),
(C %*% coefficients(fit.red))[2] + sqrt((C %*% vcov(fit.red) %*% t(C))[2,2]) * qt(1 - 0.10/6, n-3)),
c((C %*% coefficients(fit.red))[3] - sqrt((C %*% vcov(fit.red) %*% t(C))[3,3]) * qt(1 - 0.10/6, n-3),
(C %*% coefficients(fit.red))[3] + sqrt((C %*% vcov(fit.red) %*% t(C))[3,3]) * qt(1 - 0.10/6, n-3))
)
Bf
View(goat)
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
k <- 4
new_obs_young_male <- data.frame(
age = 0,
gender="male"
)
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
# e)
alpha <- 0.1
k <- 4
new_obs_young_male <- data.frame(
age = 0,
gender="male"
)
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
View(goat)
new_obs_young_male <- data.frame(
age = 0,
gender="male"
)
new_obs_young_female <- data.frame(
age = 0,
gender="female"
)
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
View(goat)
new_obs_adult_male <- data.frame(
age = 10,
gender="male"
)
new_obs_adult_female <- data.frame(
age = 10,
gender="female"
)
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
predict(fit, new_obs_young_female,  interval="confidence", level=1-alpha/k)
predict(fit, new_obs_adult_male,  interval="confidence", level=1-alpha/k)
predict(fit, new_obs_adult_female,  interval="confidence", level=1-alpha/k)
Bf
# solution of point e) (I changed C matrix)
C <- rbind(
c(1, 0, 0),  # t=0, male:     A
c(1, 1, 0),  # t=0, female:   A + A_f
c(1, 0, 1),  # t=inf, male:   A + B
c(1, 1, 1)   # t=inf, female: A + A_f + B
)
mu_hat <- C %*% coef(fit.red)
fit.red
se <- sqrt(diag(C %*% vcov(fit.red) %*% t(C)))
se
p
length(fit.red$coefficients)
qt_val <- qt(1 - 0.10 / (2*4), df = n - length(fit.red$coefficients))  # делим на 2 для двустороннего
CI <- cbind(mu_hat - qt_val * se,
mu_hat + qt_val * se)
CI
predict(fit, new_obs_young_male,  interval="confidence", level=1-alpha/k)
predict(fit, new_obs_young_female,  interval="confidence", level=1-alpha/k)
summary(fit_reduced)
# d) impement reduced model (we can ignore gender, due to it don't reject H0 of zero coefficient)
fit_reduced <- lm(height ~I(1 - exp(-age)) + I(1 - exp(-age)):gender, data=goat)
mu_hat <- C %*% coef(fit_reduced)
se <- sqrt(diag(C %*% vcov(fit_reduced) %*% t(C)))
qt_val <- qt(1 - 0.10 / (2*4), df = n - length(fit_reduced$coefficients))  # делим на 2 для двустороннего
CI <- cbind(mu_hat - qt_val * se,
mu_hat + qt_val * se)
CI
library(MASS)
library(car)
library(rgl)
options(rgl.printRglwidget = TRUE)
# Example 1: Multiple linear regression -------------------------------------------------------
# Dataset cars: distance taken to stop [ft] as a function of velocity [mph]
# for some cars in the 1920s
help(cars)
head(cars)
dim(cars)
plot(cars, xlab='Speed', ylab='Stopping distance', las=1)
n          <- dim(cars)[[1]]
distance   <- cars$dist
speed1     <- cars$speed
speed2     <- cars$speed^2
## Parameter estimation -----------------------------------------------------------------------
# Assumptions: E(Eps) = 0  and  Var(Eps) = sigma^2
help(lm)
fm <- lm(distance ~ speed1 + speed2)
summary(fm) # on p-value, we look only if the data is normal
fitted(fm)        # y hat (predicted)
residuals(fm)     # eps hat (residuals)
distance - fitted(fm)  - residuals(fm) # residuals = y_true - y_predict
coefficients(fm)  # beta_i (intercept - b0)
vcov(fm)          # cov(beta_i)
fm$rank # order of the model [r+1] (r - number of features, +1 - b0)
fm$df   # degrees of freedom of the residuals [n-(r+1)]
hatvalues(fm) # h_ii (or sometimes called "leverage effect")
# way to work with slight distortion if residuals
rstandard(fm) # standardized residuals: eps_j / sqrt(s^2*(1-h_ii))
sum(residuals(fm)^2)/fm$df  # s^2 estimate of sigma^2
residuals(fm)/sqrt(sum(residuals(fm)^2)/fm$df*(1-hatvalues(fm))) - rstandard(fm)
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-5,130))
x <- seq(0,30,by=0.1)
b <- coef(fm)
lines(x, b[1]+b[2]*x+b[3]*x^2)
## Inference on the parameters ----------------------------------------------------------------
# Assumption: Eps ~ N(0, sigma^2) -  normality assumtion on residuals
# Test (Fisher): (F-test)
#   H0: (beta1, beta2) == (0, 0) vs H1: (beta1, beta2) != (0, 0)
linearHypothesis(fm, rbind(c(0,1,0), c(0,0,1)), c(0,0)) # C-matrix
# beta0 * 0 + beta1 * 1 + beta2 * 0 = 0
# beta0 * 0 + beta1 * 0 + beta2 * 1 = 0
# это проверяерт чтобы хотя бы один был не равен 0, поэтому только одно p-value
summary(fm) # p-value < 0.05, so we can say that at least variable significant
p <- 2  # number of tested coefficients
r <- 2  # number of regressors
# Confidence region:
# center: point estimate
c(coefficients(fm)[2], coefficients(fm)[3])
# Direction of the axes?
eigen(vcov(fm)[2:3,2:3])$vectors # vcov(fm) - cov(beta_i)
plot(coefficients(fm)[2], coefficients(fm)[3], xlim = c(-6,6), ylim = c(-6,6), asp=1, xlab='beta1', ylab='beta2')
ellipse(coefficients(fm)[2:3], vcov(fm)[2:3,2:3], sqrt(p*qf(1-0.05,p,n-(r+1))))
abline(v=0)
abline(h=0)
vcov(fm)
vcov(fm)[2:3,2:3]
ellipse(coefficients(fm)[2:3], с((4,4), (4,4)), sqrt(p*qf(1-0.05,p,n-(r+1))))
ellipse(coefficients(fm)[2:3], с(c(4,4), c(4,4)), sqrt(p*qf(1-0.05,p,n-(r+1))))
as.matrix(с(c(4,4), c(4,4)))
as.matrix(c(4,4), c(4,4))
as.matrix(c(4,4, 4,4), 2,2)
rbind(c(4,4), c(4,4))
ellipse(coefficients(fm)[2:3], rbind(c(4,4), c(4,4)), sqrt(p*qf(1-0.05,p,n-(r+1))))
ellipse(coefficients(fm)[2:3], rbind(c(4,0),
c(0,4)), sqrt(p*qf(1-0.05,p,n-(r+1))))
ellipse(coefficients(fm)[2:3], rbind(c(4,0),
c(0,1)), sqrt(p*qf(1-0.05,p,n-(r+1))))
ellipse(coefficients(fm)[2:3], rbind(c(4,0),
c(0,1)), sqrt(p*qf(1-0.05,p,n-(r+1))))
plot(coefficients(fm)[2], coefficients(fm)[3], xlim = c(-6,6), ylim = c(-6,6), asp=1, xlab='beta1', ylab='beta2')
ellipse(coefficients(fm)[2:3], rbind(c(4,0),
c(0,1)), sqrt(p*qf(1-0.05,p,n-(r+1))))
vcov(fm)[2:3,2:3]
ellipse(coefficients(fm)[2:3], rbind(c(1,0.8),
c(0.8,1)), sqrt(p*qf(1-0.05,p,n-(r+1))))
ellipse(coefficients(fm)[2:3], rbind(c(1,0.9),
c(0.9,1)), sqrt(p*qf(1-0.05,p,n-(r+1))))
plot(coefficients(fm)[2], coefficients(fm)[3], xlim = c(-6,6), ylim = c(-6,6), asp=1, xlab='beta1', ylab='beta2')
ellipse(coefficients(fm)[2:3], rbind(c(1, 0.9),
c(0.9, 1)), sqrt(p*qf(1-0.05,p,n-(r+1))))
vcov(fm)[2:3,2:3]
eigen(vcov(fm)[2:3,2:3])
eigrn(rbind(c(1, 0.9),
c(0.9, 1)))
eigen(rbind(c(1, 0.9),
c(0.9, 1)))
eigen(rbind(c(100, 0.9),
c(0.9, 100)))
eigen(rbind(c(0.5, 0.9),
c(0.9, 0.5)))
